[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Space!",
    "section": "",
    "text": "What can you find in my space?\n\nNorn’s Blessing: it lists my courses about statistics, data science, machine learning.\nMímisbrunnr: A math tool box. Students can be directed to the right items whenever they need.\nSkalds: Some short notes are listed here.\n\n–\n\n\n\nYggdrasil is often referred to as the “world tree” in Norse mythology. It is an immense and central sacred tree that connects different worlds in Norse cosmology. My vision is to weave a Yggdrasil for data science. Source: Wikipidia"
  },
  {
    "objectID": "Skalds/20241028_Cracking the Code.html",
    "href": "Skalds/20241028_Cracking the Code.html",
    "title": "Cracking the Code",
    "section": "",
    "text": "In this section, we discuss a cryptography problem that Prof. Persi Diaconis introduced in his paper. He used this example to introduce the Markov Chain Monte Carlo (MCMC) approach. This is a beautiful example not only for introducing MCMC but also a nice example to demonstrate the “routine” of using data to solve real problems. We can also review some basic probability knowledge and concepts. In addition, it is a good programming exercise, and you may try to implement it by yourself.\nThe story takes place in a prison in the United States. One day, the police officer found many strange symbols on a piece of paper, see figure below. It turned out that some prisoners wanted to exchange secret information through ciphertext. The officer first visited a linguist and was told that the cipher text might be a simple substitution cipher. The linguists can’t understand the cipher either but recommend seeking help from statisticians."
  },
  {
    "objectID": "Skalds/20241028_Cracking the Code.html#problem-understanding",
    "href": "Skalds/20241028_Cracking the Code.html#problem-understanding",
    "title": "Cracking the Code",
    "section": "Problem Understanding",
    "text": "Problem Understanding\nFirst of all, let’s understand what is a substitution cipher. It is a very simple cipher using fixed symbols to replace letters, punctuation marks, numbers, and spaces. Then the key of this kind of cipher is a map from the symbol space to the letter space.\n\\[\n  f: \\{ \\text{code space} \\} \\to \\{ \\text{alphabet} \\}\n\\]\nWe use the letter itself instead of symbols to simplify the problem and make it easier to express. For example, suppose the key is\n\\[\n  f: \\{ \\text{d} \\to \\text{a}, \\text{a} \\to \\text{t}, \\text{t} \\to \\text{d}, \\dots \\}\n\\]\nAccording to this key, the regular English word “data” is encrypted as “atdt”. Next, same as the original paper, we take a small piece from the well-known section of Shakespeare’s Hamlet as a simple example.\n\n\nenter hamlet ham to be or not to be that is the question whe ther tis nobler in \nthe mind to suffer the slings and arrows of outrageous fortune or to take arms against \na sea of troubles and by opposing end.\n\n\nWe randomly generate a substitution cipher:\n\n\ntuntvmfeprtnmfepmnsmgtmsvmusnmnsmgtmnfenmjbmnftmatbnjsumdftnftvmnjbmusgrtv \nmjumnftmpjuymnsmbiitvmnftmbrjuhbmeuymevvsdbmsims nvehts bmisvn \nutmsvmnsmnextmevpb mehejubnmembtemsimnvs g rtbmeuymgwmskksbjuhmtuy\n\n\nOne can apply inverse mapping to translate the cipher and retrieve the secret information. In other words, one can understand the cipher if holds the key. Thus the problem is to find the key (mapping \\(f\\)). Well, it is also obviously not such an easy task. The size of the symbol space is too big to simply apply a brute force algorithm to search the key. The size of the symbol space is \\(27\\) even though we only consider letters in lowercase and space. In such a case, the total number of possible keys is \\(27! = 1.08\\times 10^{28}\\). So we have to figure out a smart approach that takes some prior information into account to search the key! The prior information leads to our assumptions and the model on top of it."
  },
  {
    "objectID": "Skalds/20241028_Cracking the Code.html#model-and-assumption",
    "href": "Skalds/20241028_Cracking the Code.html#model-and-assumption",
    "title": "Cracking the Code",
    "section": "Model and Assumption",
    "text": "Model and Assumption\nFor a natural language, we believe the alphabetical order should obey certain rules. For example, the probability of the letter ‘a’ being connected by the letter ‘b’ must be greater than that of the letter ‘x’. Now we translate this prior knowledge as an assumption. To do so, we view the sequence of letters in a text as a stochastic process. There could be \\(27\\) possible values for each time point (or position). We call the possible values (letters) as state. Then we can assume that a sequence of letters in a text should satisfy the first-order Markov properties.\n\\[\n  \\Pr(x_t | x_{1}, x_{2}, \\dots, x_{t-1}) = \\Pr(x_t | x_{t-1})\n\\]\nwhere \\(x_i\\) present the state at the \\(t\\)-th position in a text. In other words, the letter in \\(t\\)-th position only depends on the letter in the previous position. This assumption implies two things. First, the transition between two letters in a text should satisfy some transition probabilities and this can be summarized in a transition probability matrix,\n\\[\n    \\begin{pmatrix}\n        & a & d & t & \\dots \\\\\n        a & p_{11} & p_{12} & p_{13} & \\dots \\\\\n        d & p_{21} & p_{22} & p_{23} & \\dots \\\\\n        t & p_{31} & p_{32} & p_{33} & \\dots \\\\\n        \\vdots & \\vdots & \\vdots & \\vdots  & \\ddots\n    \\end{pmatrix}\n\\]\nThe second thing is the likelihood of a text can be factorized as the product of the corresponding transition probabilities. For example, the likelihood of ‘data’ is\n\\[\n  \\ell('data') = \\Pr(d\\to a)\\Pr(a\\to t)\\Pr(t\\to a) = p_{21}p_{13}p_{31},\n\\]\nbut the likelihood of ‘atdt’ is\n\\[\n  \\ell('atdt') = \\Pr(a\\to t)\\Pr(t\\to d)\\Pr(d\\to t) = p_{13}p_{32}p_{23}.\n\\]\nSince ‘atdt’ is much weirder than ‘data’, the likelihood of the former must be lower than the latter, \\(\\ell('atdt') &lt; \\ell('data').\\) Given the two thoughts, we can summarize our problem as an optimization problem and present it in a mathematical language,\n\\[\n  \\max_{f} \\ell( f( \\text{Cipher}) )\n\\]\nSo the problem is to find a key \\(f\\) that maximizes the likelihood of the translated text, \\(f(\\text{Cipher})\\). Great! We have obtained a promising idea, however, it is still not clear how to search for the key in a smart way. Before we discuss the final solution, we need to first discuss one practical issue, that is how to obtain the transition probability matrix."
  },
  {
    "objectID": "Skalds/20241028_Cracking the Code.html#model-estimation",
    "href": "Skalds/20241028_Cracking the Code.html#model-estimation",
    "title": "Cracking the Code",
    "section": "Model Estimation",
    "text": "Model Estimation\nAt this stage, we can finally invite our protagonist, data. So far, we have a probability model, the first-order Markov chain model. It is a parametric model and the parameter is the transition probability matrix. To be a little bit mysterious, we can’t know the real model coefficients set by God, we can only try to figure out his mind through the data generated by the model he created. Seriously speaking, we need to use the data to estimate the parameters of the model. For this problem, we can download classical books in English and that is our data. More specifically, we can count the frequencies of all \\(27^2\\) different transitions in a book. Then normalize all the frequencies by the total number of transitions as the estimation of the transition probabilities. You can see part of the estimation of the transition probability matrix bellow\n\\[\n  \\begin{matrix}\n& A & B & C & D & E & F & G & H & I & \\dots \\\\\nA & 0.000 & 0.017 & 0.034 & 0.055 & 0.001 & 0.008 & 0.017 & 0.002 & 0.043 & \\dots \\\\\nB & 0.093 & 0.007 & 0.000 & 0.000 & 0.328 & 0.000 & 0.000 & 0.000 & 0.029 & \\dots \\\\\nC & 0.110 & 0.000 & 0.017 & 0.000 & 0.215 & 0.000 & 0.000 & 0.184 & 0.042 & \\dots \\\\\nD & 0.021 & 0.000 & 0.000 & 0.011 & 0.116 & 0.001 & 0.004 & 0.000 & 0.066 & \\dots \\\\\nE & 0.042 & 0.001 & 0.017 & 0.091 & 0.026 & 0.010 & 0.007 & 0.002 & 0.011 & \\dots \\\\\nF & 0.071 & 0.001 & 0.000 & 0.000 & 0.084 & 0.052 & 0.000 & 0.000 & 0.085 & \\dots \\\\\nG & 0.066 & 0.000 & 0.000 & 0.001 & 0.115 & 0.000 & 0.009 & 0.117 & 0.049 & \\dots \\\\\nH & 0.164 & 0.000 & 0.000 & 0.000 & 0.450 & 0.000 & 0.000 & 0.000 & 0.156 & \\dots \\\\\nI & 0.017 & 0.008 & 0.049 & 0.050 & 0.049 & 0.022 & 0.025 & 0.000 & 0.001 & \\dots \\\\\n\\vdots  & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots  \n\\end{matrix}\n\\]\nOnce the transition probability is ready, then we can use it to evaluate the likelihood of a text. For example, the likelihood of the original text and the cipher is \\(-458.26\\) and respectively."
  },
  {
    "objectID": "Skalds/20241028_Cracking the Code.html#inference",
    "href": "Skalds/20241028_Cracking the Code.html#inference",
    "title": "Cracking the Code",
    "section": "Inference",
    "text": "Inference\nGreat! Now we can discuss how to find the optimal key. Imagine that all the \\(27!\\) possible keys form a huge solutions space. Similar to most optimization algorithms, we can start from a certain initial value, and then gradually approach the optimal solution through some iterative mechanism. (In fact, this is not very accurate, but we can understand it this way first, and later I will explain what our algorithm is doing.) Firstly, we randomly generate an initial key, for example\n\\[\n  f^{(0)}: \\{ \\text{a} \\to \\text{f}, \\text{b} \\to \\text{g}, \\text{c} \\to \\text{m}, \\dots \\},\n\\]\nand evaluate the likelihood of this key \\(\\ell(f^{(0)}(cipher))\\). Then we randomly move a small step to a new key \\(f^{(1)}\\) by making a random transposition of the values \\(f^{(0)}\\) assigned to two symbols, for example,\n\\[\n  f^{(1)}: \\{ \\text{a} \\to \\text{m}, \\text{b} \\to \\text{g}, \\text{c} \\to \\text{f}, \\dots \\}\n\\]\nIs the new \\(f^{(1)}\\) key a good proposal? We can calculate its likelihood value and compare it with the previous key. The simple situation is that if the likelihood value of the new key is higher than the previous one, \\(\\ell(f^{(1)}) &gt; \\ell(f^{(0)})\\), we should keep the new key. The interesting question is what if the new key is worse? Simply discard this new key? It seems to be a good way, but there is a risk of falling into the local area and finally missing the optimal key. What is the good way then? There are two situations. If the likelihood value of the new key is much lower than the previous key, then we should be inclined to abandon it; however, if the likelihood values of the two keys are almost the same, then we should tend to keep it. This plan sounds more reasonable, but the question is how exactly realize ‘tend to’? We can calculate the ratio, \\(\\frac{ \\ell(f^{(t)}) }{ \\ell( f^{(t-1)}}\\). This value, between 0 and 1, can be regarded as a probability that an action will be executed. If we generate a Bernoulli-distributed random number with this probability, and use it to determine whether to keep the new key, then the above operation can be achieved. We summarize this approach in the following algorithm\nInitialize the key f(0)\nfor(i in 1:R){\n  Step 1. Evaluate the likelihood value of f(t-1), likelihood(f(t-1))\n  Step 2. Propose a new key f(t) by making a random transposition of the values of f(t-1) assigned to two letters.\n  Step 3. Make decision, either keep the new key or stay with the old key\n  if(likelihood(f(t)) &gt; likelihood(f(t-1))){\n    keep f(t)\n  } else{\n    generate u ~ Ber(f(t)/f(t-1))\n    if(u == 1){\n      keep f(t)  \n    } else{\n      stay at f(t-1)\n    }\n  }\n}\nThe performance of our algorithm can be glimpsed from the following R outputs.\n\n\n\nR outputs"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_5.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_5.html",
    "title": "1.5 Regression Model and Classification Model",
    "section": "",
    "text": "The machine learning problem can be understood as regression problem when the target variable is a continuous variable. For example, predict the house price based on different feature variables; predict the pixel values of CT scans based on MRI scans; predict the stock price based on feature variables of market. A simple scenario displayed in the figure below, a basic regression model is a linear model, \\(y_i = w_0 + w_1x_i + \\epsilon_i\\). From a geometric perspective, a linear regression model can be seen as a straight line that passes through all sample observations. In the generalization stage, the target value can be predicted from feature variable through the regression model.\n\n\n\nRegression Problem"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_5.html#regression-model",
    "href": "Courses/c_mlwr1_2024/l1/l1_5.html#regression-model",
    "title": "1.5 Regression Model and Classification Model",
    "section": "",
    "text": "The machine learning problem can be understood as regression problem when the target variable is a continuous variable. For example, predict the house price based on different feature variables; predict the pixel values of CT scans based on MRI scans; predict the stock price based on feature variables of market. A simple scenario displayed in the figure below, a basic regression model is a linear model, \\(y_i = w_0 + w_1x_i + \\epsilon_i\\). From a geometric perspective, a linear regression model can be seen as a straight line that passes through all sample observations. In the generalization stage, the target value can be predicted from feature variable through the regression model.\n\n\n\nRegression Problem"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_5.html#classification-model",
    "href": "Courses/c_mlwr1_2024/l1/l1_5.html#classification-model",
    "title": "1.5 Regression Model and Classification Model",
    "section": "Classification Model",
    "text": "Classification Model\nThe problem can be viewed as a classification problem when the target variable is categorical. We often refer to this type of target variable as labels. For example, in the classification with Iris data, the species variable is the label variable, and we aim for finding a good “function” taking 4 shaping variables as input to predict the labels based on data. This function is often refer to a classifier. So, what kind of function can perform this role? Let’s take a look at a real classifier first, a “coin sorter.” Its operation is quite simple, as it classifies coins based on their different diameters. Inside the machine, there are holes of varying sizes corresponding to different diameters, and through vibration, coins will fall into the holes that match their size. In essence, it’s classifying by comparing a variable to a threshold value. The idea is quite simple, but it is just the essential idea of machine learning classifier.\n\n\n\nLHS: Classification with iris data. RHS: A real classifier, coin sorter. The working principle: Variable (diameter) V.S. Threshold value.\n\n\nWell, usually we have multiple feature variables in a classification problem, then how do we apply this simple working principle to design a classifier? Let’s see another example. You might not know yet, in fact, teacher becomes a classifier after an exam. Well, to pass or not to pass is a classification problem. Suppose, in a secret exam, each student answers 5 questions and each question is worth 20 points. Student passes the exam if the total points are larger or equal to 60. I have corrected all the exams; the results are summarized in Table 1, and \\(1\\) indicating the question was correctly answered and \\(0\\) indicating not. Then, who can pass the exam?\n\n\n\n\n\nI believe it is a very simple problem, for example, Super girl correctly answered 4 questions and get 80 points that is above the threshold value 60, so she passed the exam! However, spiderman only got 20 points that is lower than 60, so he can’t pass. If we clearly write down the calculation process, we actually used the following formula to calculate the totol score, then compare the total score with the critical point, 60.\n\\[\n  20\\times Q_1 + 20\\times Q_2 + 20\\times Q_3 + 20\\times Q_4 + 20\\times Q_5 \\geq 60\n\\]\n\nNow, we know what a simple classifier looks like. Essentially, it is a two-step procedure. We create a single variable through the weighted sum of all feature variables first, then compare the resulting value with a threshold value. In formal, the classifier can be represented as\n\\[\n  y = \\text{Sign}(w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_p x_p)\n\\] where \\(\\text{Sign}(x)\\) is a sign function returning 1 if \\(x&gt;0\\) and 0 if \\(x&lt;0\\). We refer coefficients \\(w_1, \\dots, w_p\\) as weights, the weighted sum of feature variables \\(w_1 x_1 + w_2 x_2 + \\dots + w_p x_p\\) as scores and \\(w_0\\), the threshold value, as bias. If the score value is equal to 0, then this observation can’t be classified by this classifier, and the thing we can do best is flip a coin to make the decision. We call all the points that satisfy equation \\(w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_p x_p = 0\\) as the decision boundary. For example, in a 2D feature space, the decision boundary \\(w_0 + w_1x_1 + w_2x_2 =0\\) is just a straight line with a slope of \\(-w_1/w_2\\) and an intercept of \\(-w_0/w_2\\), see the figure below.\n\n\n\nIn this example, the 2D feature space is cut into two parts by the decision boundary (red line). For any unlabeled observations (blue dots), if it is above the decision boundary, then it will be classified as yellow , otherwise, green.\n\n\nThis kind of classifier is called linear classifier, since the decision boundary is presented by a linear function. It is a straight line in 2D space, a plane in 3D space, and hyper-plane in a higher dimension space. You might have already realized that in fact, a classifier is solely determined by its weights and bias, and machine learning algorithms tell us how to find the optimal weights and bias through data. There are several classical methods (algorithms) for learning a linear classifier which are perceptron algorithm, linear discriminant analysis, logistic regression, and maximum margin classifier. In this course, we will introduce all of them except maximum margin classifier.\nRemark: Just as all the rules of arithmetic start with \\(1+1\\), don’t underestimate this linear classifier. You will see that all complex classifiers are built upon them. For example, maximum margin classifier is the foundation of SVM (Support vector machine) which dominate machine learning world for 20 years, the perceptron algorithm is the starting point of artificial neural net works, and no matter how complex a neural network architecture may be, as long as it is a classifier, its final layer will inevitably be a logistic regression model.\n\nPrevious page | Lecture 1 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_0.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_0.html",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "",
    "text": "In this lecture, we introduce machine learning to you. You will learn the basic elements in this field and an old, basic, but very interesting algorithm in machine learning.\nNext, I’ll begin with a review of key milestones in artificial intelligence. Then, we’ll delve into some metaphysical concepts, exploring the underlying logic of machine learning. By understanding the human learning process, we’ll gain insight into the entire machine learning process. Finally, after covering the ABCs of machine learning, we’ll focus on the fundamental forms of machine learning models.\nOutline：\n\n1.1 Mailstone of AI, AlphaGo, 2016\n1.2 Philosophy of Machine Learning\n1.3 Machine Learning Process\n1.4 Machine Learning ABC\n1.5 Regression Model and Classification Model\n\n\nLecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_3.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_3.html",
    "title": "1.3 Machine Learning Process",
    "section": "",
    "text": "What is the process of machine learning like? First, let me tell you about some observations I have made about my sons. After they learned to speak, they began asking me all sorts of questions. For example, when we were at the supermarket, he would point at apples and ask me, ‘What’s this, Daddy?’ I just simply answered them, “It is an apple.” After a few times, they would change their questioning style from special to general, like, ’Daddy, is this an apple? In about half months, they turned into high-precision apple classifiers. They could even recognize that the logo on my laptop is also an apple! Amazing! I must emphasize that I never taught them how to recognize apples.\n\n\n\nI have two boys at home. On the LHS, the boy wearing his pants frontside back is my elder son, Siyi, when he was three years old. He was earnestly planting flowers in the artificial soccer field. On the RHS, the guy who resembles a sloth is my younger son, Siqi. It is quite evident that he is a happy fellow. Actually, he is very quiet and cool.\n\n\nWe can summarize the human learning process from the example of my sons learning to recognize apples. First, they would accumulate experience through observation and questioning. Once they had enough experience, they would begin their own learning and distill this into “knowledge.” Subsequently, they would use general questions to validate their knowledge. Finally, they would use their validated knowledge to identify the Apple logo. This human learning process is summarized in the following figure (up).\nIn fact, if we just change the names of the components, this is also the process of machine learning. For computer programs, “experience” is essentially “data”, “learning” involves “training” with algorithms, for example proceptron algorithm, and the “knowledge” distilled is a type of “model”. We call the “self-exam process” as “validation” and “applying” it to new problems as “generalization”. The entire machine learning process is presented in the following figure (down). In this course, we focus on “training” and “validation” steps. For “training” step, we introduce several basic and fundamental algorithms for linear models and discuss several validation methods for “validation” step. See Figure below.\n\n\n\nThe human learning process (up) V.S. machine learning process (down)\n\n\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_s.html",
    "title": "Proceptron and Its Algorithm",
    "section": "",
    "text": "Here, we introduce an old but interesting algorithm to train a classifier. The main purpose is help you getting real feelings about taring algorithm in machine learning and have a deeper understanding of linear classifiers.\nNext, I’ll begin with a historical review of this algorithm and understanding the basic purpose of an machine learning algorithm. Then we use more algebra and geometry to show the intuitive idea behind of a linear classifier in general. In the end, the proceptron algorithm will be interpreted through algebra and geometry."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s.html#history",
    "href": "Courses/c_mlwr1_2024/l1/l1_s.html#history",
    "title": "Proceptron and Its Algorithm",
    "section": "History",
    "text": "History\nProceptron classifier is viewed as the foundation and building block of artificial neural network. For a historical introduction, see the figures below.\n\n\n\nThe Perceptron classifier was developed by Frank Rosenblatt in 1957 (LHS). Rosenblatt’s goal was to create a machine that could classify visual patterns, such as distinguishing between different shapes. At the time, he envisioned using large computers to simulate neural networks, inspired by how the brain processes information. His early experiments involved using a huge computer called the Mark I Perceptron (RHS), which attempted to recognize different shapes by adjusting weights based on input data. This work laid the foundation for modern neural networks and machine learning, despite initial limitations in its capacity to handle complex, non-linear problems.\n\n\nSuppose we are solving a binary classification problem with \\(p\\) feature variables. As discussed before, it can be represented as\n\\[\n  \\hat{y} = \\text{Sign}( \\textbf{w}^{\\top} \\textbf{x} + w_0  )\n\\]\nwhere \\(\\textbf{w} = (w_1, w_2, \\dots, w_p)^{\\top}\\), and \\(\\textbf{x} = (x_1, x_2, \\dots, x_p)^{\\top}\\). Different from before, here we represent the weighted sum of \\(p\\) feature variables, \\(w_1x_1+ \\dots + w_px_p\\), as the inner (dot) product of two vectors, i.e. \\(\\textbf{w} \\cdot \\textbf{x} = \\textbf{w}^{\\top} \\textbf{x}\\).\n\nNote: In order to understand proceptron algorithm, we need some basic knowledge about vector and its operations. If you are not familiar with it or need to refresh it, read about vector, operators, inner product before start reading the next.\n\nThe perceptron algorithm is about finding a set of reasonable weights. The key term here, “reasonable,” is easy to understand—it refers to a set of weights that can deliver good predictive performance. The core issue is how to find them.\n\nBrute-force idea: try all possible weight values and record the corresponding model performance, such as accuracy, and then choose the weights that yield the highest accuracy as the final model parameters.\n\nHowever, this idea is clearly not ideal. Even if we only have two feature variables, this would still not be a simple task. A smarter approach is to do it this way: we start with an initial guess for the weight values, and then gradually approach the most reasonable weights through some iterative mechanism. This mechanism is called the perceptron algorithm. Next, let’s dive into learning this magical mechanism—the perceptron algorithm.\nNext, the logic goes like this:\n\nFurther explore the geometric properties of linear classifiers\nUse geometry to understand how this algorithm works."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s.html#geometry-of-linear-classifiers",
    "href": "Courses/c_mlwr1_2024/l1/l1_s.html#geometry-of-linear-classifiers",
    "title": "Proceptron and Its Algorithm",
    "section": "Geometry of Linear Classifiers",
    "text": "Geometry of Linear Classifiers\nNote: From now, we will temporarily ignore the bias term, \\(w_0\\), or assume it as \\(0\\). It will not influence our final conclusion. No worries. So, the basic classifier is represented as \\(y = \\text{Sign}(\\textbf{w}^{\\top}\\textbf{x})\\)\nPreviously, we explored the geometric understanding of linear classifiers, which is that the classifier determines a linear decision boundary. Next, let’s understand a linear classifier from another view of geometry. Suppose we have a classifier with two feature variables, \\(x_1\\) and \\(x_2\\), and the “reasonable” weight vector is \\(\\textbf{w} = (0.6, 0.8)^{\\top}\\). Look at the conceptual plot below.\n\n\n\n\n\n\n\n\n\nIt is easy to see that all the vectors (points) in blue form a sharp angle with the weights vector (black). By the property of inner product, (read about inner product) for any point \\(\\textcolor{blue}{\\textbf{x}} = (\\textcolor{blue}{x_1},\\textcolor{blue}{x_2})^{\\top}\\) standing on the direction pointed by the blue arrow, \\(\\textbf{w}^{\\top}\\textcolor{blue}{\\textbf{x}} \\propto \\cos(\\alpha) &gt; 0\\), i.e. all the cases on this direction will be classify as positive. On the contrary, all the vectors (points) in blue form a obtuse angle with the weights vector, and then \\(\\textbf{w}^{\\top}\\textcolor{red}{\\textbf{x}} \\propto \\cos(\\beta) &lt; 0\\), i.e. all the points standing on the direction pointed by a red vector will be classified as negative. With this observation, we can easily understand how does a “reasonable” linear classifier work.\nBased on this principle, let’s have look at a concrete example in the figure below.\n\n\n\nIn a binary classification problem, we have two feature variables, each with 10 cases, blue for positive and red for negative. We have a “reasonable” weight vector \\(\\textbf{w}\\) (orange arrow), which determines a linear decision boundary (purple line). \\(\\textbf{w}\\) is “reasonable” because it has an angle less than 90 degrees with all positive vectors, but an angle greater than 90 degrees with all negative vectors. Of course, a more direct understanding is that this linear classification boundary divides the entire feature space into two parts, with all positive cases at the bottom and all negative cases at the top. However, the first explation is more useful for understanding the proceptron algorithm."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s.html#proceptron-algorithm",
    "href": "Courses/c_mlwr1_2024/l1/l1_s.html#proceptron-algorithm",
    "title": "Proceptron and Its Algorithm",
    "section": "Proceptron Algorithm",
    "text": "Proceptron Algorithm\nWith the discussion above, at least for me, the weights vector of a perceptron classifier is like a sword of judgment, wielded to evaluate all cases. So, I just name proceptron algorithm as finding sword of judgment algorithm. You may remember the brute force idea, so the next question is whether we have a clever way to quickly find the sword of judgment instead of trying all possibilities.\n\n\n\nSword of judgment: it is a powerful and iconic weapon in the Transformers universe, often wielded by the noble Autobot leader, Optimus Prime. This sword embodies the principles of justice and righteousness, serving as a symbol of hope in the battle against evil. Now, you understand why blue presents positive? Because The Autobots have blue eyes.\n\n\nBefore we begin to understand how this algorithm works, we must first reach a consensus that no matter how we look for it, we must have a starting point, and this starting point is preferably random. If you agree with me, let’s randomly pick up one initial guess of the sword (weights vector) in the following conceptual example.\n\n\n\n\n\nThis initial guess is not too bad since it only made one mistake, see the 2nd column of the image. This case is a positive case but wrongly predicted as negative. So, we need to correct this weights vector such that the corresponding decision boundary will be rotated clockwise slightly. From the 3rd column of the image we can see that, this aim can be achieved by updating the weights vector as \\[\n  \\textcolor{red}{\\textbf{w}}^{\\text{new}} = \\textcolor{green}{\\textbf{w}}^{\\text{old}} + \\textcolor{blue}{\\textbf{x}}\n\\]\nBy the Parallelogram Law (read about vector addition), the old weights vector will be rotated clockwise, thereafter the decision boundary is corrected properly.\nSuppose the mistake by the initial guess of sword (\\(\\textbf{w}\\)) happened for a negative case, see figure below. In this case, we need to update the weights vector as\n\\[\n  \\textcolor{red}{\\textbf{w}}^{\\text{new}} = \\textcolor{green}{\\textbf{w}}^{\\text{old}} - \\textcolor{red}{\\textbf{x}}\n\\]\nsuch that the decision boundary will be anticlockwise slightly.\n\n\n\n\n\nNotice that the target variable \\(y = -1\\) or \\(+1\\), therefore we can integrate the two updating rule as one:\n\\[\n  \\textcolor{red}{\\textbf{w}}^{\\text{new}} = \\textcolor{green}{\\textbf{w}}^{\\text{old}} + y \\textbf{x}\n\\]\nNow, we have actually found a clever way to search for the sword of judgment. Basically, we can do this. First, number all cases, and then randomly pick an initial guess for \\(\\textbf{w}\\). After these are prepared, we start to use this sword to test cases in the order of numbers. If we find a wrong judgment, such as number 3, then we correct this sword according to the updating formula above. After the correction, continue to judge in order, that is, start judging from number 4, and continue to correct the error. Repeat this process, and when we find a real sword that can correctly judge all cases, we stop the search process. This is the so called Proceptron Algorithm:\nProceptron Algorithm:\nInputs: y nx1 vector taking vaules -1 or 1, X nxp matrix\nInitialization: weight vector w randomly initialized \n\nWhile(All cases are correctly classified){\n  for(i in 1:n){\n    if(case i is misclassified){\n      w = w + y[i]X[i, ]      \n    }\n  }\n}\nBelow, you can see an animation presenting the Proceptron Algorithm in action.\n\n\n\nAnimation of running perceptron algorithm\n\n\nRemarks:\n\nThis algorithm works, that is, as long as we follow this algorithm, no matter where we start, we can always find the sword of judgment.\nHowever, as you may have realized, this algorithm has a prerequisite, that is, our training samples are linearly separable. In other words, we can find a straight line (2D case) to split the sample space, all positive and negative examples stand on each side, or we will not make any mistakes in the training samples.\nIn addition, you may also find that the “reasonable” \\(\\textbf{w}\\) obtained by this algorithm is not unique, that is, there exist many swords of judgment, and which one we finally get depends on our initial guess.\n\n\nLecture 1 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s_0.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_s_0.html",
    "title": "Proceptron and Its Algorithm",
    "section": "",
    "text": "Here, we introduce an old but interesting algorithm to train a classifier. The main purpose is help you getting real feelings about taring algorithm in machine learning and have a deeper understanding of linear classifiers.\nNext, I’ll begin with a historical review of this algorithm and understanding the basic purpose of an machine learning algorithm. Then we use more algebra and geometry to show the intuitive idea behind of a linear classifier in general. In the end, the proceptron algorithm will be interpreted through algebra and geometry.\nOutline：\n\n\nHistorical review of Proceptron\n\n\nMore geometrical insights of linear classifier\n\n\nProceptron algorithm\n\n\n\nLecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s_3.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_s_3.html",
    "title": "Proceptron Algorithm",
    "section": "",
    "text": "With the discussion above, at least for me, the weights vector of a perceptron classifier is like a sword of judgment, wielded to evaluate all cases. So, I just name proceptron algorithm as finding sword of judgment algorithm. You may remember the brute force idea, so the next question is whether we have a clever way to quickly find the sword of judgment instead of trying all possibilities.\n\n\n\nSword of judgment: it is a powerful and iconic weapon in the Transformers universe, often wielded by the noble Autobot leader, Optimus Prime. This sword embodies the principles of justice and righteousness, serving as a symbol of hope in the battle against evil. Now, you understand why blue presents positive? Because The Autobots have blue eyes.\n\n\nBefore we begin to understand how this algorithm works, we must first reach a consensus that no matter how we look for it, we must have a starting point, and this starting point is preferably random. If you agree with me, let’s randomly pick up one initial guess of the sword (weights vector) in the following conceptual example.\n\n\n\n\n\nThis initial guess is not too bad since it only made one mistake, see the 2nd column of the image. This case is a positive case but wrongly predicted as negative. So, we need to correct this weights vector such that the corresponding decision boundary will be rotated clockwise slightly. From the 3rd column of the image we can see that, this aim can be achieved by updating the weights vector as \\[\n  \\textcolor{red}{\\textbf{w}}^{\\text{new}} = \\textcolor{green}{\\textbf{w}}^{\\text{old}} + \\textcolor{blue}{\\textbf{x}}\n\\]\nBy the Parallelogram Law (read about vector addition), the old weights vector will be rotated clockwise, thereafter the decision boundary is corrected properly.\nSuppose the mistake by the initial guess of sword (\\(\\textbf{w}\\)) happened for a negative case, see figure below. In this case, we need to update the weights vector as\n\\[\n  \\textcolor{red}{\\textbf{w}}^{\\text{new}} = \\textcolor{green}{\\textbf{w}}^{\\text{old}} - \\textcolor{red}{\\textbf{x}}\n\\]\nsuch that the decision boundary will be anticlockwise slightly.\n\n\n\n\n\nNotice that the target variable \\(y = -1\\) or \\(+1\\), therefore we can integrate the two updating rule as one:\n\\[\n  \\textcolor{red}{\\textbf{w}}^{\\text{new}} = \\textcolor{green}{\\textbf{w}}^{\\text{old}} + y \\textbf{x}\n\\]\nNow, we have actually found a clever way to search for the sword of judgment. Basically, we can do this. First, number all cases, and then randomly pick an initial guess for \\(\\textbf{w}\\). After these are prepared, we start to use this sword to test cases in the order of numbers. If we find a wrong judgment, such as number 3, then we correct this sword according to the updating formula above. After the correction, continue to judge in order, that is, start judging from number 4, and continue to correct the error. Repeat this process, and when we find a real sword that can correctly judge all cases, we stop the search process. This is the so called Proceptron Algorithm:\nProceptron Algorithm:\nInputs: y nx1 vector taking vaules -1 or 1, X nxp matrix\nInitialization: weight vector w randomly initialized \n\nWhile(All cases are correctly classified){\n  for(i in 1:n){\n    if(case i is misclassified){\n      w = w + y[i]X[i, ]      \n    }\n  }\n}\nBelow, you can see an animation presenting the Proceptron Algorithm in action.\n\n\n\nAnimation of running perceptron algorithm\n\n\nRemarks:\n\nThis algorithm works, that is, as long as we follow this algorithm, no matter where we start, we can always find the sword of judgment.\nHowever, as you may have realized, this algorithm has a prerequisite, that is, our training samples are linearly separable. In other words, we can find a straight line (2D case) to split the sample space, all positive and negative examples stand on each side, or we will not make any mistakes in the training samples.\nIn addition, you may also find that the “reasonable” \\(\\textbf{w}\\) obtained by this algorithm is not unique, that is, there exist many swords of judgment, and which one we finally get depends on our initial guess.\n\n\nPrevious page | Lecture 1 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_home.html",
    "href": "Courses/c_mlwr1_2024/l7/l7_home.html",
    "title": "Lecture 7: Logistic Regression",
    "section": "",
    "text": "Course Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_7.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_7.html",
    "title": "2.7 Other Useful Things",
    "section": "",
    "text": "Alright, we now have a basic understanding of the most fundamental operations in R programming, most of which are things we will frequently use in this course. Next, we’ll introduce a few more useful concepts and some commonly used functions.\n\n2.7.1 Workspace\nIn R, the workspace refers to the environment where all objects (such as variables, functions, and data) are stored during an R session. It acts as a storage area that retains the data and objects you create, allowing you to work with them without needing to re-import or redefine them every time you start R. The most common scenario is when you’ve worked hard all day and want to take a break, but if you close R, all the objects in your working environment (memory) will disappear. In this case, you can save your current working environment as a workspace file, which has a .RData extension.\nThere are two ways to save your working environment as a workspace file. First, by mouse actions, you can click Session -&gt; Save Workspace As.... Or you can do it by command\nsave.image(\"FileName.RData\")\nThe next day, after enjoying the morning sunshine (if conditions permit) and your coffee, you can load this file and continue your hard work!\n\n\n2.7.2 Packages\nIf R could only be used for scientific computing, it would undoubtedly be overshadowed by numerous other scientific computing programs. The true strength of R lies in its extensibility, which is achieved through R packages. Initially, R packages were primarily written by statisticians to implement new methods, such as lme4 for fitting generalized linear mixed-effects models; survival for conducting survival analysis; psych for psychological research, and so on. However, writing packages is not exclusive to statisticians; an increasing number of non-statistical application packages have also been developed. Today, R has become incredibly versatile through the extension of various packages, for example this website is written by quarto package. Below, we will briefly illustrate how to install and load packages using examples.\ninstall.packages(\"kernelab\") \n# to install a new package. Note: the quotation marks are essential.\n\nlibrary(kernelab) \n# you can import a package by function `library`\n\n\n2.7.3 Useful Functions\nNext, some useful functions are introduced. These functions were extremely useful back when I was a student. However, in the era of RStudio, their usefulness has been greatly reduced. Nonetheless, they are still quite necessary for those who prefer keyboard operations or need to work on a server. In addition, these functions can, to some extent, enhance R users’ understanding of R programming.\n\nls function: it can list all the objects in the workspace or current environment.\nrm function: it can help us to remove objects from the workspace or current environment.\n\n# Example 1\nx = 1\nrm(x) \n# Example 2\nrm(list = ls()) # Danger Warning: This command will remove all objects listed by `ls`\n\nstr function: it displays the structure of an object.\n\n# Example 1\nx = list()\nx[[1]] = 1:10\nx[[2]] = letters[4:10]\nstr(x)\n# Example 2\nres = t.test(rnorm(30)) # do one sample t-test and save results in `res`\nstr(res) \n# You can see that the testing results are saved in a list of 10.\n# if you want to extract elements from it, the information coveryed by ´str´ is ideal.\n\nsummary function: it helps us to summarize useful information from an R objects. The information extracted depends on the type of the object. For examples\n\n# Example 1\ndat = iris[,-5] # we use the first 4 variable from iris data\nsummary(dat) # the type of ´dat´ is dataframe, then the summarized informations are...\n# Example 2\nres = t.test(rnorm(30))\nsummary(res) \n# the type of ´res´ is results of t test. The designer of this function decided \n# to show the names of all the elements in ´res´, similiar to the output of ´str´\n\nunique and table functions: they are useful when you want to check all possible values in a variable and the frequency of different possible values.\n\n# First, we create a small demo dataset\ntreatment = c(1,1,0,0,1,1,0,0)\nblock = c(1,1,1,1,2,2,2,2)\nsex = c(\"F\",\"M\",\"M\",\"M\",\"F\",\"F\",\"M\",\"F\")\nage = c(19,20,28,22,21,19,23,20)\noutcome = c(20,19,33,12,54,87,98,84)\nDat = data.frame(treatment, block, sex, age, outcome)\nhead(Data, 8)\n\n# Example 1:\nunique(Dat$sex)\ntable(Dat$sex)\nunique(Dat$age)\ntable(Dat$age)\n\n# Example 2:\ntable(Data$sex, Data$treatment) # do you know the name of the outputs?\n\nwhich function: it finds the index of elements that satisfy some conditions in a vector, or matrix, or data frame.\n\n# Example: Use the same demo data above\nwhich(Dat$sex == \"M\")\nwhich(Dat$age &lt; 21)\n\napply function: it is used to perform operations on rows or columns of matrices, data frames, or higher-dimensional arrays. It allows you to apply a function across the rows or columns without needing to use loops, making code more concise and often more efficient.\n\n# Syntax: \napply(X, margin, fun)\n# `margin` is an integer specifying whether to apply the `fun` across rows (1) or columns (2) \n# Examples: Use the demo data but ignore the variable `sex`\nDat = Dat[, -3]\napply(Dat, 2, mean)\nNext, show some useful functions for graphics. The ggplot2 package is definitely the top choice for plotting, but sometimes the following functions are more practical and convenient for data visualization. I will only list them below, and you are already strong enough to investigate them by yourself :)\n\nhist function: it can help use check the distribution of a variable.\nplot function: it is usually used to show the scatter plot of two variables.\npairs function: it shows the pairwise scatter plot of many variables.\n\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_1.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_1.html",
    "title": "2.1 Overview of R language",
    "section": "",
    "text": "R is a powerful and versatile programming language primarily used for statistical computing, data analysis, and graphical representation. Developed in the early 1990s by Ross Ihaka and Robert Gentleman at the University of Auckland, R has since evolved into a robust tool that supports a wide range of applications in various fields, including data science, bioinformatics, and social sciences.\nMain features of R:\n\nFor machine learning, data mining, and statistical analysis: R provides a comprehensive suite of statistical functions, making it ideal for conducting complex data analyses. It supports various statistical methods and techniques, including linear and nonlinear modeling, time-series analysis, classification, clustering, and machine learning. R facilitates the implementation of machine learning algorithms for predictive modeling and data mining.\nFor data Visualization: R excels at creating high-quality graphics and visualizations. With packages like ggplot2, users can generate intricate plots and charts to effectively communicate insights and findings.\nFor data Handling: R has powerful data manipulation capabilities, especially with packages like dplyr and tidyr. These tools allow for efficient data cleaning, transformation, and reshaping, making it easier to prepare datasets for analysis.\nEfficient matrix computing: R is inherently designed for matrix operations and linear algebra, making it particularly well-suited for tasks involving matrix computations. This feature allows users to perform complex mathematical calculations efficiently, which is essential in statistics and data analysis.\nCommunity Support: R has a vibrant and active community, offering extensive resources, tutorials, and forums for users to seek help and share knowledge. This community-driven approach fosters continuous improvement and innovation within the R ecosystem.\n\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_lab_home.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_lab_home.html",
    "title": "Lab 1: Exercises of R programming",
    "section": "",
    "text": "In this lab, we practice on R programming. Warm-Up, Strength Training, and Extreme Cardio exercises of varying intensities will be provided. You can choose your starting level based on your own situation. As mentioned earlier, the best way to learn any language is through practice. So, let’s get our brains and fingers moving!\nNote: Programming training is inseparable from mathematics and various algorithms. Here, we don’t require you to complete all tasks; you can decide based on your own situation. There is no time limit, you can come back to the challenge whenever you have time. The more you solve, the more you practice, and the more you master. In addition, you will also learn more knowledge in math and statistics BTW."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_lab_home.html#warm-up",
    "href": "Courses/c_mlwr1_2024/l2/l2_lab_home.html#warm-up",
    "title": "Lab 1: Exercises of R programming",
    "section": "1. Warm-Up",
    "text": "1. Warm-Up\n\nTask 1: Embark!\nInstall R-studio and R-base. Launch Rstudio, install package tidyverse. It integrates multiple packages and has revolutionary significance in data processing.\nImport a data set, FCourse.txt. You can download it here. This dataset includes students’ preference levels for different subjects. Do you have any comments? Delete the last 10 rows of the data set and save it as a txt file in your disk. The function you need is write.table.\n\n\nTask 2: A small test of skill\nTask 2.1: Write a program to calculate the sum of integer from 1 to 100. I’m not sure if you’ve heard the story about the great mathematician Gauss, who was the first pupil to finish this calculation and the first to go home for dinner in his class room. I’m sure you must be even faster than him!\nTask 2.2: After this, modify your code such that you can calculate the factorial of 100 (the product of integers from 1 to 100) by the program.\n\n\nTask 3: Some Basic Plots\nR is excellent at graphics, especially taking power of the ggplot2 package into account. We don’t have time to study this package, but will do some simple exercises.\nTask 3.1: One can visualize a math function by the following code\nx = seq(-pi,pi,0.01)\nplot(x, sin(x), type = \"l\")\nabline(h = 0)\nabline(v = 0)\nNow, it is your turn. Visualize the density function of the normal distribution with mean 5 and  sd  2.\nTask 3.2: Learn and practice the following basic plotting functions, hist, boxplot, and pie.\n\n\nTask 4: Data frame\nUsing the following code to generate the example data for this task. BTW, you may also have a closer look at the functions that you have not seen before or you are not familiar with.\n\nn = 100 # sample size\ntreatment = rbinom(n, 1, 0.5)\nblock = sample(c(1,2), n, replace = T)\nsex = sample(c(\"F\", \"M\"), n, replace = T)\nage = round( runif(n, 18, 40) )\noutcome = round( rnorm(n, 30, 10) )\nDat = data.frame(treatment, block, sex, age, outcome)\n\nTask 4.1: Sort the data set by variable age in ascending order; Filter out all the rows of female observations; Find out the values of variables age and outcome for all the rows that belongs to treatment 1 and block 2.\nTask 4.2: Randomly draw a sample with 80 observations from the data set and set them as sub-dataset 1 and the rest of them as sub-dataset 2."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_lab_home.html#strength-training",
    "href": "Courses/c_mlwr1_2024/l2/l2_lab_home.html#strength-training",
    "title": "Lab 1: Exercises of R programming",
    "section": "2. Strength Training",
    "text": "2. Strength Training\n\nTask 5: About Numbers\nIn this task, we solve some problems about numbers\nTask 5.1: Finding prime numbers Write a program to find all prime numbers up to 100. A prime number is a number that has only two factors, that is, 1 and the number itself.\nTask 5.2: Write a function that can convert a binary number to a decimal integer.\nTask 5.3: Write a function that can convert a decimal integer to a binary number.\n\n\nTask 6: Law of Large Numbers\nLaw of large numbers (LLN) is one of the foundations of probability theory. It states that as the number of trials or observations increases, the average of the results approaches the expected value. Simply put, suppose you have a fair coin. Each flip has an equal chance—50/50—of landing heads or tails. If you flip the coin repeatedly and track the cumulative proportion of heads, this proportion will get closer and closer to 0.5 as the number of flips increases.\nIn this task, we will “prove” the LLN by doing a small simulation. Let the computer mimic flipping a fair coin (generate a random number from Bernoulli distribution with \\(p=0.5\\)). Draw a graph to show that as the number of flips increases, the cumulative proportion of getting heads or tails converges to 0.5.\n\n\nTask 7: Central Limit Theorem\nThe Central Limit Theorem (CLT) is a key principle in statistics that states: for a sufficiently large sample size, the distribution of the sample mean approaches a normal (bell-shaped) distribution, regardless of the shape of the original population distribution. This means that if you take repeated random samples from any population with a finite mean and variance, the means of those samples will tend to follow a normal distribution as the sample size grows.\nIn this task, we will “demonstrate” the CLT through a simulation. We’ll repeatedly draw random samples, each with a sample size of 50, from a uniform distribution between 0 and 1. For each sample, we’ll calculate and record the average value. After repeating this process 1,000 times, plot a histogram to visualize the distribution of these 1,000 sample averages.\n\n\nTask 8: Box-Muller’s Algorithm\nWe roughly mentioned about Pseudo-random numbers in the lecture. Pseudo-random numbers are a sequence of numbers which are generated by some algorithm from an initial number, and they can mimic the behaviour of a random sample of uniform random variables. Once the pseudo uniform random number is ready, different algorithms can be applied to them to generate random numbers from certain distributions. Here, you implement Box and Muller’s algorithm to generate random numbers from an arbitrary Normal distribution. Write the implementation of this algorithm in a function, such that you can apply this function to simulate Normal random sample, just like function `rnorm``\nBox-Muller’s Algorithm\n\nStep 1: Randomly generate \\(u\\) and \\(v\\) from \\(U(0,1)\\), uniform distribution between 0 and 1\nStep 2: Set \\(x = \\sqrt{ -2 \\log (u) } \\cos (2\\pi v)\\) and \\(y = \\sqrt{ -2 \\log (u) } \\sin (2\\pi v)\\)\n\nBased on this procedure, the resulting values of \\(x\\) and \\(y\\) are independent normal distributed with mean 0 and variance 1.\nToDo4Sia: write a note to explain BM algorithm.\n\n\nTask 9: Newton Raphson Algorithm\nDo you know Newton Raphson’s optimization algorithm? The main idea of this algorithm is to find successively better approximations to the roots of a real-valued function. More specifically, assuming \\(f(x)\\) is differentiable and starting from an initial guess \\(x_0\\), the root of \\(f(x)=0\\) can be iteratively approximated as\n\\[\n  x_{n+1}=x_{n}-\\frac{f(x_{n})}{f'(x_{n})}\n\\] Now, you are required to apply this algorithm to find an approximated root of \\(x^3-x-1=0\\).\n\n\nTask 10: Bootstrap Algorithm\nThe bootstrap algorithm is a statistical technique used to estimate the distribution of a sample statistic by resampling the observed data with replacement. In essence, it involves repeatedly drawing samples (typically of the same size as the original sample) from the dataset, calculating the desired statistic (e.g., mean, median, or standard deviation) for each resample, and then aggregating these results. This method allows for estimating the variability or confidence intervals of statistics without requiring complex mathematical formulas, making it especially useful when traditional parametric assumptions (like normality) are not met. The well known machine learning algorithm random forest was just developed based on this algorithm.\nPrepare the data set: Simulate \\(x_i,i = 1,2,...,30\\) from uniform distribution \\(U(0,5)\\) by function ’runif’ and \\(\\epsilon_i,i=1,2,...,30\\) from the standard Gaussian distribution. Calculate \\(y_i =0.5+1.5x_i+\\epsilon_i\\).\nNext, we will apply bootstrap algorithm to estimate the confidence interval (CI) of the regression coefficient, and compare it with the CI calculated by formula.\nTask 10.1: Calculate the CI by formula. Employ function ’lm’ to estimate the regression model and use the output to calculate the confidence interval of the slop term of the regression model.\nTask 10.2: Calculate the CI of the estimation of slop term by Bootstrap Algorithm.\nFirst, we generate a bootstrap sample from the data. The bootstrap sample is resampled from the simulated data with a replacement. Second, estimate the regression model with the bootstrap sample and record the estimation of the slope term. Repeat the two steps 1000 times. The bootstrap confidence interval is the upper and lower quantile values of all the estimations of the slope term. Compare the results with 1). This procedure is summarized in the following pseudo algorithm\nB = 1000\nfor(i in 1:B){\n  #Step 1: draw a bootstap sample from the data set\n  #Step 2: Apply `lm` to estimate the model with the bootstrap sample\n  #Step 3: Save the estimation of slop term\n}\n# Calculate the quantile values of 1000 estimation of slop term."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_lab_home.html#extreme-cardio",
    "href": "Courses/c_mlwr1_2024/l2/l2_lab_home.html#extreme-cardio",
    "title": "Lab 1: Exercises of R programming",
    "section": "3. Extreme Cardio",
    "text": "3. Extreme Cardio\n\nTask 11: Proceptron Algorithm\nWe have discussed this algorithm in lecture 1. Now, let’s implement it in R.\nTask 11.1: Use the following code to generate the data\nset.seed(201606)\nN = 20\nx1 = runif(N,-1,1); x2 &lt;- runif(N,-1,1); X &lt;- cbind(x1,x2)\ny = ifelse(x2&gt;x1,-1,1); id &lt;- 1:N\nt = seq(-1.5,1.5,0.1)\ndat = cbind(y, x1, x2)\nHere, you can ignore the bias (constant) term in the classifier, just like what we discussed in lecture 1. Write your function and use it to find the sword of judgment!\nTask 11.2: Use the following code to get the data\n# Here, we will train a Proceptron algorithm to a subset of iris data\nX = iris[1:100,1:2]\ny = as.numeric(iris[1:100,5])\ndat = cbind(y, X)\nWrite your function and use it to find the sword of judgment!\n\n\nTask 12: Decipher Problem\nDo you know substitution cipher? A substitution cipher is a method of encryption where each letter in a text is replaced by another letter according to a specific system. For example, we use H to present D, \\(H \\to D\\), and similarly \\(A \\to T\\), and $ U A$, then the substitution cipher of word DATA is HUAU. Next, I ask you a secrete question in a text and encrypt it using a substitution cipher. My cipher is simple; it only includes the 26 lowercase letters and space. The task is to write a program that implement the algorithm illustrated in the notes to crack my cipher and answer my secrete question.\n\nMy ciphertext: download it here: download it here\nThe transition probability matrix: download it here\n\n\n\nCongratulations! You have become so powerful!\n\n\nLecture 2 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_3.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_3.html",
    "title": "2.3 Love at First Sight?",
    "section": "",
    "text": "In R, we can enter commands in the console to have the computer perform the corresponding tasks. For example, we want to print ‘welcome to R’\nprint(\"welcome to R\")\n\n[1] \"welcome to R\""
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_3.html#the-main-actor-function",
    "href": "Courses/c_mlwr1_2024/l2/l2_3.html#the-main-actor-function",
    "title": "2.3 Love at First Sight?",
    "section": "2.3.1 The main actor: function",
    "text": "2.3.1 The main actor: function\nAs a computing language specifically designed for statisticians, the most essential elements in R are functions, since almost all tasks are accomplished through various functions. Similar to most programming languages, the general form of a function in R is\nFunctionName([input 1], [input 2], ..., [argument 1], [argument 2], ...)\ni.e. it consists of a function name, inputs and arguments enclosed in parentheses. For example, you can type the following math functions in the console,\n\nsin(pi) # 'pi' is built-in constant representing the mathematical value of Pi\n\n[1] 1.224647e-16\n\nexp(1) # It returns the natural logarithm\n\n[1] 2.718282\n\nlog(10, base = 10) # The logarithm of 10 to the base 10\n\n[1] 1\n\n\nRemark: As you can see from the code above, we use # sign to comment the code, in other words, characters after # are not considered as part of the command.\nR has extensive and well-organized help documentation. You can access help for specific functions using the ? or help() function. For example:\n\n?chisq.test\n\nYou will see the help document of this function in Rstudio."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_3.html#functions-for-generating-a-sequence-of-values",
    "href": "Courses/c_mlwr1_2024/l2/l2_3.html#functions-for-generating-a-sequence-of-values",
    "title": "2.3 Love at First Sight?",
    "section": "2.3.2 Functions for generating a sequence of values",
    "text": "2.3.2 Functions for generating a sequence of values\nThe first function is c function, which can create a sequence of numbers based on your inputs and store in in memory. For example, we want to create a sequence of the integers from 1 to 10 and store it in a variable x.\nx = c(1,2,3,4,5,6,7,8,9,10)\nYou also can include characters in x by function c, for example\nx = c(letters)\n(x = c(\"I\", \"like\", \"R\", \"How about you?\"))\nRemark: In R, if you use parentheses to enclose a command, then the outputs will be printed automatically.\nTyping the integers from 1 to 10 can be done by another function seq\nseq(1,10)\nor simply by colon operator :\n1:10\nWell, the simple colon operator is neat but limited to integer sequence and increments of 1 (or -1, try 10:1 in your console). Use seq when you need more control over the sequence, for example, try the following code in your console\nseq(1, 10, 2) \nseq(1, 10, length.out = 5)\nseq(0, 1, 0.1)\nThe last function for creating sequence is rep, which can create copies of your inputs, for example\n# guess what will we get by the following commands?\nrep(1, 10)\nrep(1:4, 4)\nrep(rep(1:4,4), 2)\nrep(\"I like R\", 3)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_3.html#custom-function",
    "href": "Courses/c_mlwr1_2024/l2/l2_3.html#custom-function",
    "title": "2.3 Love at First Sight?",
    "section": "2.3.3 Custom function",
    "text": "2.3.3 Custom function\nIn R, you are allowed to encapsulate specific tasks or calculations that you can reuse throughout your code. The syntax for defining a function is\n# Syntax of custom function\nfunction_name = function(inputs, arguments) {\n  # Function body: code to execute\n  # Optionally return a value\n  return(value)\n}\nFor example, we want a function to calculate the sum of two input values\nmySum = function(x = 1, y = 1){\n    res = x+y\n    return(res)\n}\nmySum(2,3)\nmySum()\n\nIn the parentheses after the syntax key word function, the values assigned to x and y are default values, namely R will take two ones as input automatically if we don’t enter any inputs into function mySum, for example, the result of the last line above should be 2.\nIf we don’t use return to indicate the results should be returned, then the results of the last line in the function will be returned as output.\n\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_home.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_home.html",
    "title": "Lecture 2: Introduction to R Programming",
    "section": "",
    "text": "In this lecture, we provide you a genital introduction to R programming. It covers the following things:"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_home.html#lecture-notes",
    "href": "Courses/c_mlwr1_2024/l2/l2_home.html#lecture-notes",
    "title": "Lecture 2: Introduction to R Programming",
    "section": "Lecture notes:",
    "text": "Lecture notes:\n\nIntroduction to R programming:\n\nRead the integrated notes: here;\nRead the pagination notes: here;\nDownload the PDF notes: here."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_home.html#reading-guidelines-of-textbook",
    "href": "Courses/c_mlwr1_2024/l2/l2_home.html#reading-guidelines-of-textbook",
    "title": "Lecture 2: Introduction to R Programming",
    "section": "Reading Guidelines of textbook:",
    "text": "Reading Guidelines of textbook:\nFor Lecture 2, it is recommended that you read the following sections in the textbook.\n\nChapter 2: Read sections 2.3, pages 42 - 51."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_home.html#prepare-before-the-qa-session",
    "href": "Courses/c_mlwr1_2024/l2/l2_home.html#prepare-before-the-qa-session",
    "title": "Lecture 2: Introduction to R Programming",
    "section": "Prepare before the Q&A session",
    "text": "Prepare before the Q&A session\n\nRead and study the materials of lecture 1 and 2 as much as you can.\nYou may have some questions, and you’re welcome to ask me. If you prefer, you can also email me or send a message via Canvas to inquire.\nThink about the following questions：\n\nImagine an idea for a machine learning project in your field. If you’re having trouble coming up with one, you can revisit our discussion in the philosophy of machine learning section.\nIf you don’t have a good idea for question 1, learn about a potential machine learning project idea.\nBased on your answers to questions 1 or 2, think about what your target variable is, what potential feature variables you have, and whether your problem is a regression problem or a classification problem.\nConsider if you have any questions about Lectures 1 and 2."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_home.html#lab-exercises",
    "href": "Courses/c_mlwr1_2024/l2/l2_home.html#lab-exercises",
    "title": "Lecture 2: Introduction to R Programming",
    "section": "Lab exercises:",
    "text": "Lab exercises:\nLaboratory Entrance\n\nCourse Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_5_0.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_5_0.html",
    "title": "2.5 Flow Control",
    "section": "",
    "text": "Flow control refers to the mechanisms and structures used in programming to dictate the order in which instructions are executed in a program. It allows for making decisions, repeating actions, and controlling the flow of execution based on certain conditions or logic. In R and most programming languages, flow control is essential for creating dynamic and responsive programs. Here, we will mainly introduce loops and conditional statement.\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_5_3.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_5_3.html",
    "title": "2.5.3 While Loop",
    "section": "",
    "text": "Different from a for loop, a while loop continues to execute a block of code as long as a specified condition remains true. This means that the number of iterations is not predetermined; instead, it depends on the state of the condition being evaluated. The syntax is\n# Syntax of while loop\nwhile (CONDITION) {\n    # Code block to be executed\n}\nThe CONDITION could be a logical value, or a command resulting logical value. The same example of for loop above, example 4, also can be implemented through a while loop.\n# Example 7\n# Initialize the starting number\nnumber = 3\n# While loop to print numbers divisible by 3 up to 50\nwhile(number &lt;= 50){\n  print(number)      # Print the current number\n  number = number + 3  # Move to the next multiple of 3\n}\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_8.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_8.html",
    "title": "Final Words",
    "section": "",
    "text": "Alright, our short journey with R programming has come to an end. I hope you now have a basic understanding of this software, its basic operations, and various syntax. As I mentioned earlier, the goal of this lecture, or even this course, is not solely to train you in R but rather to give you a quick introduction to this simple programming language so that we can use it to study machine learning concepts later on. The R programming language is a powerful tool in the field of data science. You can continue exploring various advanced techniques afterward.\nLastly, I want to say that the best way to learn any language is to engage in conversation; the same applies to programming languages. Only by repeatedly typing commands and scripts into the computer will you truly master and become familiar with this programming language.\n\nPrevious page | Lecture 2 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_4_3.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_4_3.html",
    "title": "2.4.3 Data Frame",
    "section": "",
    "text": "As a programming language originally designed for statisticians, importing data and setting a specific structure for it is essential. It is so called data frame. In R, we can use various functions to read in different types of data, such as txt, csv, xlsx, and more. For example, you can apply read.table function to import data saved in a txt file. You can download Boston data here.\n# Prepare a txt file, 'Boston.txt', in a director\nsetwd(\"dir containing your data\") \n# we set the dir containg your data as the working director.\ndat = read.table(\"Boston.txt\", sep=\",\", header = T)\nRemark: Setting working director (WD) is always useful since it can simplify many things, for example, if we don’t set the WD as the folder containing ‘Boston.txt’, then you have to specify the dir in the first argument of the read.table function. Setting a WD can be done by function setwd, and for checking the current WD, you can use function getwd. In Rstudio, this action also can be done using mouse actions, see figure below.\n\n\n\nFirst, go to the right folder. Second, in tab ‘Files’, click the gear icon, then you will find it.\n\n\nData frame is a fundamental data structure used for storing tabular data, where each column can hold different types of data (e.g., numeric, character, or factor). Data frame can be created by function data.frame. For example:\n(X = cbind(x1,x2))\n(dat = data.frame(x1,x2)) \nclass(X)\nclass(dat) # it seems there is no difference between a matrix and a dataframe\nX%*%t(X) # try this \ndat%*%t(dat) # try this -&gt; matrix multiplication is not allowed.\nSo, usually, the operations and functions for a matrix are not allowed to apply to a data frame. Including different types of data is the main difference between data frame and matrix. For example:\n# with the same demo data above\nx3 = letters[1:3] # define another variable \nX = cbind(x1, x2, x3)\ndat = data.frame(x1, x2, x3)\nX\ndat # compare `X` and `dat`, draw yoru conclusions.\nFor a data frame, we still can use the same method as for matrix to slice. Another more practical way is using $ to slice. For examples:\n# with the same example above\ndat[,3] \ndat$x3\nSome useful functions for data frames\n\nhead and tail functions: they can help us to check the first and last few lines respectively. For examples:\n\ndat = iris # iris is a pre-stored data set in R which includes 150 iris flowers\nhead(dat)\ntail(dat)\nhead(dat, 10)\n\nnames function: it can help us quickly check the names of all variables.\nattach and detach functions: people feel very inconvenient to use $ to slice a data frame, but want to use the variable names directly. In this case, ´attach´ function can help us go into such kind of mode, and apparently detach function can cease this mode. For examples:\n\ndat = iris\nnames(iris) # [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"  \nSpecies # you can't find it\ndat$Species # works\n\nattach(dat)\nSpecies # also works\n\ndetach(dat)\nSpecies\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_4_0.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_4_0.html",
    "title": "2.4 Objects in R",
    "section": "",
    "text": "In R, data types (structure) define the nature of data that can be stored and manipulated. The main data types include numeric, character, logical, factor, array, matrix, data frame, list, function, each serving different purposes in data analysis and programming. Function class can check the type of a variable in the memory.\nNext, we list the simple types first and then illustrate more complex structures in details.\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_home.html",
    "href": "Courses/c_mlwr1_2024/l5/l5_home.html",
    "title": "Lecture 5: Regression Models",
    "section": "",
    "text": "In this lecture,\n\n\n\n\nLecture notes: Integrated notes; Pagination notes; Download the PDF notes\nReading Guidelines: You can read the book…\nR codes:\nCourse Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4.html",
    "href": "Courses/c_mlwr1_2024/l4/l4.html",
    "title": "Lecture 4: Gaussian Discriminant Analysis",
    "section": "",
    "text": "In this lecture, we discuss a simple classifier, Gaussian Discriminant Analysis (GDA). This classifier is constructed based on a probability model with Gaussian distribution.\n\nBasic Idea\nLet’s understand the basic idea of GDA from some toy examples based on simulated data. In the first example, we have simulated data from two bivariate normal distributions. The first 3 observations from each normal distribution are displayed below:\n\nhead(d1, 3) # the first three observations in group 1\n\n           [,1]        [,2]\n[1,] -0.6580041 -0.01608795\n[2,] -0.7871478  1.66187648\n[3,] -0.1271127  0.40841214\n\nhead(d2, 3) # the first three observations in group 2\n\n         [,1]     [,2]\n[1,] 4.594490 5.297224\n[2,] 4.529155 3.228726\n[3,] 4.247869 5.710223\n\n\nWe visualize all the data in a scatter plot. Observations from Group 1 and 2 are in blue and orange respectively. Also, we add a new unlabeled case \\((x_1 = 5, x_2 = 1)\\), green point, into the feature space. Question: Should be the new case predicted as orange or blue?\n\n\n\n\n\n\n\n\n\nI believe we all think the green point should be classified as an orange point. The reasoning is also very simple, because the green point is close to the center point of the orange group. More formally, the decision is made by comparing the Euclidean distance from the green point to the means of each group, i.e. \\(dist({\\color{green}\\blacksquare}, {\\color{red}\\blacktriangle})\\) V.S. \\(dist({\\color{green}\\blacksquare}, {\\color{red}\\bullet})\\).\n\n\n\n\n\n\n\n\n\nNice! Let’s see another example. In this case, there are also two groups, and a new unlabled case \\((x_1 = 5, x_2 = 0.5)\\). Question: Should be the new case predicted as orange or blue?\n\n\n\n\n\n\n\n\n\nIf we apply the same decision rule as before, then it should be classified as an orange point. However, [TODO]. What is the problem then? Why Euclidean distance doesn’t work now? We can see that the orange group has smaller variation on the \\(X_1\\) direction than the blue group. This analysis provide us an solution immediately, that is normalize the data with respect to the variance first, then apply the decision rule based on Euclidean distance. It indeed works and we can see it from the following R code.\n\n# normalize blue points and new case\nd1_normalized &lt;- t(t(d1)/apply(d1,2,sd)) # apply(d1, 2, sd) is calculating sd for each variable\nnew_blue_normalized &lt;- c(5,0.5)/apply(d1,2,sd)\n# normalize orange points and new case\nd2_normalized &lt;- t(t(d2)/apply(d2,2,sd))\nnew_orange_normalized &lt;- c(5,0.5)/apply(d2,2,sd)\n\n\n\n\n\n\n\n\n\n\nNext, we can calculate the center points of the normalized data and measure the Euclidean distance. As you can see, the normalized new point is closer to the center of blue group, therefore we should predict it as a blue point. Specially, we refer the Euclidean distance after normalization as statistical distance.\n\ncenter_blue = colMeans(d1_normalized)\ncenter_blue\n\n[1] -0.03174584  0.05009728\n\ncenter_orange = colMeans(d2_normalized)\ncenter_orange\n\n[1] 2.9329448 0.4930851\n\n\n\n\n\n\n\n\n\n\n\nNow we are in a good position to completely summarize the decision rule. Compare the statistical distance between the the observation to the center point of each group, and assign the label with a shorter statistical distance. \\[\n  \\arg \\min_{ {\\color{blue} \\bullet} \\text{or} {\\color{orange} \\blacktriangle } }\n                                 \\{ Sdist( {\\color{green} \\blacksquare }, {\\color{red} \\blacktriangle }) ,\n                                    Sdist( {\\color{green} \\blacksquare },   {\\color{red} \\bullet })) \\}\n\\] where \\(\\arg \\min_{{\\color{blue} \\bullet} \\text{or} {\\color{orange} \\blacktriangle }}\\) means the one gives the minimum in the bracket.\nRemark: You might have noticed that in the second example, we only considered the case with different variations, without taking into account the scenario where feature variables are correlated. In other words, we only considered a specific covariance matrix, which is a diagonal matrix. For a more general covariance matrix, we would need to eliminate the covariances between the feature variables in the normalization step before computing distances. However, this involves some matrix calculations, so we are skipping this scenario for now. If you are interested in this, I would recommend reading articles about Mahalanobis distance.\n\n\nGaussian Discriminant Analysis (GDA)\nSo far, we have explained the basic idea of GDA based on statistical distance. However, you may wonder why this classifier has anything to do with Normal (Gaussian) distribution. Well, do you remember what is the exact meaning of normal distribution discussed in lecture 3? If an observation is normally distributed, then its likelihood value is inversely proportional to its statistical distance from the mean. In other words, the decision can be made by comparing the likelihood value of the green point and predict the label with a higher likelihood value. Therefore the decision rule in terms of likelihood value can be represented as\n\\[\n  \\arg \\max_{{\\color{blue} \\bullet} \\text{or} {\\color{orange} \\blacktriangle }}\n  \\{\n    L( {\\color{green} \\blacksquare} | {\\color{green} \\blacksquare} \\in {\\color{orange}\\blacktriangle } ),\n    L( {\\color{green} \\blacksquare} | {\\color{green} \\blacksquare} \\in {\\color{blue} \\bullet} )\n  \\}\n\\]\nBased on the analysis, we can formally state what is GDA. First we assume that both groups follow their respective Normal distributions, i.e. \\[\n  \\begin{matrix}\n      \\textbf{x}|y=1 & \\sim & \\mathcal{N}(\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1)\\\\\n        \\textbf{x}|y=0 & \\sim & \\mathcal{N}(\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}_2)\n    \\end{matrix}\n\\] Then the decision rule is if \\(f(\\textbf{x}; \\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1) &gt; f(\\textbf{x}; \\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}_2)\\), then it is predicted as group 1, otherwise, group 2. \\(f\\) is the Normal density function (likelihood value of observation \\(\\textbf{x}\\)). According to this formulation, as long as we know these parameters, we can apply this classifier to make predictions. These parameters can be easily estimated from the data. In simple terms, we group all observations by their labels, then estimate the mean and covariance matrix of feature variables within each group. Let’s look at a simple example.\n\n\nSimple examples\nIn this section, we use ‘iris’ data to illustrate GDA. First, we import the data and include two species, ‘versicolor’ and ‘virginica’. The feature space is displayed in a pairwise scatter plot.\n\n# import data\ndata = iris[51:150,]\ndata$Species = as.factor(as.numeric(data$Species))\nlevels(data$Species) = c(\"versicolor\",\"virginica\")\npairs(data[,-5], col = data[,5], pch = 20)\n\n\n\n\n\n\n\n\nFor convincing, we split data to feature variables, \\(X\\), and target variable \\(y\\)\n\n# split data to X and Y\nX = as.matrix(data[,-5])\ny = data[,5]\n\nFor each group, we use data to estimate the parameters of GDA, i.e. means and covariance matrix.\n\n# estimate the mean vectors and covariance matrices \nmu1 = colMeans(X[1:50,])\nS1 = cov(X[1:50,])\nmu2 = colMeans(X[-(1:50),])\nS2 = cov(X[-(1:50),])\n\nNow, the model is ready and write a function as the GDA classifier. Here, we apply function ‘dmvnorm’ in ‘mvtnorm’ package to calculate the density (likelihood) value. In the end, the prediction is made by comparing the density (likelihood) value.\n\nlibrary(mvtnorm)\n# function for making decision\nclassifier = function(x,mu1,S1,mu2,S2){\n  # Here, we use function 'dmvnorm' in package 'mvtnorm'\n  ell1 = dmvnorm(x,mu1,S1)\n  ell2 = dmvnorm(x,mu2,S2)\n  res = ifelse(ell1 &gt; ell2, \"versicolor\", \"virginica\")\n  return(res)\n}\n\nIn the end, we apply our classifier to predict, compare with the truth, and calculate the accuracy of our classifier.\n\nres = numeric(100)\nfor(i in 1:100){\n  res[i] = classifier(X[i,],mu1, S1,mu2,S2)\n}\n(acc = mean(res == y)) # accuracy\n\n[1] 0.97\n\n\n\n\nDecision Boundary\nIt appears that this classifier is still quite effective. So, is GDA a linear classifier? What does its decision boundary look like? Let’s investigate these questions in this section. First, the decision boundary of this classifier can be identified as\n\\[\n  \\left\\{ \\textbf{x}: f_1(\\textbf{x})=f_2(\\textbf{x}) \\right\\}\n\\] This is a set notation, but it is not difficult to understand. The meaning is the decision boundary is a set of points satisfying the condition \\(f_1(\\textbf{x})=f_2(\\textbf{x})\\). In other words, if we can find a sufficient number of points that meet this condition, we can then connect them to obtain the decision boundary. Based on this idea, let’s find the decision boundary through some experiments.\nAnalysis: The classifier has two kind of parameters, means and covariance matrix. Mean is location parameters, so it won’t affect the shape of the decision boundary. So, we study the impact of the covariance matrix on the decision boundary.\nScenario 1: Two groups share a simple covariance structure, variance 1 and correlation 0. In the following plot, we visualize the density function of two Normal distribution.\n\n\n\n\n\n\n\n\n\nIn the plot, the color indicates the level of likelihood, then intersection point of two curve with the same color has equal likelihood with respect to each group. In other words, it is a point standing on the decision boundary. If we connect these intersection points together, we obtain the decision boundary for this GDA, and it is a straight line. Let’s relax this condition.\nScenario 2: Two groups share a common arbitrary covariance matrix.\n\n\n\n\n\n\n\n\n\nBased on the same idea, we can find the decision boundary of GDA and it is still a straight line. Let’s continue to relax this condition.\nScenario 3: Each group has an arbitrary covariance matrix.\n\n\n\n\n\n\n\n\n\nAs we can see, the decision boundary is not a straight line anymore, but a curve. Now, we can draw a conclusion. If two groups share the same covariance matrix, then the GDA is a linear classifier, and it is refer to Linear Discriminant Analysis (LDA), otherwise the decision boundary of GDA is actually a quadratic curve, and it is refer to Quadratic Discriminant Analysis (QDA).\n\n\n‘lda’ function in ‘MASS’ package\nThe LDA classifier has been implemented in ‘MASS’ package in R. Next, we continue to use ‘iris’ data for demonstration.\n\nlibrary(MASS)\nmodel = lda(Species~., data)\nmodel\n\nCall:\nlda(Species ~ ., data = data)\n\nPrior probabilities of groups:\nversicolor  virginica \n       0.5        0.5 \n\nGroup means:\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026\n\nCoefficients of linear discriminants:\n                    LD1\nSepal.Length -0.9431178\nSepal.Width  -1.4794287\nPetal.Length  1.8484510\nPetal.Width   3.2847304\n\n\nThe syntax is very simple. ‘Species~., data’ indicates we want to use all the feature variables in ‘data’ to predict variable ‘Species’. One thing we have to emphasize is ‘data’ must be type of data frame. Next, let’s understand the output of ‘lda’ function. First, ‘coefficients of linear discriminants’ is just the weights of linear classifier. Second, the bias term can be calculated as the average value of weighted sum of mean of each feature variables for each group, i.e.\n\n(sum(model$means[1,]*model$scaling) + sum(model$means[2,]*model$scaling))/2\n\n[1] 4.418986\n\n\nSo, the decision boundary of linear classifier is\n\\[\n  -0.94 X_1 -1.48 X_2 + 1.85 X_3 + 3.28 X_4 = 4.42\n\\]\nYou might have noticed ‘Prior probabilities of groups’ in the outputs, then what is prior probability here and what is the role of prior probability in a LDA classifier?\n\n\nPrior probability\nSo far, the GDA classifier can be represented as \\[\n  \\widehat{y}_{new}=\\arg\\max_{y} f(\\textbf{x}_{new}|y).\n\\] However, there is a minor flaw here, and that is not taking into account the impact of prior probabilities on predictions. To understand this, let’s think about the following question first. Suppose you tested positive for a rare disease (incidence rate is 0.01%), and we know the test is 95% accurate for patients with this disease. Do you need to worry about it? \\[\n  \\Pr(Test = +1 |Sick = 1 ) = 0.95\n\\] This conditional probability tells us that if you have this disease, the probability of testing positive is 95%. It sounds really scary, but we missed using this conditional probability. For classification, the conditional probability we should consider is how likely it is to have the disease if the test comes back positive, i.e. \\(\\Pr(Sick = 1 | Test = +1)\\). By Bayes formula, it can be calculated as \\[\n  \\Pr(Sick = 1 | Test = +1) = \\frac{\\Pr(Test = +1 |Sick = 1 ) \\Pr(Sick = 1)}{\\Pr(Test = +1)} = \\frac{0.95 \\times 0.0001}{ 0.95 \\times 0.0001 + 0.05 \\times 0.9999 }\n\\] So, the probability that you are sick under the condition of a positive testing result is very low, and you don’t worry about it since the medical test is not good enough to detect this rare disease. This example is telling us that it is more reasonable to use \\(\\Pr(Sick = 1 | Test = +1)\\) for making decision than \\(\\Pr(Test = +1 |Sick = 1 )\\). The main reason is the prior probability of the disease is unbalanced (0.0001 V.S. 0.9999). So, the prior probability matters. Let’s have look at another example:\n\n\n\n\n\n500 blue points generated from the bivariate normal distribution with mean (0,0), equal variance 1, and correlation 0. 10 orange points are observations from bivariate normal with mean (3,0), and the same covariance matrix. The green point is (2,1).\n\n\n\n\nIn this example, the green point is closer to the orange group, however, a more reasonable label is blue since we have more chance to observe a blue point than an orange point in the neighborhood of green point. Next, we will discuss how to use the prior probability to correct our prediction. First, let’s correct our decision rule. Instead of using the conditional likelihood \\(f(\\textbf{x}_{new}|y)\\), we predict the label as the one returns the largest posterior likelihood. Then the prediction should be \\[\n  \\widehat{y}=\\arg \\max_{y}\\Pr(y|\\textbf{x})\n\\]\nBy Bayes formula, the target variable is predicted as \\[\n  \\widehat{y}=\\arg \\max_{y}\\Pr(y|\\textbf{x})= \\arg \\max_{y} \\Pr(\\textbf{x}|y)\\Pr(y)\n\\] Eventually, the bias term should be corrected by \\(-\\log\\left(\\Pr(y=-1)/\\Pr(y=1)\\right)\\).\n\n\nEvaluations\nFor classification problems, the first way for evaluating the classifier is calculating the accuracy, i.e. the ratio between the number of correctly classified observations and the total number of observations. However, only use accuracy to quantify the performance of classifier is not sufficient. For example, suppose we have a dataset of 100 samples in which 95 observations are healthy people and the remaining 5 are cancer patients. We want to find a classifier based this dataset to predict the diagnosis result. One ridiculous classifier is predict everyone observation as positive case without considering any other feature variables. The accuracy of this classifier is 0.95, however, it is completely useless.\nA better way to evaluate the performance of a classifier is checking several statistics simultaneously based on the confusion matrix.\n\n\n\nConfusion Matrix\n\n\nFor example, sensitivity (or recall, True positive rate), \\(\\text{TP}/(\\text{TP}+\\text{FN})\\) and Specificity (or True negative rate), \\(\\text{TN}/(\\text{FP} + \\text{TN})\\) should be used."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html",
    "href": "Courses/c_mlwr1_2024/l3/l3.html",
    "title": "Lecture 3: Probability Theory Review",
    "section": "",
    "text": "In this lecture, let’s discuss two important preliminary knowledge in machine learning: probability theory and statistical analysis. These two subjects are inextricably linked, like an object and its shadow. There are plenty of dual concepts, for example, probability V.S. frequency, expected value V.S. average value, probability V.S. likelihood, and so on. So, I’d like to introduce them interspersed and expect that this note will serve as a guide for you to grasp probability theory and statistics through a more intuitive lens.\nBefore we start, let’s use a mathematical concept with which you are familiar with to comprehend what I refer to as the “object-shadow relationship.” Circle is a fundamental concept in geometry, but where did it come from? The process of delineating the circle likely unfolds as follows: within the tangible world, a multitude of objects exhibit analogous characteristics in their forms. Take, for instance, the full moon, ripples in a lake, a wheel, and so forth. All of these things have a “full” shape, or their shapes appear to be very consistent from all directions. Consequently, individuals distilled this observation into a succinct principle, thereby defining the circle as follows:\nCircle is a collection of points that are equal distances to a fixed point.\nThis definition aptly encompasses the majority of objects with a circular shape. Nevertheless, it’s worth noting that once mathematicians have established the circle’s definition, the notion of a perfect circle no longer finds a direct counterpart in the tangible world. This is due to the challenge of locating a precise circle-like form within the physical realm. Even if one were to stumble upon an exceptionally round iron ring, ensuring that the distances from the ring’s center remain consistently uniform presents a challenge. In essence, the concept of a perfect circle, as defined in mathematics, resides solely within the confines of our abstract realm of reason, rather than within the tangible world.\nWhile this might evoke a sense of melancholy, it underscores a fundamental connection between mathematical models and the tangible world. Mathematical models are, in essence, abstractions that represent real-world phenomena. Conversely, the real world can be approximated and understood through the lens of mathematical models. A parallel relationship exists between probability models and statistics. Grasping this interplay between the realm of reason and the physical reality, as encapsulated by mathematical models, greatly enriches our comprehension of numerous concepts within probability theory and statistics. Admittedly, I might be a touch verbose, but fortunately, we can now shift our focus to the heart of the matter."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#expected-value",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#expected-value",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Expected Value",
    "text": "Expected Value\nWe have introduced the most basic elements of probability theory, namely random variables and their distributions. So if there are two independent random variables, can we compare them? For example, suppose we have two uneven coins, and use \\(X_1 \\sim \\text{Ber}(0.4)\\) and \\(X_2 \\sim \\text{Ber}(0.6)\\) to denote the result of flipping two coins It seems that the value of \\(X_2\\) should be generally higher than the value of \\(X_1\\). However, how can I explain it more clearly in a mathematical language? We then need to introduce another important concept, the expected value of a random variable. The expected value of a random variable is very similar to the concept that you are very familiar with, that is average value. If I ask you how often do you go to IKSU per week? Then most likely, you will answer me that you go to IKSU, on average, 3 times per week, since the number of visiting IKSU (the largest sports center in Umeå) per week is not a certain number (you often go there 3 times, but not always, for example, you are sick or have an important exam to prepare). Let me show you the number of my IKSU visits in the last 10 weeks.\n\\[\n  3,5,3,2,2,4,5,3,3,4\n\\]\nWe all know that the average value can be\n\\[\n  \\frac{3+5+3+2+2+4+5+3+3+4}{10} = 3.4\n\\]\nOf course, it is a super easy calculation, but let’s have a close look at it. This calculation can be represented as\n\\[\n  \\frac{2 \\times 2 + 4\\times3 + 2\\times4 +2\\times 5 }{10} = 0.2 \\times 2 + 0.4 \\times 3 + 0.2 \\times 4 + 0.2 \\times 5 = 3.4\n\\]\nNotice that the decimal in front of each integer, the possible value, is the percentage of the corresponding value that happened in the last ten weeks. In the rational world, if you still remember it, the percentage is replaced by the probability. Therefore, the definition of expected value is defined as the weighted sum of all possible values, and the weights are the corresponding probabilities. In a mathematical notation, the expected value of a random variable is presented as \\[\n  \\text{E}(X) = \\sum_{k} k \\Pr (X = k)\n\\] We can see that the expected value of a binary distributed random variable and a binomial distributed random variable is \\(p\\) and \\(Np\\) respectively. It is a good exercise to verify it. Now, we can turn back to the question at the beginning. [TODO]\nThe expected value satisfies linearity. Suppose \\(a\\) and \\(b\\) are constant numbers and \\(X\\) is a random variable, then \\(\\text{E}(aX+b) = a \\text{E}(X) + b\\). In other words, linearity means the expectation operator and the linear operator (scalar multiplication and addition) are exchangeable."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#variance",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#variance",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Variance",
    "text": "Variance\nThe expected value can help us determine the size of the “common” value of a random variable so that we can compare two random variables. One can also compare two random variables from another dimension, which is “value stability”. For example, we have two coins, one is even and the other is so uneven that there is a very high probability, 90%, of getting Heads. Then imagine that if we flip two coins repeatedly, we will get many heads for the uneven coin and occasionally get Tails; but for the even coin, we will get the same number of heads and tails with high probability. From the perspective of taking values, the values of uneven coins are very stable, while those of uniform coins are not. The stability of a random also refers to variation. High variation means low stability. The two things can be quantified by the variance.\nThe variance of a random variable \\(X\\) is defined as \\[\n  \\text{Var}(X) = \\text{E}(X - \\text{E}(X))^2\n\\] Based on this definition, one can easily verify the variance of a binary distributed random variable and a binomial distributed random variable is \\(p(1-p)\\) and \\(np(1-p)\\) respectively. In the example above, the variance of the even coin is \\(0.25\\), but the uneven is \\(0.09\\).\nDifferent from the expected value, variance doesn’t satisfy the linearity, i.e. the variance operator and the linear operator are not exchangeable. However, it satisfies the following rules, \\[\n  \\text{Var}(aX+b) = a^2\\text{Var}(X)  \n\\]\nBased on the results above, we can easily find that a special linear combination, \\(\\left( X-\\text{E}(X) \\right)/\\sqrt{\\text{Var}(X)}\\), produces a standardized random variable, i.e. mean zero and variance 1."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#sum-rule-and-product-rule",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#sum-rule-and-product-rule",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Sum rule and product rule",
    "text": "Sum rule and product rule\nLet’s think about a question like this. Suppose there are two boxes, red and blue, and the red box contains two apples and six oranges, while the blue box contains three apples and one orange. Now we want to randomly pick a box and then randomly take out a fruit from it, then what is the probability that the fruit taken out is an apple? We can randomly pick the box by throwing a dice. If we get a number less than 5 then we choose the red box, or we choose the blue box.\n\n\n\nBox and Furits Problem\n\n\nThis example is a bit more complicated than the previous one because there are two randomized actions involved here. The result of this action involves a combination of the color of the box and the type of fruits. Let’s start by considering the probability that the red box is drawn while an apple is picked up. Again, we can use the previous formula to calculate this probability. There are six numbers corresponding to the dice, and the number of fruits inside the red box is 8, so all the possibilities are \\(6 \\times 8\\). But there are only numbers 1 through 4 and two apples, so there are only \\(4 \\times 2\\) possibilities that qualify. So the probability is \\[\n  \\frac{4\\times2}{6\\times8} = \\frac{4}{6} \\times \\frac{2}{8} = \\frac{1}{6}\n\\] It is easy to see that \\(4/6\\) is the probability of getting a number less than \\(5\\), i.e. the red box is selected. But what is the meaning of the second part, \\(2/8\\)? Since \\(8\\) is the number of fruits and \\(2\\) is the number of apples in the red box, it can be understood as the probability of getting an apple when the red box was selected. We refer to this probability as conditional probability and present it as \\(\\Pr(\\text{Apple} | \\text{Red}) = 2/8\\). This discussion can be summarized as \\[\n  \\Pr ( \\text{Red AND Apple} ) = \\Pr( \\text{Apple} | \\text{Red} ) \\Pr ( \\text{Red} )\n\\] We can also easily get the probability of getting an apple under the other possibility, i.e. \\[\n  \\Pr(\\text{Blue AND Apple}) = \\Pr(\\text{Apple} | \\text{Blue} ) \\Pr( \\text{Blue} ) = \\frac{3}{4} \\times \\frac{2}{6} = \\frac{1}{4}\n\\] “AND” corresponds to the “product rule”. \\[\n  \\Pr(E_1 \\text{ AND } E_2) = \\Pr(E_1 | E_2)\\Pr(E_2) = \\Pr(E_2 | E_1)\\Pr(E_1)\n\\] Going back to our original question, what is the probability of getting an apple? This random event can be labeled as “Red AND Apple OR Blue AND Apple”. Here, we have the second rule, i.e. “sum rule”, when considering the “OR” operator between two events that don’t happen simultaneously. \\[\n  \\Pr(E_1 \\text{ OR } E_2) = \\Pr(E_1) + \\Pr(E_2)\n\\]\nBased on the sum rule, the probability of getting an apple is calculated as \\[\n  \\Pr( \\text{Apple} ) = \\Pr( \\text{Red AND Apple} ) + \\Pr( \\text{Blue AND Apple} ) = \\frac{5}{12}\n\\]\nRemark: we can compare it with the sum-product rule in permutation and combinatorics. When there are different types of solutions for one thing, the total number of possible solutions is the sum of the number of possible solutions for each type. When there are different steps in doing one thing, the total number of solutions is the product of the number of possible solutions in each step."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#joint-distribution-and-marginal-distribution",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#joint-distribution-and-marginal-distribution",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Joint distribution and marginal distribution",
    "text": "Joint distribution and marginal distribution\nSimilarly, you can verify the probabilities when orange is considered, and summarise them in a 2 by 2 table\n\\begin{table}[h]\n    \\centering\n    \\begin{tabular}{c|c|c|c}\n         & Red & Blue &  \\\\ \\hline\n        Apple & $1/6$ & $1/4$ & $5/12$ \\\\ \\hline\n        Orange & $1/2$ & $1/12$ & $7/12$ \\\\ \\hline\n         & $2/3$ & $1/3$ & \n    \\end{tabular}\n\\end{table}\nIf we use random variables to present the events, for example, the random variable \\(X\\) presents the box selected, \\(1\\) indicates red, and \\(0\\) indicates blue; the random variable \\(Y\\) presents the fruit drew, then the table can be viewed as the joint distribution of two random variables. The last column is called the marginal distribution of random variable \\(Y\\), and the last row is the marginal distribution of random variable \\(X\\)."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#posterior-probability",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#posterior-probability",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Posterior probability",
    "text": "Posterior probability\nWith the example above, the last interesting question is “What is the probability we chose the blue box if we get an orange?”. It is a conditional probability, \\(\\Pr(X = 0 | Y = 0)\\). According to the product rule, we know that this conditional probability is the ratio between \\(\\Pr(X=0 \\text{ and } Y=0)\\) and \\(\\Pr(Y=0)\\). The first second has been calculated before and that is \\(7/12\\). Well, the first can be calculated by product rule again, i.e. \\(\\Pr( Y = 0 | X = 0 ) \\Pr(X=0)\\). Summarise, \\[\n  \\Pr(X = 0 | Y = 0) = \\frac{\\Pr( Y = 0 | X = 0 ) \\Pr(X=0)}{\\Pr(Y=0)}\n\\] This probability is referred to as the posterior probability in the sense that we use the observation in the second step to update the probability of the first step. Correspondingly, the probability of drawing a blue box at the first step is called prior probability. The formula above is well known as Bayes formula."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#statistically-independent",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#statistically-independent",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Statistically independent",
    "text": "Statistically independent\nFrom the joint distribution, the probability of drawing an apple \\(\\Pr(Y = 1)\\) is \\(5/12\\). It is different from the probability of drawing an apple under the condition that the red box was selected, \\(\\Pr(Y=1 | X = 1) = 2/8\\). This fact implies that the value of random variable \\(Y\\) depends on the value of \\(X\\), or random variable \\(X\\) and \\(Y\\) are dependent. If we add 1 apple and 11 oranges to the blue box, then \\(\\Pr(Y=1) = \\Pr(Y=1 | X = 1)\\). In this case, the value of random variable \\(Y\\) doesn’t depend on the value of \\(X\\), i.e. they are independent."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#covariance",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#covariance",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Covariance",
    "text": "Covariance\nFor two random variables \\(X\\) and \\(Y\\), we can use covariance to quantify the degress of association between two random variables. The covariance is defined as \\[\n  \\text{Cov}(X, Y) = \\text{E}(X-\\text{E}(X))(Y-\\text{E}(Y))\n\\] The mean and variance of a linear combination of two random variables are \\[\n  \\text{E}(aX+bY) = a\\text{E}(X) + b\\text{E}(Y)\n\\] and \\[\n  \\text{Var}(aX+bY) = a^2\\text{Var}(X) + 2ab\\text{Cov}(X,Y) + b^2\\text{Var}(Y)\n\\] respectively."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#continuous-distribution",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#continuous-distribution",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Continuous distribution",
    "text": "Continuous distribution\nContinuous random variables are not difficult to understand, they are nothing more than random variables that take real numbers, but the problem is how to describe their distribution. Let us still abstract mathematical concepts from reality. Let’s consider such a background problem, assuming that I have height data for all boys in middle school. Height is obviously not a categorical variable, but we can still use grouping to describe the distribution of height from a discrete perspective. Specifically, we can evenly divide the possible range of height into several groups, and then calculate the percentage of the number of people in each group to the total number of people. Yes, if you are familiar with basic statistics, you can tell that this is a histogram at a glance.\n\n\n\nHistogram\n\n\nSuch an approach has obvious flaws. For example, on the left-top of the plot, it is difficult for us to distinguish the probability of height being less than 175 and greater than 170 because these two values are combined into one group. How to do it? Very simple, we can split each large group into two small groups, and then calculate the frequency of each group to represent the distribution of height, for example, on the right-top of the plot. If we still cannot distinguish the above probability, we can continue to split each group into two groups. Doing this we can see that the histogram is more detailed. If we have a large amount of data, we can continue to subdivide the height group and know the probability that we can distinguish the above two events. Suppose we put all living, dead, and unborn men in the world into this histogram, and each group can be subdivided infinitely. We can imagine that the upper edge of the histogram will be a smooth curve. We call this smooth curve the density function and denote it as \\(f(x)\\). A valid density function has to inherit two conditions from the probability mass function of a discrete random. First, the density value must be positive, $f(x) &gt; 0, $ and the integral on the whole domain should be 1, \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\). With this function, we can calculate the probability of many events, as well as the expectation and variance. \\[\n  \\Pr(X&lt;b) = \\int_{-\\infty}^bf(x)dx\n\\] \\[\n  \\text{E}(X) =  \\int_{-\\infty}^{\\infty} xf(x) dx\n\\]\nOne can compare the formula above with the expected value of a discrete random variable, \\(\\text{E}(X) = \\sum_k k\\Pr(x=k)\\). You can see similar patterns, they all are the “sum” of all possible values times the corresponding probability or density values. Keep in mind that the integral symbol is an elongated S which indicates “sum”."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#normal-distribution",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#normal-distribution",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Normal distribution",
    "text": "Normal distribution\nA continuous random variable is Normally distributed, \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), if its density function is\n\\[\n  f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} } e^{- \\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma}\\right)^2 }\n\\] The normal distribution is determined by two parameters, location parameter \\(\\mu\\) and shape parameter \\(\\sigma^2\\). Density functions of normal distribution with different parameters are displayed in the following picture.\n\n\n\n\n\nLHS: Normal distribution with fixed shape parameter (sigma = 1) and different location parameters, orange: mu = -4, blue: mu = 0, red: mu = 2. RHS: Normal distribution with fixed location parameter (mu = 0) and different shape parameters, orange: sigma = 3, blue: sigma = 1, red: sigma = 0.5."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#likelihood-value-v.s.-probability",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#likelihood-value-v.s.-probability",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Likelihood value V.S. Probability",
    "text": "Likelihood value V.S. Probability\nWe have build up the concept of probability. In simple terms, probability is a mathematical model used to quantify the likelihood of an event. The discussion of the possibility of this event is limited to the realm of rationality. For example, we can use probability to discuss the following questions. “ Imagine you are standing in a square in the city center, close your eyes for 30 seconds, then open your eyes and catch the first man you see. How likely is his height above 180 cm?”\nLet’s have a look at another scenario, “You went downtown last weekend. You stood in the square and closed your eyes, then after 30 seconds you opened your eyes and grabbed the man you saw first and measured his height. His height is 230 and you think it is unbelievable.” In this case, we have a concrete observation of a random variable and want to evaluate the possibility of this observation. In this case, a proper word is likelihood value.\n\\begin{table}[]\n  \\begin{tabular}{c|cccc}\n  \\hline\n                       & Concept              & Object       & Discrete Variable  & Continuous Variable  \\\\  \\hline\n      Probability      & Mathematical Concept & Events       & $\\Pr(X = 1)$ p.m.f. & $\\Pr(X&lt;b) = \\int_{-\\infty}^bf(x)dx$ \\\\\n      Likelihood value & Statistical Concept  & Observations & $\\Pr(X = 1)$ p.m.f. & $f(x)$ p.d.f.              \\\\     \n  \\hline\n  \\end{tabular}\n\\end{table}"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#the-secrete-message-delivered-by-normal-density-function",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#the-secrete-message-delivered-by-normal-density-function",
    "title": "Lecture 3: Probability Theory Review",
    "section": "The secrete message delivered by normal density function",
    "text": "The secrete message delivered by normal density function\nNotice that \\(\\left(\\frac{x-u}{\\sigma}\\right)^2\\) is the normalized distance between observation \\(x\\) and the center point \\(\\mu\\). The value of density function is the likelihood value of one observation, so Normal distribution tells us that the likelihood of an observation \\(x_0\\) is inversly propotion to the normalized distance between observed value \\(x_0\\) and the mean value \\(\\mu\\)."
  },
  {
    "objectID": "MathToolBox/la/la_08.html",
    "href": "MathToolBox/la/la_08.html",
    "title": "Review and Outlook",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_18.html",
    "href": "MathToolBox/la/la_18.html",
    "title": "Orthonormal Matrix",
    "section": "",
    "text": "Previous page | LA4DS Homepage"
  },
  {
    "objectID": "MathToolBox/la/la_02.html",
    "href": "MathToolBox/la/la_02.html",
    "title": "Basic Operators",
    "section": "",
    "text": "In a space of vectors, we need to define some operations for studying the relations among vectors. In this section, we focus on two basic operations, i.e. scalar multiplication and addition, and their geometric meanings."
  },
  {
    "objectID": "MathToolBox/la/la_02.html#scalar-multiplication",
    "href": "MathToolBox/la/la_02.html#scalar-multiplication",
    "title": "Basic Operators",
    "section": "Scalar multiplication",
    "text": "Scalar multiplication\nFor a vector \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n\\right)^{\\top}\\) and a scalar \\(k\\), \\(k\\cdot \\textbf{x} = \\left( kx_1, kx_2, \\dots, kx_n\\right)^{\\top}\\). In data science, scalar multiplication is very common, the simplest example would be we want to change the units of a variable. In the Swedish men’s height example, the units is M, if we want to change it to CM, then we actually calcluate\n\\[\n  100\\times(1.78, 1.85, 1.74, 1.82, 1.90, 1.88)^{\\top}\n\\] Scalar multiplication often appears in the form of weights as well. We will see more examples in the linear combination.\nThis operation can only change the magnitude (norm) of the vector, and you can verify the following equation \\[\n  \\|k\\textbf{x}\\| = k\\|\\textbf{x}\\|\n\\] From the geometric point of view, after doing this operation, the vector will be stretched.\n\n\n\n\n\nAlso, \\(k\\textbf{x}\\) represents a line in the direction of and crossing the original point.\nGiven a vector \\(\\textbf{x}\\) there is a special scalar multiplication if set \\(k = \\|\\textbf{x}\\|^{-1}\\) since the length of the scaled vector is \\(1\\) and it is called the unit vector."
  },
  {
    "objectID": "MathToolBox/la/la_02.html#addition",
    "href": "MathToolBox/la/la_02.html#addition",
    "title": "Basic Operators",
    "section": "Addition",
    "text": "Addition\nSo far we have only considered single vectors and will consider the first operation on two vectors, addition. For vectors \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n\\right)^{\\top}\\) and \\(\\textbf{y} = \\left( y_1, y_2, \\dots, y_n\\right)^{\\top}\\), \\(\\textbf{x} + \\textbf{y} = \\left( x_1 + y_1, x_2+y_2, \\dots, x_n + y_n\\right)^{\\top}\\)\nFrom a geometric point of view, addition follows the so-called ‘parallelogram rule’:\n\nTwo vectors complete a parallelogram, then the sum of the two vectors is the directed diagonal of the parallelogram.\n\n\n\n\n\n\nThe difference between two vectors can be viewed as the first vector adding the second vector which is rescaled by \\(-1\\)\n\n\n\n\n\nThe last example essentially shows that the two vectors create the resulting purple vector through the two basic operations. ‘Create’ is the key word here, introducing the next concept, linear combination.\n\nPrevious page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_17.html",
    "href": "MathToolBox/la/la_17.html",
    "title": "Eigen Decomposition",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_home.html",
    "href": "MathToolBox/la/la_home.html",
    "title": "Linear Algebra for Data Science",
    "section": "",
    "text": "Here, I provide"
  },
  {
    "objectID": "MathToolBox/la/la_home.html#notes",
    "href": "MathToolBox/la/la_home.html#notes",
    "title": "Linear Algebra for Data Science",
    "section": "Notes:",
    "text": "Notes:\n\nIntegrated Notes\nPagination notes"
  },
  {
    "objectID": "MathToolBox/la/la_home.html#list-of-items",
    "href": "MathToolBox/la/la_home.html#list-of-items",
    "title": "Linear Algebra for Data Science",
    "section": "List of items:",
    "text": "List of items:\n\n\nVector Part:\n1. Vectors\n2. Basic Operators\n3. Linear Combination\n4. Linearly Independent\n5. Basis\n6. Inner Product\n7. Orthogonal and Projection\n8. Review and Outlook\n\nMatrix Part:\n9. Matrix, its true color\n10. Rank\n11. Multiplication\n12. Square Matrix\n13. Transpose, Inverse, Determinant, and Trace\n14. Symmetric Matrix\n15. Quadratic Form\n16. Eigen-value and Eigen-vector\n17. Eigen Decomposition\n18. Orthonormal Matrix"
  },
  {
    "objectID": "MathToolBox/la/la_01.html",
    "href": "MathToolBox/la/la_01.html",
    "title": "Vector",
    "section": "",
    "text": "What is a vector? First, a vector is an array of numbers. For example,\n\\[\n  \\textbf{x} = \\left( 38, 1, 170, 67, 2, 0 \\right).\n\\]\nConventionally, we use a lowercase letter in bold to denote a vector. There are two ways to arrange these numbers, horizontally or vertically. If we stack the numbers vertically, then I get a column vector, like this\n\\[\n  \\textbf{x} =  \\begin{pmatrix}\n38\\\\\n1\\\\\n170\\\\\n67\\\\\n2\\\\\n0\n\\end{pmatrix}\n\\] Conventionally, again, it defaults to a column vector when we say a vector. However, a column vector takes much space, therefore people place it flat as a row vector and add a little “T” in the upper right corner.\n\\[\n  \\textbf{x} = \\left( 38, 1, 170, 67, 2, 0 \\right)^{\\top}.\n\\]\nThe little “T” denotes an operation, transpose, which means changing the size of the array from (1,7) to (7,1). Thus vector is still a column vector.\nIn data science, there usually are two ways to understand a vector. First, it can be viewed as a single case with observations on several variables. We call this vector as an observation, or a case, or an example. For example, the vector above presents a 38 year old guy’s (1 indicates male) basic information. Hight is 170, weight is 67, he has two kids, and the number of pub visits per month is 0. He is probably a home guy.\nAnother example is displayed in the following 3D scatter plot. You can move your cursor to a certain point, and you’ll find a string of numbers. Each string of numbers represents a flower, and all the flowers are blooming in this 3D world, which we usually call the feature space.\n\n\n\n\n\n\nSecond, it also can be understood as a sample of a single variable. For example, \\[\n  (1.78, 1.85, 1.74, 1.82, 1.90, 1.88)^{\\top}\n\\] it is a sample of height of audlt Swedish men."
  },
  {
    "objectID": "MathToolBox/la/la_01.html#array-view",
    "href": "MathToolBox/la/la_01.html#array-view",
    "title": "Vector",
    "section": "",
    "text": "What is a vector? First, a vector is an array of numbers. For example,\n\\[\n  \\textbf{x} = \\left( 38, 1, 170, 67, 2, 0 \\right).\n\\]\nConventionally, we use a lowercase letter in bold to denote a vector. There are two ways to arrange these numbers, horizontally or vertically. If we stack the numbers vertically, then I get a column vector, like this\n\\[\n  \\textbf{x} =  \\begin{pmatrix}\n38\\\\\n1\\\\\n170\\\\\n67\\\\\n2\\\\\n0\n\\end{pmatrix}\n\\] Conventionally, again, it defaults to a column vector when we say a vector. However, a column vector takes much space, therefore people place it flat as a row vector and add a little “T” in the upper right corner.\n\\[\n  \\textbf{x} = \\left( 38, 1, 170, 67, 2, 0 \\right)^{\\top}.\n\\]\nThe little “T” denotes an operation, transpose, which means changing the size of the array from (1,7) to (7,1). Thus vector is still a column vector.\nIn data science, there usually are two ways to understand a vector. First, it can be viewed as a single case with observations on several variables. We call this vector as an observation, or a case, or an example. For example, the vector above presents a 38 year old guy’s (1 indicates male) basic information. Hight is 170, weight is 67, he has two kids, and the number of pub visits per month is 0. He is probably a home guy.\nAnother example is displayed in the following 3D scatter plot. You can move your cursor to a certain point, and you’ll find a string of numbers. Each string of numbers represents a flower, and all the flowers are blooming in this 3D world, which we usually call the feature space.\n\n\n\n\n\n\nSecond, it also can be understood as a sample of a single variable. For example, \\[\n  (1.78, 1.85, 1.74, 1.82, 1.90, 1.88)^{\\top}\n\\] it is a sample of height of audlt Swedish men."
  },
  {
    "objectID": "MathToolBox/la/la_01.html#geometry-view",
    "href": "MathToolBox/la/la_01.html#geometry-view",
    "title": "Vector",
    "section": "Geometry View",
    "text": "Geometry View\nAnother understanding is to understand a vector from a geometric point of view as a directed line segment starting from the origin in the coordinate space and ending at the point of this string of numbers in the space. Here is an example of a vector in a 2D space.\n\n\n\n\n\nThis way of understanding allows us to learn linear algebra intuitively from a geometric point of view. For example, we can quantify a vector from two geometric perspectives, i.e. magnitude and direction. The magnitude of a vector can be quantified by the length of the corresponding arrow and we call it as norm. For a general vector \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n \\right)^{\\top}\\), its norm is \\(\\|\\textbf{x}\\|=\\sqrt{ \\sum_{i=1}^n x^2_i }\\).\n\n\n\n\n\nDirection is another important characteristic of a vector. The angle between the vector and x-axes can determine it. How can the angle be quantified? We will return to it when discussing an important operator in linear algebra, the inner product.\n\n\n\n\n\nLet’s go ahead and study more from a geometric point of view.\n\nPrevious page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_14.html",
    "href": "MathToolBox/la/la_14.html",
    "title": "Symmetric Matrix",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_04.html",
    "href": "MathToolBox/la/la_04.html",
    "title": "Linear Independent",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_11.html",
    "href": "MathToolBox/la/la_11.html",
    "title": "Multiplication",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_13.html",
    "href": "MathToolBox/la/la_13.html",
    "title": "Transpose, Inverse, Determinant, and Trace",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_06.html",
    "href": "MathToolBox/la/la_06.html",
    "title": "Inner Product",
    "section": "",
    "text": "So far, we have defined one operation on two vectors, i.e. addition, and can see the power of geometry. In order to use more intuitive geometric ideas, we need to introduce another operation on two vectors, that is the inner product."
  },
  {
    "objectID": "MathToolBox/la/la_06.html#inner-product-at-first-sight",
    "href": "MathToolBox/la/la_06.html#inner-product-at-first-sight",
    "title": "Inner Product",
    "section": "Inner product, at first sight",
    "text": "Inner product, at first sight\nThe inner product is also well known as the dot product. Here we use the inner product notations. For two vectors \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n\\right)^{\\top}\\) and \\(\\textbf{y} = \\left( y_1, y_2, \\dots, y_n\\right)^{\\top},\\) the inner product is \\(\\langle \\textbf{x}, \\textbf{y} \\rangle = \\textbf{x}^{\\top}\\textbf{y} = \\sum_{i=1}^n x_iy_i\\). In textbooks in certain disciplines, the inner product is also represented as \\(\\textbf{x} \\cdot \\textbf{y}\\).\n\n\n\n\n\nRemark 1: Since the inner product of two vectors is a scalar, the result doesn’t depend on the order of two vectors.\n\n\n\n\n\nRemark 2: A special case is the inner product of a vector and itself. We actually have seen it before and it is the square of norm of this vector."
  },
  {
    "objectID": "MathToolBox/la/la_06.html#inner-product-the-geometric-view",
    "href": "MathToolBox/la/la_06.html#inner-product-the-geometric-view",
    "title": "Inner Product",
    "section": "Inner product, the geometric view",
    "text": "Inner product, the geometric view\nIn a 2D space, two non-overlapping vectors can form a triangle. Based on the cosine theorem, one can show that the inner product of two vectors is equal to the cosine value of the angle between two vectors times the product of the length of two vectors, i.e.\n\\[\n  \\langle \\textbf{x}, \\textbf{y} \\rangle =  \\textbf{x}^{\\top}\\textbf{y} = cos(\\theta)\\cdot ||\\textbf{x} ||||\\textbf{y}||\n\\]\nIf the two vectors all are unit vectors, then the inner product is equal to the cosine value of the angle between the two vectors. In one word, the inner product of two vectors is proportional to the cosine value of the angle between two vectors, i.e. \\(\\langle \\textbf{x}, \\textbf{y} \\rangle =  \\textbf{x}^{\\top}\\textbf{y} \\propto   cos(\\theta).\\) Now, the connection between geometry and algebra has been established.\n\n\n\n\n\nBased on this geometrical idea, one may have realized that the inner product quantifies the similarity of two vectors since the angle indicates if the two vectors are close to each other. This idea is very important in data science and it has been applied in a proptotpye algorithm in machine learning, that is the well-known algorithm, perceptron algorithm. In statistics, one famous statistic is just based on the inner product. Do you know what is that?\n\n\n\n\n\nWe have seen that there is a strong connection between inner product and the angle between two vectors. In our real life and mathematics, an angle of 90 degrees is special and the most useful one and it leads to the next concept, orthogonal.\n\nPrevious page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "04_about_me.html",
    "href": "04_about_me.html",
    "title": "About me",
    "section": "",
    "text": "My name is Xijia Liu (刘西嘉) or Sia.\n\n\n\nTL: I am originally from China. This is the landmark of my hometown, Beijing-the temple of heaven. This is the place where I played as a child. My parents still live near the Heaven temple park. BL: In Sweden, people usually pronounce my name as ‘Sia’. I love this somewhat sweet nickname because it’s also the name of a popular ice cream brand in the Nordics. TM: This is the landmark of Uppsala, Uppsala Cathedral, which is said to be the largest Eastern Orthodox church in the Nordic countries. I spent five wonderful years in Uppsala. The campus culture and the colorful autumn left a deep impression on me. BM: This is the emblem of Uppsala University. I completed my Ph.D. study in statistics at Uppsala University. TR: Currently, I live in Umeå with my wife and two naughty boys. This is a vibrant city; the Northern Lights are a common sight; it’s a paradise for winter sports enthusiasts. BR: I work as a senior lecturer at the Department of Statistics of Umeå University.\n\n\n\nAcademic Interests:\n\nMultivariate data analysis\nMachine Learning\nFunction data analysis\nStatistical modelling in general\n\nHobbies:\n\nWeiqi: its English name is Go. An ancient board game from China. With only two rules, you can build your universe with black and white stones. I really enjoy playing Go, but my level isn’t very high, just an amateur 2-dan.\nBadminton: In recent years, I have started playing badminton. After several training sessions with my son, I fell in love with this sport. Its charm lies in the fact that on the court, you need both abundant physical energy and a clear mind."
  },
  {
    "objectID": "02_m_index.html",
    "href": "02_m_index.html",
    "title": "Mímisbrunnr",
    "section": "",
    "text": "On this page, you can see the mathematical concepts and tools I’ve collected for my courses. You can think of it as a small dictionary of math for data science. In different courses, if needed, you will be directed to a specific item to refresh or learn it.\n\n\nLinear Algebra\nProbability\nCalculus and Optimization\nAdvance Probability Theory\n\n\n\n\n\nMímisbrunnr, or Mimir’s Well, is a significant source of wisdom in Norse mythology. It is located beneath one of the roots of Yggdrasil, the World Tree, and is guarded by Mimir, a figure renowned for his vast knowledge and wisdom. The well contains immense wisdom, and it is said that those who drink from it gain great knowledge and understanding. Odin drinks from Mímisbrunnr, he sacrificed one of his eyes to the wellspring in exchange for a drink. Odin tells us with his actions that there’s truly no free lunch, or, in other words, no pain no gain. Source: Wikipidia."
  },
  {
    "objectID": "MathToolBox/la/la_12.html",
    "href": "MathToolBox/la/la_12.html",
    "title": "Square Matrix",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_07.html",
    "href": "MathToolBox/la/la_07.html",
    "title": "Orthogonal and Projection",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_05.html",
    "href": "MathToolBox/la/la_05.html",
    "title": "Basis",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_10.html",
    "href": "MathToolBox/la/la_10.html",
    "title": "Rank",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_00.html",
    "href": "MathToolBox/la/la_00.html",
    "title": "Linear Algebra for Data Science",
    "section": "",
    "text": "LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_15.html",
    "href": "MathToolBox/la/la_15.html",
    "title": "Quadratic Form",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_03.html",
    "href": "MathToolBox/la/la_03.html",
    "title": "Linear Combination",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_16.html",
    "href": "MathToolBox/la/la_16.html",
    "title": "Eigen-value and Eigen-vector",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la.html",
    "href": "MathToolBox/la/la.html",
    "title": "Linear Algebra for Data Science",
    "section": "",
    "text": "Linear algebra is a basic tool in many disciplines, and multivariate statistical analysis is inseparable from it. If you asked me to give this course another name, I think it would be “Applied Linear Algebra”. In this section, I will provide you with a comprehensive review combining some statistical ideas. In addition, I believe the best way of learning linear algebra is through geometry, so many animations are applied to visualize the geometry. I hope it can help you recall everything and even have a deeper understanding of linear algebra.\nBefore we start, I have one question for you. Who is the protagonist of linear algebra? You may think it is the Matrix since usually we start from many complicated calculations on matrices in most linear algebra courses. However, I don’t think so and believe the leading man in linear algebra is Vector!"
  },
  {
    "objectID": "MathToolBox/la/la.html#vector",
    "href": "MathToolBox/la/la.html#vector",
    "title": "Linear Algebra for Data Science",
    "section": "1.1 Vector",
    "text": "1.1 Vector\n\nArray View:\nWhat is a vector? First, a vector is an array of numbers. For example,\n\\[\n  \\textbf{x} = \\left( 38, 1, 170, 67, 2, 0 \\right).\n\\]\nConventionally, we use a lowercase letter in bold to denote a vector. There are two ways to arrange these numbers, horizontally or vertically. If we stack the numbers vertically, then I get a column vector, like this\n\\[\n  \\textbf{x} =  \\begin{pmatrix}\n38\\\\\n1\\\\\n170\\\\\n67\\\\\n2\\\\\n0\n\\end{pmatrix}\n\\] Conventionally, again, it defaults to a column vector when we say a vector. However, a column vector takes much space, therefore people place it flat as a row vector and add a little “T” in the upper right corner.\n\\[\n  \\textbf{x} = \\left( 38, 1, 170, 67, 2, 0 \\right)^{\\top}.\n\\]\nThe little “T” denotes an operation, transpose, which means changing the size of the array from (1,7) to (7,1). Thus vector is still a column vector.\nIn data science, there usually are two ways to understand a vector. First, it can be viewed as a single case with observations on several variables. We call this vector as an observation, or a case, or an example. For example, the vector above presents a 38 year old guy’s (1 indicates male) basic information. Hight is 170, weight is 67, he has two kids, and the number of pub visits per month is 0. He is probably a home guy.\nAnother example is displayed in the following 3D scatter plot. You can move your cursor to a certain point, and you’ll find a string of numbers. Each string of numbers represents a flower, and all the flowers are blooming in this 3D world, which we usually call the feature space.\n\n\n\n\n\n\nSecond, it also can be understood as a sample of a single variable. For example, \\[\n  (1.78, 1.85, 1.74, 1.82, 1.90, 1.88)^{\\top}\n\\] it is a sample of height of audlt Swedish men.\n\n\nGeometry View\nAnother understanding is to understand a vector from a geometric point of view as a directed line segment starting from the origin in the coordinate space and ending at the point of this string of numbers in the space. Here is an example of a vector in a 2D space.\n\n\n\n\n\nThis way of understanding allows us to learn linear algebra intuitively from a geometric point of view. For example, we can quantify a vector from two geometric perspectives, i.e. magnitude and direction. The magnitude of a vector can be quantified by the length of the corresponding arrow and we call it as norm. For a general vector \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n \\right)^{\\top}\\), its norm is \\(\\|\\textbf{x}\\|=\\sqrt{ \\sum_{i=1}^n x^2_i }\\).\n\n\n\n\n\nDirection is another important characteristic of a vector. The angle between the vector and x-axes can determine it. How can the angle be quantified? We will return to it when discussing an important operator in linear algebra, the inner product.\n\n\n\n\n\nLet’s go ahead and study more from a geometric point of view."
  },
  {
    "objectID": "MathToolBox/la/la.html#basic-operators",
    "href": "MathToolBox/la/la.html#basic-operators",
    "title": "Linear Algebra for Data Science",
    "section": "1.2 Basic Operators",
    "text": "1.2 Basic Operators\nIn a space of vectors, we need to define some operations for studying the relations among vectors. In this section, we focus on two basic operations, i.e. scalar multiplication and addition, and their geometric meanings.\n\nScalar multiplication\nFor a vector \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n\\right)^{\\top}\\) and a scalar \\(k\\), \\(k\\cdot \\textbf{x} = \\left( kx_1, kx_2, \\dots, kx_n\\right)^{\\top}\\). In data science, scalar multiplication is very common, the simplest example would be we want to change the units of a variable. In the Swedish men’s height example, the units is M, if we want to change it to CM, then we actually calcluate\n\\[\n  100\\times(1.78, 1.85, 1.74, 1.82, 1.90, 1.88)^{\\top}\n\\] Scalar multiplication often appears in the form of weights as well. We will see more examples in the linear combination.\nThis operation can only change the magnitude (norm) of the vector, and you can verify the following equation \\[\n  \\|k\\textbf{x}\\| = k\\|\\textbf{x}\\|\n\\] From the geometric point of view, after doing this operation, the vector will be stretched.\n\n\n\n\n\nAlso, \\(k\\textbf{x}\\) represents a line in the direction of and crossing the original point.\nGiven a vector \\(\\textbf{x}\\) there is a special scalar multiplication if set \\(k = \\|\\textbf{x}\\|^{-1}\\) since the length of the scaled vector is \\(1\\) and it is called the unit vector.\n\n\n\n\n\n\n\nAddition\nSo far we have only considered single vectors and will consider the first operation on two vectors, addition. For vectors \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n\\right)^{\\top}\\) and \\(\\textbf{y} = \\left( y_1, y_2, \\dots, y_n\\right)^{\\top}\\), \\(\\textbf{x} + \\textbf{y} = \\left( x_1 + y_1, x_2+y_2, \\dots, x_n + y_n\\right)^{\\top}\\)\nFrom a geometric point of view, addition follows the so-called ‘parallelogram rule’:\n\nTwo vectors complete a parallelogram, then the sum of the two vectors is the directed diagonal of the parallelogram.\n\n\n\n\n\n\nThe difference between two vectors can be viewed as the first vector adding the second vector which is rescaled by \\(-1\\)\n\n\n\n\n\nThe last example essentially shows that the two vectors create the resulting purple vector through the two basic operations. ‘Create’ is the key word here, introducing the next concept, linear combination."
  },
  {
    "objectID": "MathToolBox/la/la.html#linear-combination",
    "href": "MathToolBox/la/la.html#linear-combination",
    "title": "Linear Algebra for Data Science",
    "section": "1.3 Linear Combination",
    "text": "1.3 Linear Combination\nInspired by subtraction, it is obvious that if one changes the coefficients, then we will end up with another vector.\n\n\n\n\n\nInversely, we also can find two proper coefficients such that an arbitrary vector (purple) can be represented as the sum of two given vectors (red and blue).\n\n\n\n\n\nSo, the linear combination of two vectors is \\(c_1\\textbf{x}_1 + c_2\\textbf{x}_2.\\) In general, the linear combination can be defined as the weighted sum of a group of vectors. For \\(\\textbf{x}_1, \\textbf{x}_2, \\dots, \\textbf{x}_n\\), and n scalars \\(c_1, c_2, \\dots, c_n\\), the linear combination is presented as \\(\\sum_{i=1}^n c_i\\textbf{x}_i\\).\nIt seems that we can create all the vectors as long as hold two vectors in a 2D space. However, that is not always true."
  },
  {
    "objectID": "MathToolBox/la/la.html#linearly-independent",
    "href": "MathToolBox/la/la.html#linearly-independent",
    "title": "Linear Algebra for Data Science",
    "section": "1.4 Linearly Independent",
    "text": "1.4 Linearly Independent\nNext, I will show you one simple counter-example. Considering two vectors pointing in the same direction, we know that one vector can be represented as another vector scaled by some scalar. Therefore, the linear combination of the two vectors is equivalent to a scalar multiplication of arbitrary one vector. In this case, the resulting vector only stays in one direction or presents a line but not the 2D plane. Thus, we can’t create arbitrary vectors by the two vectors through linear combinations. It is also the case when the two vectors point in opposite directions.\n\n\n\n\n\nIn simple words, for two overlapping vectors, we can only create a new vector in the direction of two vectors. In a mathematical language, we can use a linear combination of two vectors with non-zero coefficients to get the zero vector. In this case, we say the two vectors are linearly dependent.\n\n\n\n\n\nOppositely, for two non-overlapping vectors, we can’t use a linear combination of them to create the zero vector unless the coefficients all are zeros, and we say the two vectors are linearly independent.\n\n\n\n\n\nThis definition can be extended to a more general scenario and leads to the general definition of linear dependence.\n\n\n\n\n\n\nFor 2D space, we can maximumly have two linearly independent vectors. For k-D space, we can maximumly have k linearly independent vectors. Is that true?\n\nGreat! It is a good time to introduce the next concept, basis, which is not only important in linear algebra but also in data analysis."
  },
  {
    "objectID": "MathToolBox/la/la.html#basis",
    "href": "MathToolBox/la/la.html#basis",
    "title": "Linear Algebra for Data Science",
    "section": "1.5 Basis",
    "text": "1.5 Basis\nBefore, we emphasized a keyword, ‘create’, and say an arbitrary vector can be created by two non-overlapping vectors in 2D space. Now, I want to replace this keyword with ‘represented’, maybe a more formal one. With this new keyword, we move our attention from “resulting vector” to “the two original linearly independent vectors”. We call them a set of basis in the sense that they are the backbone of the space constituted by all possible vectors.\n\n\n\nI have a metaphor that may not particularly fit. The basis is like the various departments under the student union of a university, such as the study department, the culture and sports department, the life department, and so on. The students in these departments are by no means all students in this university, but they can represent all students very well. If a student is dissatisfied with the university, it is best not to quarrel with the dean alone. The smartest way is to negotiate with the institute through the student union. Keep this in mind! Creator: Alejandro A. Alvarez | Credit: Alejandro A. Alvarez / Staff Photographer\n\n\nLet’s summarize. In a 2D space, a pair of non-overlapped vectors can be a basis. So the angle between the two vectors is very essential. Then how do we deal with angles in linear algebra? Another question is can we find a better basis in the sense that the coefficients of linear combination can be easily obtained? These questions lead to the next important operation, the inner product."
  },
  {
    "objectID": "MathToolBox/la/la.html#inner-product",
    "href": "MathToolBox/la/la.html#inner-product",
    "title": "Linear Algebra for Data Science",
    "section": "1.6 Inner Product",
    "text": "1.6 Inner Product\nSo far, we have defined one operation on two vectors, i.e. addition, and can see the power of geometry. In order to use more intuitive geometric ideas, we need to introduce another operation on two vectors, that is the inner product.\n\nInner product, at first sight\nThe inner product is also well known as the dot product. Here we use the inner product notations. For two vectors \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n\\right)^{\\top}\\) and \\(\\textbf{y} = \\left( y_1, y_2, \\dots, y_n\\right)^{\\top},\\) the inner product is \\(\\langle \\textbf{x}, \\textbf{y} \\rangle = \\textbf{x}^{\\top}\\textbf{y} = \\sum_{i=1}^n x_iy_i\\). In textbooks in certain disciplines, the inner product is also represented as \\(\\textbf{x} \\cdot \\textbf{y}\\).\n\n\n\n\n\nRemark 1: Since the inner product of two vectors is a scalar, the result doesn’t depend on the order of two vectors.\n\n\n\n\n\nRemark 2: A special case is the inner product of a vector and itself. We actually have seen it before and it is the square of norm of this vector.\n\n\n\n\n\n\n\nInner product, the geometric view\nIn a 2D space, two non-overlapping vectors can form a triangle. Based on the cosine theorem, one can show that the inner product of two vectors is equal to the cosine value of the angle between two vectors times the product of the length of two vectors, i.e.\n\\[\n  \\langle \\textbf{x}, \\textbf{y} \\rangle =  \\textbf{x}^{\\top}\\textbf{y} = cos(\\theta)\\cdot ||\\textbf{x} ||||\\textbf{y}||\n\\]\nIf the two vectors all are unit vectors, then the inner product is equal to the cosine value of the angle between the two vectors. In one word, the inner product of two vectors is proportional to the cosine value of the angle between two vectors, i.e. \\(\\langle \\textbf{x}, \\textbf{y} \\rangle =  \\textbf{x}^{\\top}\\textbf{y} \\propto   cos(\\theta).\\) Now, the connection between geometry and algebra has been established.\n\n\n\n\n\nBased on this geometrical idea, one may have realized that the inner product quantifies the similarity of two vectors since the angle indicates if the two vectors are close to each other. This idea is very important in data science and it has been applied in a proptotpye algorithm in machine learning, that is the well-known algorithm, perceptron algorithm. In statistics, one famous statistic is just based on the inner product. Do you know what is that?\n\n\n\n\n\nWe have seen that there is a strong connection between inner product and the angle between two vectors. In our real life and mathematics, an angle of 90 degrees is special and the most useful one and it leads to the next concept, orthogonal."
  },
  {
    "objectID": "MathToolBox/la/la.html#orthogonal-and-projection",
    "href": "MathToolBox/la/la.html#orthogonal-and-projection",
    "title": "Linear Algebra for Data Science",
    "section": "1.7 Orthogonal and Projection",
    "text": "1.7 Orthogonal and Projection\n\nOrthogonal\nIn linear algebra, if the angle between two vectors is 90 degrees, then we say they are orthogonal.\nRecall your knowledge in high school, the cosine value of 90 degrees is 0. This fact provides a simple method to determine whether two vectors are orthogonal, i.e. two vectors are orthogonal if and only if their inner product is zero.\n\n\n\n\n\nOrthogonal is an important concept in many ways. In statistics, there is a simple and good example. From the previous section, we learned that the correlation between two variables is just the cosine value of the angle between two variables. So if two variables are orthogonal, then they are also uncorrelated. Another reason is that the concept of orthogonal leads to the next important concept, projection.\n\n\nProjection\nA projection can be viewed as a vector created by two vectors through the following procedure. Suppose we have two non-overlapping vectors \\(\\textbf{x}\\) and \\(\\textbf{y}\\), a beam of light is incident from a direction perpendicular to \\(\\textbf{y}\\) cause a shadow of \\(\\textbf{x}\\) on \\(\\textbf{y}\\) and the shadow is called the projection of \\(\\textbf{x}\\) onto \\(\\textbf{y}\\).\n\n\n\n\n\nOne interesting problem is how to represent the shadow \\(\\textbf{x}_1\\) by the two existing vectors. You may have realized that \\(\\textbf{x}_1\\) overlaps with vector \\(\\textbf{y}\\), therefore \\(\\textbf{x}_1\\) should be scaled \\(\\textbf{y}\\), i.e. \\(\\textbf{x}_1 = k \\cdot \\textbf{y}\\). So the problem becomes finding the coefficient \\(k\\). Based on the orthogonal properties, we can find that\n\\[\n  k = \\frac{\\textbf{x}^{\\top} \\textbf{y}}{\\textbf{y}^{\\top} \\textbf{y}} \\text{, and }  \\textbf{x}_1 = \\frac{\\textbf{x}^{\\top} \\textbf{y}}{\\textbf{y}^{\\top} \\textbf{y}} \\textbf{y}.\n\\] through the following derivations.\n\n\n\n\n\nRemark: The scalar \\(k\\) is just the length of projection and it is called scalar projection. One can go further and find that the scalar projection is obtained by the inner product of \\(\\textbf{x}\\) and the unit vector in \\(\\textbf{y}\\) direction.\n\n\n\n\n\n\n\nOrthonormal Basis\nIn 2D space, there is a pair of two special and nice unit vectors, \\((1,0)^{\\top}\\) and \\((0,1)^{\\top}\\). One can easily get the scalar projection of arbitrary vectors on them, isn’t it? Obviously, they also form a basis of 2D space. This basis is very nice since they are not only unit vectors but also orthogonal to each other. We call this kind of basis an orthonormal basis."
  },
  {
    "objectID": "MathToolBox/la/la.html#review-and-outlook",
    "href": "MathToolBox/la/la.html#review-and-outlook",
    "title": "Linear Algebra for Data Science",
    "section": "1.8 Review and Outlook",
    "text": "1.8 Review and Outlook\nLet’s review linear combinations in 2D space from a view of the inner product.\n\\[\n  \\textbf{y} = x_1 \\cdot \\textbf{a}_1 + x_2 \\cdot \\textbf{a}_2\n\\]\nThe vector \\(\\textbf{y}\\) is represented as the linear combination of the two basis vectors \\(\\textbf{a}_1, \\textbf{a}_2\\). It can be reviewed as the inner product of two “vectors”, one is the vector of coefficients, \\((x_1, x_2)^{\\top}\\), that we are familiar with, and another is a vector of two vectors, \\((\\textbf{a}_1, \\textbf{a}_2)^{\\top}\\). Based on this thinking, we can rewrite it as\n\\[\n  \\textbf{y} = (\\textbf{a}_1, \\textbf{a}_2) \\begin{pmatrix}\nx_1\\\\\nx_2\n\\end{pmatrix}\n\\]\nLet’s focus on \\((\\textbf{a}_1, \\textbf{a}_2)\\). It is a 2 by 2 rectangle array and we denote it by a bold capital letter, \\(\\textbf{A}\\), and name it matrix!"
  },
  {
    "objectID": "MathToolBox/la/la.html#matrix-its-true-colors",
    "href": "MathToolBox/la/la.html#matrix-its-true-colors",
    "title": "Linear Algebra for Data Science",
    "section": "2.1 Matrix, its true colors",
    "text": "2.1 Matrix, its true colors\n\nMatrix, at first sight\nThe most straightforward definition of a matrix is the rectangle array. One needs to use two numbers to describe the size (shape) of a matrix, the numbers of rows and columns. In the following example, the matrix consists of 3 rows and 4 columns. We say it is a 3 by 4 matrix. One neat but informative notation is \\(\\textbf{A} = \\left\\{ a_{ij} \\right\\}_{3 \\times 4}\\). In this course, keep in mind, we always use \\(i\\) indicates for the row index and \\(j\\) for column index.\n\n\n\n\n\nYou may remember the idea in the previous section that a matrix can be viewed as a “row vector” of column vectors. In this example, suppose we can find 3 linearly independent vectors from the 4 column vectors, then they can be viewed as a basis and generate a space. Similarly, the matrix also can be viewed as a “column vector” of row vectors, and they also can generate a space if they are linearly independent. In statistics, especially MDA, we mainly work with a data matrix. Let’s talk about it later on.\n\n\n\n\n\n\n\nMatrix, true colors\nThe definition of the matrix above is very straightforward and very helpful when we understand a data matrix in MDA. However, the disadvantage of this definition is that it is too dry, and we cannot understand it from a functional point of view as well as a geometric point of view. Let’s take a look at what its true color. Let’s turn back to the concept of ‘linear combination’ and watch the following animation.\n\n\n\n\n\nFrom this animation, we can see that the \\(n \\times p\\) matrix \\(\\textbf{A}\\) determines a map (function) from the \\(\\mathbb{R}^p\\) space to the \\(\\mathbb{R}^n\\) space. We call this map or function a linear transformation. In some sense, the matrix can be viewed as a bridge between two spaces.\nRemark: we also want to emphasize that the meaning of an action that pre-multiply a vector by a matrix, \\(\\textbf{Ax}\\), even though we have not defined matrix multiplication yet. Keep in mind that this action means that one uses the elements of \\(\\textbf{x}\\) as coefficients to calculate the linear combination of column vectors of the matrix \\(\\textbf{A}\\). It is very helpful when we discuss the next important concept, the rank of a matrix."
  },
  {
    "objectID": "MathToolBox/la/la.html#rank",
    "href": "MathToolBox/la/la.html#rank",
    "title": "Linear Algebra for Data Science",
    "section": "2.2 Rank",
    "text": "2.2 Rank"
  },
  {
    "objectID": "MathToolBox/la/la.html#matrix-multiplication",
    "href": "MathToolBox/la/la.html#matrix-multiplication",
    "title": "Linear Algebra for Data Science",
    "section": "2.3 Matrix Multiplication",
    "text": "2.3 Matrix Multiplication"
  },
  {
    "objectID": "MathToolBox/la/la.html#square-matrix",
    "href": "MathToolBox/la/la.html#square-matrix",
    "title": "Linear Algebra for Data Science",
    "section": "2.4 Square Matrix",
    "text": "2.4 Square Matrix"
  },
  {
    "objectID": "MathToolBox/la/la.html#transpose-inverse-determinant-and-trace",
    "href": "MathToolBox/la/la.html#transpose-inverse-determinant-and-trace",
    "title": "Linear Algebra for Data Science",
    "section": "2.5 Transpose, Inverse, Determinant, and Trace",
    "text": "2.5 Transpose, Inverse, Determinant, and Trace"
  },
  {
    "objectID": "MathToolBox/la/la.html#symmetric-matrix",
    "href": "MathToolBox/la/la.html#symmetric-matrix",
    "title": "Linear Algebra for Data Science",
    "section": "2.6 Symmetric Matrix",
    "text": "2.6 Symmetric Matrix"
  },
  {
    "objectID": "MathToolBox/la/la.html#quadratic-form",
    "href": "MathToolBox/la/la.html#quadratic-form",
    "title": "Linear Algebra for Data Science",
    "section": "2.7 Quadratic Form",
    "text": "2.7 Quadratic Form"
  },
  {
    "objectID": "MathToolBox/la/la.html#eigen-values-and-eigen-vector",
    "href": "MathToolBox/la/la.html#eigen-values-and-eigen-vector",
    "title": "Linear Algebra for Data Science",
    "section": "2.8 Eigen-values and Eigen-vector",
    "text": "2.8 Eigen-values and Eigen-vector"
  },
  {
    "objectID": "MathToolBox/la/la.html#eigen-decomposition",
    "href": "MathToolBox/la/la.html#eigen-decomposition",
    "title": "Linear Algebra for Data Science",
    "section": "2.9 Eigen Decomposition",
    "text": "2.9 Eigen Decomposition"
  },
  {
    "objectID": "MathToolBox/la/la.html#orthonormal-matrix",
    "href": "MathToolBox/la/la.html#orthonormal-matrix",
    "title": "Linear Algebra for Data Science",
    "section": "2.10 Orthonormal Matrix",
    "text": "2.10 Orthonormal Matrix\n\nLA4DS Homepage"
  },
  {
    "objectID": "MathToolBox/la/la_09.html",
    "href": "MathToolBox/la/la_09.html",
    "title": "Matrix, its true color",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "01_c_index.html",
    "href": "01_c_index.html",
    "title": "Norns’ Blessing",
    "section": "",
    "text": "On this page, you can find a list of my courses. May the three goddesses of fate bless our journey through data, models, and predictions.\n\n\nMachine Learning with R, Part 1\nMachine Learning with R, Part 2\nMultivariate Data Analysis\nComputational Statistics\nUsing R to Learn Basic Mathematics for Data Science\n\n\n\n\n\nIn Norse mythology, the three Norns—Urd, who governs the past; Verdandi, who governs the present; and Skuld, who governs the future—perfectly align with the perspectives we take as statisticians: data reflecting the past, models capturing the present, and predictions shaping the future. Figure: the Norns spin the threads of fate at the foot of Yggdrasil, the tree of the world. Source: google search."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_home.html",
    "href": "Courses/c_mlwr1_2024/l3/l3_home.html",
    "title": "Lecture 3: Probability Knowledge",
    "section": "",
    "text": "In this lecture,\n\n\n\n–\nLecture notes:\nR codes:\nCourse Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_home.html",
    "href": "Courses/c_mlwr1_2024/l4/l4_home.html",
    "title": "Lecture 4: Gaussian Discrimination Analysis",
    "section": "",
    "text": "In this lecture,\n\nLecture notes: Integrated notes; Pagination notes; Download the PDF notes\nReading Guidelines: You can read the book…\nR codes:\nCourse Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html",
    "title": "Machine Learning with R, Part 1",
    "section": "",
    "text": "This is a public course designed for master’s level students in Umeå University, and it is given at the department of statistics, Umeå University. Students need to have elementary programming knowledge and some understanding of basic statistics, at most understanding regression analysis. In the first part of this course, we will focus on familiarizing students with the basic concepts of machine learning through basic linear models and preparing them for the second part of the course.\n\n\n\nIn this new era, learning various skills in data science is as straightforward as learning to get a driver’s license, but not everyone can become an engineer or technician for the Ferrari racing team. My visions: First, every student can get their license and add new tools to their data analysis toolbox. Second, I hope this course can provide some helps to those who want to understand the engine behind the tools. If that sounds like you, you’ll need to invest some time and a bit of patience :)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#introduction",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#introduction",
    "title": "Machine Learning with R, Part 1",
    "section": "",
    "text": "This is a public course designed for master’s level students in Umeå University, and it is given at the department of statistics, Umeå University. Students need to have elementary programming knowledge and some understanding of basic statistics, at most understanding regression analysis. In the first part of this course, we will focus on familiarizing students with the basic concepts of machine learning through basic linear models and preparing them for the second part of the course.\n\n\n\nIn this new era, learning various skills in data science is as straightforward as learning to get a driver’s license, but not everyone can become an engineer or technician for the Ferrari racing team. My visions: First, every student can get their license and add new tools to their data analysis toolbox. Second, I hope this course can provide some helps to those who want to understand the engine behind the tools. If that sounds like you, you’ll need to invest some time and a bit of patience :)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#course-design",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#course-design",
    "title": "Machine Learning with R, Part 1",
    "section": "Course Design",
    "text": "Course Design\nIn this course, after understanding the basic concepts of machine learning, we start with useful tools, the elementary probability knowledge and an introduction to the R language. In lecture 4, 5, and 7, we focus on linear solutions to machine learning problems. Two linear classifiers, Linear Discriminant Analysis and Logistic regression, and linear regression models. During this period, we will also introduce a simple idea of nonlinear extension, which will lead to an important concept and challenge in machine learning, namely the overfitting problem. By this, we consider avoiding overfitting as a model selection problem, and the corresponding methods are discussed."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#textbook",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#textbook",
    "title": "Machine Learning with R, Part 1",
    "section": "Textbook",
    "text": "Textbook\nWe use ‘An Introduction to statistical learning’ as our textbook. The website of the book: https://www.statlearning.com. On this website, you can not only get an electronic copy of this book, but also find a lot of useful information."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#teaching-methods",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#teaching-methods",
    "title": "Machine Learning with R, Part 1",
    "section": "Teaching Methods",
    "text": "Teaching Methods\nAs an online course, we naturally choose the flipped classroom teaching method. That is, students first read and study the materials and textbooks we provided, and then conduct laboratory lessons after discussions in a question-and-answer (Q&A) session."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#examination-method",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#examination-method",
    "title": "Machine Learning with R, Part 1",
    "section": "Examination method",
    "text": "Examination method\nWe use a combination of take home exams and oral interviews to assess students’ mastery of course knowledge."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#list-of-lectures",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#list-of-lectures",
    "title": "Machine Learning with R, Part 1",
    "section": "List of lectures",
    "text": "List of lectures\n\nLecture 1. Introduction to Machine Learning\nLecture 2. Introduction to R Programming\nLecture 3. Probability Theory\nLecture 4. Linear Discriminant Analysis\nLecture 5. Regression Models\nLecture 6. Model Validation and Selection\nLecture 7. Logistic Regression"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html",
    "href": "Courses/c_mlwr1_2024/l5/l5.html",
    "title": "Lecture 5: Regression Models",
    "section": "",
    "text": "Linear regression model: \\[\n  y = w_0 + w_1 x_1 + \\dots w_px_p + \\epsilon\n\\] Based on this model, we assume a mechanism the target variable is generated as the sum of \\(w_0 + w_1 x_1 + \\dots w_px_p\\) and a error term, \\(\\epsilon\\). The error term contains many things. It could be measurement errors, or random noise, or all the variations that can not be explained by all the feature variables. For the latest case, it also means we need more feature variables for a better prediction of the target variable.\nObviously, we have to apply some methods (algorithm) to learn (estimate) the coefficients \\(w_0, w_1, \\dots, w_p\\) from a data set. Here, we mainly discuss two methods, least square methods and maximum likelihood method. Eventually, the two methods are equivalent for a regression problem, however, it is still necessary to explain maximum likelihood method for understanding logistic regression in lecture 7.\n\n\nLeast Square method was proposed by the famous mathematician Gauss. He applied this method for data analysis to accurately predict the time of the second appearance of the asteroid, Ceres. His idea is quite simple: to use data to find the optimal line, represented by two coefficients, in order to minimize prediction errors. Supoose that we have a set of paired observations, \\((y_1,x_1), \\dots, (y_n, x_n)\\), then the mathematical formulation is \\[\n  \\hat{w}_0, \\hat{w}_1 = \\arg\\min_{w_0, w_1} \\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n\\] where \\(\\hat{y}_i = w_0 + w_1x_i\\). The solution of this optimization problem is \\(\\widehat{w}_1=\\frac{\\sum_{i=1}^N(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^N(x_i-\\overline{x})^2}\\), and \\(\\widehat{w}_0=\\overline{y}-\\widehat{w}_1\\overline{x}\\)\n\n\n\nDifferent from least square method, next, we are going to reexamine the regression model from the perspective of probability models. To do so, we assume the error term \\(\\epsilon\\) is normally distributed, \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\). Based on this assumption, the target variable is normally distributed conditional on feature variables. Therefore, we essentially predict the expected value of the target variable conditional on \\(X_1, \\dots, X_p\\) as a linear model, i.e.\n\n\n\nRegression Model: the expected value of target variable is a linear function of X1 and X2\n\n\n\\[\n  \\text{E}(Y | X_1, \\dots, X_p) = w_0 + w_1 X_1 + w_2 X_2 + \\dots + w_p X_p\n\\] Based on the normality assumption, another estimation method, MLE, for coefficients can be discussed.\n\n\nYou may remember the question when we discussed the difference between probability and likelihood in the first lecture. How do you calculate the likelihood value of this observation when you stand in a square in the center of town and randomly grab a man and measure his height at 230? Based on Normality assumption, the likelihood of this observation can be evaluated as \\(f(230, 179, 5)\\) where \\(f\\) is the density function of normal distribution, or\n\ndnorm(230, 179, 5)\n\n[1] 2.041461e-24\n\n\nNow, let’s consider a new scenario. Last weekend, I was abducted by a small alien creature to an unfamiliar planet… They released me at a square in the center of town. I felt very bored so just grabbed a little male alien monster and measured his height at 125. Please kindly don’t ask me how to tell the difference between male and female and focus on the next question. What is the likelihood value of this observation?\n\n\n\nLovely little Monsters\n\n\nWell, different from the previous question about Swedish men, I don’t have any prior information about this planet but only believe their height should be also Gaussian distributed. What can I do is grab more monsters and measure their height, then estimate the mean and variance by the random sample. For example, we have a sample of little monsters: \\(125, 131, 190, 35, 230, 23, 142, 195, 175, 168\\).\nIn this case, the likelihood of this sample can be represented as \\[\n  L(\\mu, \\sigma^2, | x_1, \\dots, x_{10}) = f(x_1,\\dots,x_{10} | \\mu, \\sigma^2) = \\prod_{i = 1}^{10} f(x_i, \\mu, \\sigma^2)\n\\]\nThe likelihood of this sample is not determined, but a function of unknown parameters \\(\\mu\\) and \\(\\sigma\\), \\(L(\\mu, \\sigma^2, | x_1, \\dots, x_{10})\\). We call this function as likelihood function. The optimal estimation of the unknown parameters should be the values that maximize the likelihood function.\n\n\n\nUnder the normality assumption, we have \\(y_i \\sim \\mathcal{N}( w_0 + w_1x_1 , \\sigma^2)\\). If you remember the secrete message behind the normal distribution, the likelihood of each observation, \\(y_i\\), is inversely proportionally to the distance to the expected value, i.e. \\[\n   f( y_i | w_0, w_1, \\sigma^2 ) \\propto -(y_i - (w_0 + w_1x_1 ) )^2\n\\] Therefore the likelihood function of the sample \\(\\left\\{ y_i, x_i \\right\\}_{i=1}^n\\) is \\[\n\\log \\left( L( w_0, w_1, \\sigma^2 | (y_i, x_i) ) \\right) \\propto -\\sum_{i=1}^n (y_i - (w_0 + w_1x_1 ) )^2\n\\]\nNotice that, on the LHS, it is sum square of residual. Therefore, minimize sum square of residuals is equivalent to maximize the log likelihood function. In other words, the two methods are equivalent."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#least-square-method",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#least-square-method",
    "title": "Lecture 5: Regression Models",
    "section": "",
    "text": "Least Square method was proposed by the famous mathematician Gauss. He applied this method for data analysis to accurately predict the time of the second appearance of the asteroid, Ceres. His idea is quite simple: to use data to find the optimal line, represented by two coefficients, in order to minimize prediction errors. Supoose that we have a set of paired observations, \\((y_1,x_1), \\dots, (y_n, x_n)\\), then the mathematical formulation is \\[\n  \\hat{w}_0, \\hat{w}_1 = \\arg\\min_{w_0, w_1} \\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n\\] where \\(\\hat{y}_i = w_0 + w_1x_i\\). The solution of this optimization problem is \\(\\widehat{w}_1=\\frac{\\sum_{i=1}^N(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^N(x_i-\\overline{x})^2}\\), and \\(\\widehat{w}_0=\\overline{y}-\\widehat{w}_1\\overline{x}\\)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#maximum-likelihood-method",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#maximum-likelihood-method",
    "title": "Lecture 5: Regression Models",
    "section": "",
    "text": "Different from least square method, next, we are going to reexamine the regression model from the perspective of probability models. To do so, we assume the error term \\(\\epsilon\\) is normally distributed, \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\). Based on this assumption, the target variable is normally distributed conditional on feature variables. Therefore, we essentially predict the expected value of the target variable conditional on \\(X_1, \\dots, X_p\\) as a linear model, i.e.\n\n\n\nRegression Model: the expected value of target variable is a linear function of X1 and X2\n\n\n\\[\n  \\text{E}(Y | X_1, \\dots, X_p) = w_0 + w_1 X_1 + w_2 X_2 + \\dots + w_p X_p\n\\] Based on the normality assumption, another estimation method, MLE, for coefficients can be discussed.\n\n\nYou may remember the question when we discussed the difference between probability and likelihood in the first lecture. How do you calculate the likelihood value of this observation when you stand in a square in the center of town and randomly grab a man and measure his height at 230? Based on Normality assumption, the likelihood of this observation can be evaluated as \\(f(230, 179, 5)\\) where \\(f\\) is the density function of normal distribution, or\n\ndnorm(230, 179, 5)\n\n[1] 2.041461e-24\n\n\nNow, let’s consider a new scenario. Last weekend, I was abducted by a small alien creature to an unfamiliar planet… They released me at a square in the center of town. I felt very bored so just grabbed a little male alien monster and measured his height at 125. Please kindly don’t ask me how to tell the difference between male and female and focus on the next question. What is the likelihood value of this observation?\n\n\n\nLovely little Monsters\n\n\nWell, different from the previous question about Swedish men, I don’t have any prior information about this planet but only believe their height should be also Gaussian distributed. What can I do is grab more monsters and measure their height, then estimate the mean and variance by the random sample. For example, we have a sample of little monsters: \\(125, 131, 190, 35, 230, 23, 142, 195, 175, 168\\).\nIn this case, the likelihood of this sample can be represented as \\[\n  L(\\mu, \\sigma^2, | x_1, \\dots, x_{10}) = f(x_1,\\dots,x_{10} | \\mu, \\sigma^2) = \\prod_{i = 1}^{10} f(x_i, \\mu, \\sigma^2)\n\\]\nThe likelihood of this sample is not determined, but a function of unknown parameters \\(\\mu\\) and \\(\\sigma\\), \\(L(\\mu, \\sigma^2, | x_1, \\dots, x_{10})\\). We call this function as likelihood function. The optimal estimation of the unknown parameters should be the values that maximize the likelihood function.\n\n\n\nUnder the normality assumption, we have \\(y_i \\sim \\mathcal{N}( w_0 + w_1x_1 , \\sigma^2)\\). If you remember the secrete message behind the normal distribution, the likelihood of each observation, \\(y_i\\), is inversely proportionally to the distance to the expected value, i.e. \\[\n   f( y_i | w_0, w_1, \\sigma^2 ) \\propto -(y_i - (w_0 + w_1x_1 ) )^2\n\\] Therefore the likelihood function of the sample \\(\\left\\{ y_i, x_i \\right\\}_{i=1}^n\\) is \\[\n\\log \\left( L( w_0, w_1, \\sigma^2 | (y_i, x_i) ) \\right) \\propto -\\sum_{i=1}^n (y_i - (w_0 + w_1x_1 ) )^2\n\\]\nNotice that, on the LHS, it is sum square of residual. Therefore, minimize sum square of residuals is equivalent to maximize the log likelihood function. In other words, the two methods are equivalent."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#basic-ideas-of-non-linear-extension",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#basic-ideas-of-non-linear-extension",
    "title": "Lecture 5: Regression Models",
    "section": "Basic ideas of Non-linear Extension",
    "text": "Basic ideas of Non-linear Extension\nOf course, nonlinear models are not the focus of our course. However, here we can explore the basic approach to finding nonlinear models, that is feature mapping. Feature mapping involves the basic idea of introducing new variables by transforming the original feature variables with functions. This expands the original feature space, allowing the exploration of potential linear solutions within the augmented feature space. Let’s start from the example of classification problem.\nIn the classification problem, we can consider three new variables \\(x_1^2, \\sqrt{2}x_1x_2, x_2^2\\) instead of the two original variables \\(x_1, x_2\\). The new data set is visualzied in the following plot.\n\n\n\nAugmented Feature Space\n\n\nAs we can see from the LHS, the classification problem becomes a linearly separable case in the augmented feature space. Also, if we change the direction of our view, the linear model in the augmented feature space is eventually a nonlinear model in the original space."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#polynomial-regression-1",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#polynomial-regression-1",
    "title": "Lecture 5: Regression Models",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nIn the regression problem, we also can consider an augmented feature space \\((x, x^2, x^3)\\) according to the data visualization. In other words, we consider the true model as \\(y = w_0 + w_1x + w_2x^2 + w_3x^3\\) which is the 3rd order polynomial function of \\(x\\). We call this regression model as polynomial regression model, however, it is an essentially a linear regression in the augmented feature space."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_4_1.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_4_1.html",
    "title": "2.4.1 Basic Data Types",
    "section": "",
    "text": "Numeric: Represents real numbers (both integers and decimals).\n\nx = 3.14; y = 2\nclass(x)\nclass(y)\nThis is a sample sentence with an annotated text that shows additional information on hover.\n\nInteger ( NE ): Represents whole numbers explicitly (defined with L).\n\nx = 5L; y = c(1L, 2L)\nclass(x)\nclass(y)\n\nComplex ( NE ): Represents complex numbers with real and imaginary parts.\n\nx = 1 + 2i\nclass(x)\n\nCharacter: Represents strings of text enclosed in quotes.\n\nx = \"Machine Learning\"\nclass(x)\n\nLogical: Represents Boolean values: TRUE or FALSE.\n\nx = TRUE; y = FALSE\nclass(x)\nclass(y)\nx & y # '&' is the AND operator\nx | y # '|' is the OR operator\nx + y # Can we calculate it?\nx/y # And this?\nRemark: if you apply basic arithmetic operators to logical values, then they will be transformed to numeric first, and then execute the calculations.\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_4_2.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_4_2.html",
    "title": "2.4.2 Array",
    "section": "",
    "text": "Array is a data structure that can hold multiple values in a grid-like format, organized into dimensions (like rows, columns, and layers). Arrays can be one-dimensional (like a vector), two-dimensional (like a matrix), or multi-dimensional (with three or more dimensions)."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_4_2.html#vector",
    "href": "Courses/c_mlwr1_2024/l2/l2_4_2.html#vector",
    "title": "2.4.2 Array",
    "section": "Vector:",
    "text": "Vector:\nIn R, a vector is one-dimensional array that contains data with same type. The outputs of the functions c, seq, and rep are essentially vectors, with the only difference being that they are viewed as a sequence with a length, but do not have dimensional attributes. For example,\nx = seq(1,10, by = 0.1)\nclass(x) # output is numeric\nlength(x) # output is 91\ndim(x) # output is NULL\nIn some programming language, such kind of objects without dimensional attributes are called tuple. This means that we can slice this type of sequence object to extract the desired elements. We will introduce slicing in the discussion about vectors below.\nOne can use the array function to embed the dimension attribute into such an object and then obtain a real vector, for example,\nx = seq(0,1,0.2)\nx = array(x, dim = length(x)) \n# length(x) is the default value for 'dim'\nclass(x)\nlength(x)\ndim(x)\nOne can use [index] to slice an array, for examples\n# Guess the outputs first, verify them in R\nx = seq(0,1,by=0.2)\nx[1] # the first element in an array\nx[length(x)] \nx[1:3]  \nx[c(1,2,5)]\nx[rep(2:4, 3)]\nx[-1]\nx[-c(1,2,5)]\nSome operations, examples\nx = seq(0,1,by=0.2)\n2*x + 1 # elementwise operations\nt(x) # transpose\ndim(t(x)) # column vector\ndim(t(t(x))) # row vector"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_4_2.html#matrix",
    "href": "Courses/c_mlwr1_2024/l2/l2_4_2.html#matrix",
    "title": "2.4.2 Array",
    "section": "Matrix:",
    "text": "Matrix:\nMatrices in R are the same as matrices in linear algebra, that is matrix is a rectangle array. So, we can apply function array to create matrix as well, but need to specify two numbers for the argument dim. For example,\nX = array(1:10, dim = c(2,5))\nX\nFrom the output of the example above we can see that we can only fill in the elements by columns. If we want to fill in the elements by rows, then we have to use another function matrix. For example,\nX = matrix(1:10, nrow = 2, ncol = 5, byrow = FALSE) \nX\nX = matrix(1:10, nrow = 2, ncol = 5, byrow = TRUE)\nX\nFor slicing of matrix, see examples\n# Guess the outputs first, verify them in R\nX = matrix(1:9,3,3,byrow=T)\nX[1,2]\nX[1,]\nX[,3]\nX[1:2,2:3]\nX[ array(c(1,2), dim = c(1,2)) ]\nX[ array(c(1,2,3,2), dim = c(2,2)) ]\nMore operations for matrix, see examples\nX = matrix(1:9,3,3,byrow=T)\nX*X # elementwise multiplication\nX%*%X # matrix multiplication (in linear algebra)\nt(X) # transpose \ndet(X) # calculate determinant\n\nX = matrix(c(1,2,2,1),2,2)\neigen(X) # eigen decomposition of a square matrix\nTwo useful functions, cbind and rbind, which can combine objects by column and row, respectively. See examples,\nx1 = 1:3\nx2 = 3:5\n(X = cbind(x1,x2))\n(X = rbind(x1,x2))\nclass(X) # check the type of outputs\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_4_4.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_4_4.html",
    "title": "2.4.4 List",
    "section": "",
    "text": "The last type of data structure I want to introduce to you is the list. Compared to matrices and data frames, its frequency of use in programming is not very high, but it is still very useful and reflects a higher level of understanding of R data structures. It is a versatile and powerful data structure that can hold an ordered collection of elements, which can be of different types. Unlike vectors, matrices, and data frames, lists can contain mixed data types, including other lists, vectors, data frames, and even functions. conversely, a data frame is essentially a type of list. Let’s see some examples\n# Example 1 \ny = list()\ny[[1]] = 1:10 # In a list, we use double square brackets to slice. \ny[[2]] = letters[2:7]\ny[[3]] = function(x){2*x}\ny\ny[[3]](2) # what is this?\n# Example 2\ny = list()\ny$x = 1:10 # In a list, we also can use `$` to slice.\ny$letters = letters[2:7]\ny$double = function(x){2*x}\ny\ny$double(2) # Now, you understand why data frame is also a list.\nRemark: Function is alo a kind of data type or structure in R. It has been investigated before, so we don’t discuss it here again.\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_5_2.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_5_2.html",
    "title": "2.5.2 For Loop",
    "section": "",
    "text": "For loop repeats a block of code a specified number of times. The syntax is\n# Syntax of for loop\nfor (i in SEQUENCE) {\n    # Code block to be executed\n}\nIn loop syntax, the code block will be executed with respect to each elements in SEQUENCE. For example, we want to print all numbers within 50 that are divisible by 3.\n# Example 4\nfor(i in 1:50){\n  if (i %% 3 == 0){ # %% is modulo operator, i.e. returns the remiainder of i/3\n    print(i)\n  }\n}\nRemark: The form of SEQUENCE can vary in many ways and doesn’t even need to be a numeric array. See the examples below.\n# Example 5: print all the even number within 50\nfor(i in seq(2, 50, by = 2)){\n  print(i)\n}\n# Example 6: say hello to famous mathematicians \nname_list = c(\"Gauss\", \"Euler\", \"Fourier\", \"Cantor\")\nfor(i in name_list){\n  print(paste0(\"Hello, \", i, \"!\"))\n}\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_5_1.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_5_1.html",
    "title": "2.5.1 If/Else",
    "section": "",
    "text": "The if/else statement executes a block of code based on a specified condition. The syntax is:\n# Syntax of If/Else statement\nif (CONDITION) {\n    # Code block to be executed if CONDITION is TRUE\n} else {\n    # Code block to be executed if CONDITION is FALSE\n}\nIn this syntax, the code block will be executed based on whether the CONDITION is TRUE or FALSE. For example:\n# Example 1\nx = 6\nif(x &gt; 5){\n  print(\"x is greater than 5\")\n}else{\n  print(\"x is 5 or less\")\n}\nIn R, else is not mandatory, especially for simple conditional checks. R allows you to use if to evaluate a condition and execute code if the condition is true, without requiring an else block to handle other cases. For example,\n# Example 2\nx = 6\nif(x &gt; 5){\n  print(\"x is greater than 5\")\n}\nThe ifelse function in R is used to execute one of two values based on a specified condition, element-wise across a vector. The syntax is:\n# Syntax of function 'ifelse'\nifelse(CONDITION, VALUE_IF_TRUE, VALUE_IF_FALSE)\nIn this syntax, ifelse evaluates each element of CONDITION. If the element meets the CONDITION (is TRUE), VALUE_IF_TRUE is returned; otherwise, VALUE_IF_FALSE is returned. For example:\n# Example 3\nx = c(3, 9, 1, 6, 5)\nresult = ifelse(x &gt; 5, \"grater than 5\", \"not grater than 5\")\nprint(result)\nIn this example, the code will check each element in x to see if it is greater than 5. If true, it will return “greater than 5”; otherwise, it will return “not greater than 5”.\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html",
    "href": "Courses/c_mlwr1_2024/l2/l2.html",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "",
    "text": "Machine learning is a discipline based on data and algorithms, so naturally, we need a programming language to implement algorithms and conduct experiments. For many reasons, the R language is a good choice, with its open-source nature and simple syntax being the primary ones. It’s important to note that this course does not focus on advanced applications of R, so we aim to minimize the learning curve, allowing students to quickly understand and master the language for convenient experimentation. In short, our ultimate goal is to understand models and algorithms through experimentation."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#an-overview-of-r-language",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#an-overview-of-r-language",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "2.1 An Overview of R Language",
    "text": "2.1 An Overview of R Language\nR is a powerful and versatile programming language primarily used for statistical computing, data analysis, and graphical representation. Developed in the early 1990s by Ross Ihaka and Robert Gentleman at the University of Auckland, R has since evolved into a robust tool that supports a wide range of applications in various fields, including data science, bioinformatics, and social sciences.\nMain features of R:\n\nFor machine learning, data mining, and statistical analysis: R provides a comprehensive suite of statistical functions, making it ideal for conducting complex data analyses. It supports various statistical methods and techniques, including linear and nonlinear modeling, time-series analysis, classification, clustering, and machine learning. R facilitates the implementation of machine learning algorithms for predictive modeling and data mining.\nFor data Visualization: R excels at creating high-quality graphics and visualizations. With packages like ggplot2, users can generate intricate plots and charts to effectively communicate insights and findings.\nFor data Handling: R has powerful data manipulation capabilities, especially with packages like dplyr and tidyr. These tools allow for efficient data cleaning, transformation, and reshaping, making it easier to prepare datasets for analysis.\nEfficient matrix computing: R is inherently designed for matrix operations and linear algebra, making it particularly well-suited for tasks involving matrix computations. This feature allows users to perform complex mathematical calculations efficiently, which is essential in statistics and data analysis.\nCommunity Support: R has a vibrant and active community, offering extensive resources, tutorials, and forums for users to seek help and share knowledge. This community-driven approach fosters continuous improvement and innovation within the R ecosystem."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#a-prowerful-weapon-rstudio",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#a-prowerful-weapon-rstudio",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "2.2 A Prowerful Weapon, Rstudio",
    "text": "2.2 A Prowerful Weapon, Rstudio\nWhen I was a student, we could only program using R’s rudimentary built-in interface, and many tasks required our direct involvement. After I graduated with my Ph.D., RStudio became increasingly popular due to its powerful and powerful features. We also inevitably fell into it. Nowadays, while enjoying the convenience it brings, I can’t help but feel nostalgic when I look back on those youthful days.\nRStudio is a powerful integrated development environment (IDE) specifically designed for R programming. It provides a user-friendly interface that enhances the R programming experience, making it easier for users to write code, visualize data, and manage projects. Whether you are a beginner or an experienced programmer, RStudio offers a range of features that streamline the data analysis process.\nThe main features of Rstudio are introduced in the following Figure. More advanced operations await your exploration.\n\n\n\nThe entire workspace is divided into four main areas. Top Left: This is where we write code. After writing the code, you can save it for future use. Top Right: This is our console, where you can directly enter commands at the cursor and get results. Alternatively, you can select a line or several consecutive lines in the top left code area and press cmd + enter, allowing you to quickly obtain results here. Bottom Left: This is our working environment area, where the most important tab is ‘Environment’. Here, you can easily see all the objects in the current environment at a glance. Bottom Right: There are two important tabs. First, after running data visualization commands, you will see results in tab Plots (4). Secondly, in the second tab (5), you can browse your path and search for the files you need.\n\n\n\nThe outline of this tutorial:\n\nThe first sight of R: We quickly start this journey with understanding the main actor in R, function.\nData type and structure: Different types and structure of data are introduced.\nFlow control: Continue learning the syntax of three main flow controls.\nProbability: Study the functions related to probability theory.\nOther important things: We close the tutorial by remarking on some useful things."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#love-at-first-sight",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#love-at-first-sight",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "2.3 Love at First Sight?",
    "text": "2.3 Love at First Sight?\nIn R, we can enter commands in the console to have the computer perform the corresponding tasks. For example, we want to print ‘welcome to R’\n\nprint(\"welcome to R\")\n\n[1] \"welcome to R\"\n\n\n\n2.3.1 The main actor: function\nAs a computing language specifically designed for statisticians, the most essential elements in R are functions, since almost all tasks are accomplished through various functions. Similar to most programming languages, the general form of a function in R is\nFunctionName([input 1], [input 2], ..., [argument 1], [argument 2], ...)\ni.e. it consists of a function name, inputs and arguments enclosed in parentheses. For example, you can type the following math functions in the console,\n\nsin(pi) # 'pi' is built-in constant representing the mathematical value of Pi\n\n[1] 1.224647e-16\n\nexp(1) # It returns the natural logarithm\n\n[1] 2.718282\n\nlog(10, base = 10) # The logarithm of 10 to the base 10\n\n[1] 1\n\n\nRemark: As you can see from the code above, we use # sign to comment the code, in other words, characters after # are not considered as part of the command.\nR has extensive and well-organized help documentation. You can access help for specific functions using the ? or help() function. For example:\n\n?chisq.test\n\nYou will see the help document of this function in Rstudio.\n\n\n2.3.2 Functions for generating a sequence of values\nThe first function is c function, which can create a sequence of numbers based on your inputs and store in in memory. For example, we want to create a sequence of the integers from 1 to 10 and store it in a variable x.\nx = c(1,2,3,4,5,6,7,8,9,10)\nYou also can include characters in x by function c, for example\nx = c(letters)\n(x = c(\"I\", \"like\", \"R\", \"How about you?\"))\nRemark: In R, if you use parentheses to enclose a command, then the outputs will be printed automatically.\nTyping the integers from 1 to 10 can be done by another function seq\nseq(1,10)\nor simply by colon operator :\n1:10\nWell, the simple colon operator is neat but limited to integer sequence and increments of 1 (or -1, try 10:1 in your console). Use seq when you need more control over the sequence, for example, try the following code in your console\nseq(1, 10, 2) \nseq(1, 10, length.out = 5)\nseq(0, 1, 0.1)\nThe last function for creating sequence is rep, which can create copies of your inputs, for example\n# guess what will we get by the following commands?\nrep(1, 10)\nrep(1:4, 4)\nrep(rep(1:4,4), 2)\nrep(\"I like R\", 3)\n\n\n2.3.3 Custom function\nIn R, you are allowed to encapsulate specific tasks or calculations that you can reuse throughout your code. The syntax for defining a function is\n# Syntax of custom function\nfunction_name = function(inputs, arguments) {\n  # Function body: code to execute\n  # Optionally return a value\n  return(value)\n}\nFor example, we want a function to calculate the sum of two input values\nmySum = function(x = 1, y = 1){\n    res = x+y\n    return(res)\n}\nmySum(2,3)\nmySum()\n\nIn the parentheses after the syntax key word function, the values assigned to x and y are default values, namely R will take two ones as input automatically if we don’t enter any inputs into function mySum, for example, the result of the last line above should be 2.\nIf we don’t use return to indicate the results should be returned, then the results of the last line in the function will be returned as output."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#data-type-and-structure-in-r",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#data-type-and-structure-in-r",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "2.4 Data type and structure in R",
    "text": "2.4 Data type and structure in R\nIn R, data types (structure) define the nature of data that can be stored and manipulated. The main data types include numeric, character, logical, factor, array, matrix, data frame, list, function, each serving different purposes in data analysis and programming. Function class can check the type of a variable in the memory. Next, we list the simple types first and then illustrate more complex structures in details.\n\n2.4.1 Basic Data Types\n\nNumeric: Represents real numbers (both integers and decimals).\n\nx = 3.14; y = 2\nclass(x)\nclass(y)\nThis is a sample sentence with an annotated text that shows additional information on hover.\n\nInteger ( NE ): Represents whole numbers explicitly (defined with L).\n\nx = 5L; y = c(1L, 2L)\nclass(x)\nclass(y)\n\nComplex ( NE ): Represents complex numbers with real and imaginary parts.\n\nx = 1 + 2i\nclass(x)\n\nCharacter: Represents strings of text enclosed in quotes.\n\nx = \"Machine Learning\"\nclass(x)\n\nLogical: Represents Boolean values: TRUE or FALSE.\n\nx = TRUE; y = FALSE\nclass(x)\nclass(y)\nx & y # '&' is the AND operator\nx | y # '|' is the OR operator\nx + y # Can we calculate it?\nx/y # And this?\nRemark: if you apply basic arithmetic operators to logical values, then they will be transformed to numeric first, and then execute the calculations.\n\n\n2.4.2 Array\nArray is a data structure that can hold multiple values in a grid-like format, organized into dimensions (like rows, columns, and layers). Arrays can be one-dimensional (like a vector), two-dimensional (like a matrix), or multi-dimensional (with three or more dimensions).\nVector:\nIn R, a vector is one-dimensional array that contains data with same type. The outputs of the functions c, seq, and rep are essentially vectors, with the only difference being that they are viewed as a sequence with a length, but do not have dimensional attributes. For example,\nx = seq(1,10, by = 0.1)\nclass(x) # output is numeric\nlength(x) # output is 91\ndim(x) # output is NULL\nIn some programming language, such kind of objects without dimensional attributes are called tuple. This means that we can slice this type of sequence object to extract the desired elements. We will introduce slicing in the discussion about vectors below.\nOne can use the array function to embed the dimension attribute into such an object and then obtain a real vector, for example,\nx = seq(0,1,0.2)\nx = array(x, dim = length(x)) \n# length(x) is the default value for 'dim'\nclass(x)\nlength(x)\ndim(x)\nOne can use [index] to slice an array, for examples\n# Guess the outputs first, verify them in R\nx = seq(0,1,by=0.2)\nx[1] # the first element in an array\nx[length(x)] \nx[1:3]  \nx[c(1,2,5)]\nx[rep(2:4, 3)]\nx[-1]\nx[-c(1,2,5)]\nSome operations, examples\nx = seq(0,1,by=0.2)\n2*x + 1 # elementwise operations\nt(x) # transpose\ndim(t(x)) # column vector\ndim(t(t(x))) # row vector\nMatrix:\nMatrices in R are the same as matrices in linear algebra, that is matrix is a rectangle array. So, we can apply function array to create matrix as well, but need to specify two numbers for the argument dim. For example,\nX = array(1:10, dim = c(2,5))\nX\nFrom the output of the example above we can see that we can only fill in the elements by columns. If we want to fill in the elements by rows, then we have to use another function matrix. For example,\nX = matrix(1:10, nrow = 2, ncol = 5, byrow = FALSE) \nX\nX = matrix(1:10, nrow = 2, ncol = 5, byrow = TRUE)\nX\nFor slicing of matrix, see examples\n# Guess the outputs first, verify them in R\nX = matrix(1:9,3,3,byrow=T)\nX[1,2]\nX[1,]\nX[,3]\nX[1:2,2:3]\nX[ array(c(1,2), dim = c(1,2)) ]\nX[ array(c(1,2,3,2), dim = c(2,2)) ]\nMore operations for matrix, see examples\nX = matrix(1:9,3,3,byrow=T)\nX*X # elementwise multiplication\nX%*%X # matrix multiplication (in linear algebra)\nt(X) # transpose \ndet(X) # calculate determinant\n\nX = matrix(c(1,2,2,1),2,2)\neigen(X) # eigen decomposition of a square matrix\nTwo useful functions, cbind and rbind, which can combine objects by column and row, respectively. See examples,\nx1 = 1:3\nx2 = 3:5\n(X = cbind(x1,x2))\n(X = rbind(x1,x2))\nclass(X) # check the type of outputs\n\n\n2.4.3 Data Frame\nAs a programming language originally designed for statisticians, importing data and setting a specific structure for it is essential. It is so called data frame. In R, we can use various functions to read in different types of data, such as txt, csv, xlsx, and more. For example, you can apply read.table function to import data saved in a txt file. You can download Boston data here.\n# Prepare a txt file, 'Boston.txt', in a director\nsetwd(\"dir containing your data\") \n# we set the dir containg your data as the working director.\ndat = read.table(\"Boston.txt\", sep=\",\", header = T)\nRemark: Setting working director (WD) is always useful since it can simplify many things, for example, if we don’t set the WD as the folder containing ‘Boston.txt’, then you have to specify the dir in the first argument of the read.table function. Setting a WD can be done by function setwd, and for checking the current WD, you can use function getwd. In Rstudio, this action also can be done using mouse actions, see figure below.\n\n\n\nFirst, go to the right folder. Second, in tab ‘Files’, click the gear icon, then you will find it.\n\n\nData frame is a fundamental data structure used for storing tabular data, where each column can hold different types of data (e.g., numeric, character, or factor). Data frame can be created by function data.frame. For example:\n(X = cbind(x1,x2))\n(dat = data.frame(x1,x2)) \nclass(X)\nclass(dat) # it seems there is no difference between a matrix and a dataframe\nX%*%t(X) # try this \ndat%*%t(dat) # try this -&gt; matrix multiplication is not allowed.\nSo, usually, the operations and functions for a matrix are not allowed to apply to a data frame. Including different types of data is the main difference between data frame and matrix. For example:\n# with the same demo data above\nx3 = letters[1:3] # define another variable \nX = cbind(x1, x2, x3)\ndat = data.frame(x1, x2, x3)\nX\ndat # compare `X` and `dat`, draw yoru conclusions.\nFor a data frame, we still can use the same method as for matrix to slice. Another more practical way is using $ to slice. For examples:\n# with the same example above\ndat[,3] \ndat$x3\nSome useful functions for data frames\n\nhead and tail functions: they can help us to check the first and last few lines respectively. For examples:\n\ndat = iris # iris is a pre-stored data set in R which includes 150 iris flowers\nhead(dat)\ntail(dat)\nhead(dat, 10)\n\nnames function: it can help us quickly check the names of all variables.\nattach and detach functions: people feel very inconvenient to use $ to slice a data frame, but want to use the variable names directly. In this case, ´attach´ function can help us go into such kind of mode, and apparently detach function can cease this mode. For examples:\n\ndat = iris\nnames(iris) # [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"  \nSpecies # you can't find it\ndat$Species # works\n\nattach(dat)\nSpecies # also works\n\ndetach(dat)\nSpecies\n\n\n2.4.4 List\nThe last type of data structure I want to introduce to you is the list. Compared to matrices and data frames, its frequency of use in programming is not very high, but it is still very useful and reflects a higher level of understanding of R data structures. It is a versatile and powerful data structure that can hold an ordered collection of elements, which can be of different types. Unlike vectors, matrices, and data frames, lists can contain mixed data types, including other lists, vectors, data frames, and even functions. conversely, a data frame is essentially a type of list. Let’s see some examples\n# Example 1 \ny = list()\ny[[1]] = 1:10 # In a list, we use double square brackets to slice. \ny[[2]] = letters[2:7]\ny[[3]] = function(x){2*x}\ny\ny[[3]](2) # what is this?\n# Example 2\ny = list()\ny$x = 1:10 # In a list, we also can use `$` to slice.\ny$letters = letters[2:7]\ny$double = function(x){2*x}\ny\ny$double(2) # Now, you understand why data frame is also a list.\nRemark: Function is alo a kind of data type or structure in R. It has been investigated before, so we don’t discuss it here again."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#flow-control",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#flow-control",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "2.5 Flow Control",
    "text": "2.5 Flow Control\nFlow control refers to the mechanisms and structures used in programming to dictate the order in which instructions are executed in a program. It allows for making decisions, repeating actions, and controlling the flow of execution based on certain conditions or logic. In R and most programming languages, flow control is essential for creating dynamic and responsive programs. Here, we will mainly introduce loops and conditional statement.\n\n2.5.1 If/Else\nThe if/else statement executes a block of code based on a specified condition. The syntax is:\n# Syntax of If/Else statement\nif (CONDITION) {\n    # Code block to be executed if CONDITION is TRUE\n} else {\n    # Code block to be executed if CONDITION is FALSE\n}\nIn this syntax, the code block will be executed based on whether the CONDITION is TRUE or FALSE. For example:\n# Example 1\nx = 6\nif(x &gt; 5){\n  print(\"x is greater than 5\")\n}else{\n  print(\"x is 5 or less\")\n}\nIn R, else is not mandatory, especially for simple conditional checks. R allows you to use if to evaluate a condition and execute code if the condition is true, without requiring an else block to handle other cases. For example,\n# Example 2\nx = 6\nif(x &gt; 5){\n  print(\"x is greater than 5\")\n}\nThe ifelse function in R is used to execute one of two values based on a specified condition, element-wise across a vector. The syntax is:\n# Syntax of function 'ifelse'\nifelse(CONDITION, VALUE_IF_TRUE, VALUE_IF_FALSE)\nIn this syntax, ifelse evaluates each element of CONDITION. If the element meets the CONDITION (is TRUE), VALUE_IF_TRUE is returned; otherwise, VALUE_IF_FALSE is returned. For example:\n# Example 3\nx = c(3, 9, 1, 6, 5)\nresult = ifelse(x &gt; 5, \"grater than 5\", \"not grater than 5\")\nprint(result)\nIn this example, the code will check each element in x to see if it is greater than 5. If true, it will return “greater than 5”; otherwise, it will return “not greater than 5”.\n\n\n2.5.2 For Loop\nFor loop repeats a block of code a specified number of times. The syntax is\n# Syntax of for loop\nfor (i in SEQUENCE) {\n    # Code block to be executed\n}\nIn loop syntax, the code block will be executed with respect to each elements in SEQUENCE. For example, we want to print all numbers within 50 that are divisible by 3.\n# Example 4\nfor(i in 1:50){\n  if (i %% 3 == 0){ # %% is modulo operator, i.e. returns the remiainder of i/3\n    print(i)\n  }\n}\nRemark: The form of SEQUENCE can vary in many ways and doesn’t even need to be a numeric array. See the examples below.\n# Example 5: print all the even number within 50\nfor(i in seq(2, 50, by = 2)){\n  print(i)\n}\n# Example 6: say hello to famous mathematicians \nname_list = c(\"Gauss\", \"Euler\", \"Fourier\", \"Cantor\")\nfor(i in name_list){\n  print(paste0(\"Hello, \", i, \"!\"))\n}\n\n\n2.5.3 While Loop\nDifferent from a for loop, a while loop continues to execute a block of code as long as a specified condition remains true. This means that the number of iterations is not predetermined; instead, it depends on the state of the condition being evaluated. The syntax is\n# Syntax of while loop\nwhile (CONDITION) {\n    # Code block to be executed\n}\nThe CONDITION could be a logical value, or a command resulting logical value. The same example of for loop above, example 4, also can be implemented through a while loop.\n# Example 7\n# Initialize the starting number\nnumber = 3\n# While loop to print numbers divisible by 3 up to 50\nwhile(number &lt;= 50){\n  print(number)      # Print the current number\n  number = number + 3  # Move to the next multiple of 3\n}"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#about-probability",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#about-probability",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "2.6 About Probability",
    "text": "2.6 About Probability\nNote: You may temporarily ignore this part and come back to read after the lecture 3.\nAs a programming language originally designed for statisticians, functions related to probability distribution are essential. For example, how to generate random numbers from a distribution, how to calculate the probability of a random event, how to find the corresponding quantile values based on a given probability, and how to calculate the density function value of a given distribution are all well implemented in R. Below, we will introduce 4 functions using the normal (Gaussian) distribution as an example.\n\n2.6.1 Generate Random Numbers\nGenerating random numbers is essential for simulations, statistical modeling, and resampling techniques like bootstrapping, where random data or sampling is needed to test models, explore scenarios, or understand variability.\nIn R, the rnorm function is applied to generate random numbers from a normal (Gaussian) distribution. The syntax is\n# Syntax of fucntion 'rnorm'\nrnorm(SampleSize, mean = 0, sd = 1)\nFor example,\nx = rnorm(100)\nhist(x) \nRemark: We can’t simulate real random numbers on computer, but only generate pseudo-random numbers through algorithms designed to produce sequences that mimic the properties of randomness. These algorithms start with an initial value known as the random seed, which serves as the starting point for generating the sequence. By changing the seed, we can create different sequences of pseudo-random numbers, allowing for reproducibility in simulations and analyses. While these numbers may appear random, they are ultimately determined by the algorithm and the initial seed value. In R, the random seed can be controled by function ‘set.seed’, for example\n# Next, if you get the same value for 'a' and 'b', I will pay you 1000kr\n(a = rnorm(1))\n(b = rnorm(1)) \n\n# Next, if you get different values for 'a' and 'b', I will pay you 1000kr \nset.seed(2024)\n(a = rnorm(1))\nset.seed(2024)\n(b = rnorm(1))\nLife is seems like a tapestry woven with random numbers, however, to some extent it is not really random, but pseudo-random. On the one hand, it is a box of chocolates - every moment is a surprise, sweet or bitter, unfolding in unpredictable ways. You never know what the next piece will bring. However, on the other hand, we seem to be unable to escape the arrangement of fate. When you were born, God, like a careful gardener, had selected a unique seed for you. This seed contains the potential of your existence and shapes your journey. No matter what your current situation is, we should always cherish our unique seed.\n\n\n2.6.2 Find the Density Values\nThe density value is useful not only in probability but also statistics, because it presents an important quantity, likelihood value. It will be discussed in the next lecture in details.\nIn R, the dnorm function calculates the density (or height) of the normal distribution at a specific value. The syntax is\n# Syntax of function 'dnorm'\ndnorm(x, mean = 0, sd = 1)\nFor example\n# The outputs of the following two lines should be equal. Why?\ndnorm(0)\n1/sqrt(2*pi)\n\n\n2.6.3 Calculate the Probability\nIn R, the pnorm function calculates the probability of an event of a normal distribution. The syntax is\n# Syntax of function 'pnorm'\npnorm(a, mean = 0, sd = 1)\nThis function calculate the probability \\(\\Pr(X&lt;a)\\) which is the area under the normal density curve within the interval \\([-\\infty, a]\\), for example,\n\npnorm(1)\n\n[1] 0.8413447\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.4 Determine the quantile value\nQuantile value is very essential in statistics, for example, you all need quantile value whether you are conducting hypothesis testing or calculating confidence intervals. Actually, it is just an inverse operation of calculating probability, e.g. the quantile value of \\(0.84\\) for a standard normal distribution is approximately \\(1\\) The syntax is similar, for example, you should be familiar with the following quantile value\nqnorm(0.975, 0, 1) # what is it?\n\n\n2.6.5 Summary and Remark\n# Functions related to normal distribution.\nrnorm() # generate random numbers from normal distribution\ndnorm() # find the density value of normal distribution\npnorm() # calculate the probability of an event associated to normal distribution\nqnorm() # determine the quantile value of normal distribution\nRemark: As you can see, there is a pattern in the function names, i.e. a letter + norm, for example in ‘rnorm’,\n\nr: This prefix indicates that the function generates random numbers (random variates) from a specific distribution.\nnorm: This part of the name refers to the normal distribution, also known as the Gaussian distribution.\n\nThis naming convention is applied to other distributions as well. For example:\n\nrbinom: generates random numbers from a binomial distribution\ndexp: finds density value of an exponential distribution\nppois: calculates probability of a Poisson distribution\nqunif: determines the quantile value of a uniform distribution"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#other-useful-things",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#other-useful-things",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "2.7 Other Useful Things",
    "text": "2.7 Other Useful Things\nAlright, we now have a basic understanding of the most fundamental operations in R programming, most of which are things we will frequently use in this course. Next, we’ll introduce a few more useful concepts and some commonly used functions.\n\n2.7.1 Workspace\nIn R, the workspace refers to the environment where all objects (such as variables, functions, and data) are stored during an R session. It acts as a storage area that retains the data and objects you create, allowing you to work with them without needing to re-import or redefine them every time you start R. The most common scenario is when you’ve worked hard all day and want to take a break, but if you close R, all the objects in your working environment (memory) will disappear. In this case, you can save your current working environment as a workspace file, which has a .RData extension.\nThere are two ways to save your working environment as a workspace file. First, by mouse actions, you can click Session -&gt; Save Workspace As.... Or you can do it by command\nsave.image(\"FileName.RData\")\nThe next day, after enjoying the morning sunshine (if conditions permit) and your coffee, you can load this file and continue your hard work!\n\n\n2.7.2 Packages\nIf R could only be used for scientific computing, it would undoubtedly be overshadowed by numerous other scientific computing programs. The true strength of R lies in its extensibility, which is achieved through R packages. Initially, R packages were primarily written by statisticians to implement new methods, such as lme4 for fitting generalized linear mixed-effects models; survival for conducting survival analysis; psych for psychological research, and so on. However, writing packages is not exclusive to statisticians; an increasing number of non-statistical application packages have also been developed. Today, R has become incredibly versatile through the extension of various packages, for example this website is written by quarto package. Below, we will briefly illustrate how to install and load packages using examples.\ninstall.packages(\"kernelab\") \n# to install a new package. Note: the quotation marks are essential.\n\nlibrary(kernelab) \n# you can import a package by function `library`\n\n\n2.7.3 Useful Functions\nNext, some useful functions are introduced. These functions were extremely useful back when I was a student. However, in the era of RStudio, their usefulness has been greatly reduced. Nonetheless, they are still quite necessary for those who prefer keyboard operations or need to work on a server. In addition, these functions can, to some extent, enhance R users’ understanding of R programming.\n\nls function: it can list all the objects in the workspace or current environment.\nrm function: it can help us to remove objects from the workspace or current environment.\n\n# Example 1\nx = 1\nrm(x) \n# Example 2\nrm(list = ls()) # Danger Warning: This command will remove all objects listed by `ls`\n\nstr function: it displays the structure of an object.\n\n# Example 1\nx = list()\nx[[1]] = 1:10\nx[[2]] = letters[4:10]\nstr(x)\n# Example 2\nres = t.test(rnorm(30)) # do one sample t-test and save results in `res`\nstr(res) \n# You can see that the testing results are saved in a list of 10.\n# if you want to extract elements from it, the information coveryed by ´str´ is ideal.\n\nsummary function: it helps us to summarize useful information from an R objects. The information extracted depends on the type of the object. For examples\n\n# Example 1\ndat = iris[,-5] # we use the first 4 variable from iris data\nsummary(dat) # the type of ´dat´ is dataframe, then the summarized informations are...\n# Example 2\nres = t.test(rnorm(30))\nsummary(res) \n# the type of ´res´ is results of t test. The designer of this function decided \n# to show the names of all the elements in ´res´, similiar to the output of ´str´\n\nunique and table functions: they are useful when you want to check all possible values in a variable and the frequency of different possible values.\n\n# First, we create a small demo dataset\ntreatment = c(1,1,0,0,1,1,0,0)\nblock = c(1,1,1,1,2,2,2,2)\nsex = c(\"F\",\"M\",\"M\",\"M\",\"F\",\"F\",\"M\",\"F\")\nage = c(19,20,28,22,21,19,23,20)\noutcome = c(20,19,33,12,54,87,98,84)\nDat = data.frame(treatment, block, sex, age, outcome)\nhead(Data, 8)\n\n# Example 1:\nunique(Dat$sex)\ntable(Dat$sex)\nunique(Dat$age)\ntable(Dat$age)\n\n# Example 2:\ntable(Data$sex, Data$treatment) # do you know the name of the outputs?\n\nwhich function: it finds the index of elements that satisfy some conditions in a vector, or matrix, or data frame.\n\n# Example: Use the same demo data above\nwhich(Dat$sex == \"M\")\nwhich(Dat$age &lt; 21)\n\napply function: it is used to perform operations on rows or columns of matrices, data frames, or higher-dimensional arrays. It allows you to apply a function across the rows or columns without needing to use loops, making code more concise and often more efficient.\n\n# Syntax: \napply(X, margin, fun)\n# `margin` is an integer specifying whether to apply the `fun` across rows (1) or columns (2) \n# Examples: Use the demo data but ignore the variable `sex`\nDat = Dat[, -3]\napply(Dat, 2, mean)\nNext, show some useful functions for graphics. The ggplot2 package is definitely the top choice for plotting, but sometimes the following functions are more practical and convenient for data visualization. I will only list them below, and you are already strong enough to investigate them by yourself :)\n\nhist function: it can help use check the distribution of a variable.\nplot function: it is usually used to show the scatter plot of two variables.\npairs function: it shows the pairwise scatter plot of many variables."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#final-words",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#final-words",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "Final Words:",
    "text": "Final Words:\nAlright, our short journey with R programming has come to an end. I hope you now have a basic understanding of this software, its basic operations, and various syntax. As I mentioned earlier, the goal of this lecture, or even this course, is not solely to train you in R but rather to give you a quick introduction to this simple programming language so that we can use it to study machine learning concepts later on. The R programming language is a powerful tool in the field of data science. You can continue exploring various advanced techniques afterward.\nLastly, I want to say that the best way to learn any language is to engage in conversation; the same applies to programming languages. Only by repeatedly typing commands and scripts into the computer will you truly master and become familiar with this programming language.\n\nLecture 2 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_2.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_2.html",
    "title": "2.2 A powerful weapon, Rstudio",
    "section": "",
    "text": "When I was a student, we could only program using R’s rudimentary built-in interface, and many tasks required our direct involvement. After I graduated with my Ph.D., RStudio became increasingly popular due to its powerful and powerful features. We also inevitably fell into it. Nowadays，while enjoying the convenience it brings, I can’t help but feel nostalgic when I look back on those youthful days.\nRStudio is a powerful integrated development environment (IDE) specifically designed for R programming. It provides a user-friendly interface that enhances the R programming experience, making it easier for users to write code, visualize data, and manage projects. Whether you are a beginner or an experienced programmer, RStudio offers a range of features that streamline the data analysis process.\nThe main features of Rstudio are introduced in the following Figure. More advanced operations await your exploration.\n\n\n\nThe entire workspace is divided into four main areas. Top Left：This is where we write code. After writing the code, you can save it for future use. Top Right: This is our console, where you can directly enter commands at the cursor and get results. Alternatively, you can select a line or several consecutive lines in the top left code area and press cmd + enter, allowing you to quickly obtain results here. Bottom Left: This is our working environment area, where the most important tab is ‘Environment’. Here, you can easily see all the objects in the current environment at a glance. Bottom Right: There are two important tabs. First, after running data visualization commands, you will see results in tab Plots (4). Secondly, in the second tab (5), you can browse your path and search for the files you need.\n\n\nPrevious page: | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_0.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_0.html",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "",
    "text": "Machine learning is a discipline based on data and algorithms, so naturally, we need a programming language to implement algorithms and conduct experiments. For many reasons, the R language is a good choice, with its open-source nature and simple syntax being the primary ones. It’s important to note that this course does not focus on advanced applications of R, so we aim to minimize the learning curve, allowing students to quickly understand and master the language for convenient experimentation. In short, our ultimate goal is to understand models and algorithms through experimentation.\nOutline："
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_0.html#r-programming",
    "href": "Courses/c_mlwr1_2024/l2/l2_0.html#r-programming",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "R programming",
    "text": "R programming\n\nLecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_6.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_6.html",
    "title": "2.6 About Probability",
    "section": "",
    "text": "Note: You may temporarily ignore this part and come back to read after the lecture 3.\nAs a programming language originally designed for statisticians, functions related to probability distribution are essential. For example, how to generate random numbers from a distribution, how to calculate the probability of a random event, how to find the corresponding quantile values based on a given probability, and how to calculate the density function value of a given distribution are all well implemented in R. Below, we will introduce 4 functions using the normal (Gaussian) distribution as an example.\n\n2.6.1 Generate Random Numbers\nGenerating random numbers is essential for simulations, statistical modeling, and resampling techniques like bootstrapping, where random data or sampling is needed to test models, explore scenarios, or understand variability.\nIn R, the rnorm function is applied to generate random numbers from a normal (Gaussian) distribution. The syntax is\n# Syntax of fucntion 'rnorm'\nrnorm(SampleSize, mean = 0, sd = 1)\nFor example,\nx = rnorm(100)\nhist(x) \nRemark: We can’t simulate real random numbers on computer, but only generate pseudo-random numbers through algorithms designed to produce sequences that mimic the properties of randomness. These algorithms start with an initial value known as the random seed, which serves as the starting point for generating the sequence. By changing the seed, we can create different sequences of pseudo-random numbers, allowing for reproducibility in simulations and analyses. While these numbers may appear random, they are ultimately determined by the algorithm and the initial seed value. In R, the random seed can be controled by function ‘set.seed’, for example\n# Next, if you get the same value for 'a' and 'b', I will pay you 1000kr\n(a = rnorm(1))\n(b = rnorm(1)) \n\n# Next, if you get different values for 'a' and 'b', I will pay you 1000kr \nset.seed(2024)\n(a = rnorm(1))\nset.seed(2024)\n(b = rnorm(1))\nLife is seems like a tapestry woven with random numbers, however, to some extent it is not really random, but pseudo-random. On the one hand, it is a box of chocolates - every moment is a surprise, sweet or bitter, unfolding in unpredictable ways. You never know what the next piece will bring. However, on the other hand, we seem to be unable to escape the arrangement of fate. When you were born, God, like a careful gardener, had selected a unique seed for you. This seed contains the potential of your existence and shapes your journey. No matter what your current situation is, we should always cherish our unique seed.\n\n\n2.6.2 Find the Density Values\nThe density value is useful not only in probability but also statistics, because it presents an important quantity, likelihood value. It will be discussed in the next lecture in details.\nIn R, the dnorm function calculates the density (or height) of the normal distribution at a specific value. The syntax is\n# Syntax of function 'dnorm'\ndnorm(x, mean = 0, sd = 1)\nFor example\n# The outputs of the following two lines should be equal. Why?\ndnorm(0)\n1/sqrt(2*pi)\n\n\n2.6.3 Calculate the Probability\nIn R, the pnorm function calculates the probability of an event of a normal distribution. The syntax is\n# Syntax of function 'pnorm'\npnorm(a, mean = 0, sd = 1)\nThis function calculate the probability \\(\\Pr(X&lt;a)\\) which is the area under the normal density curve within the interval \\([-\\infty, a]\\), for example,\n\npnorm(1)\n\n[1] 0.8413447\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.4 Determine the quantile value\nQuantile value is very essential in statistics, for example, you all need quantile value whether you are conducting hypothesis testing or calculating confidence intervals. Actually, it is just an inverse operation of calculating probability, e.g. the quantile value of \\(0.84\\) for a standard normal distribution is approximately \\(1\\) The syntax is similar, for example, you should be familiar with the following quantile value\nqnorm(0.975, 0, 1) # what is it?\n\n\n2.6.5 Summary and Remark\n# Functions related to normal distribution.\nrnorm() # generate random numbers from normal distribution\ndnorm() # find the density value of normal distribution\npnorm() # calculate the probability of an event associated to normal distribution\nqnorm() # determine the quantile value of normal distribution\nRemark: As you can see, there is a pattern in the function names, i.e. a letter + norm, for example in ‘rnorm’,\n\nr: This prefix indicates that the function generates random numbers (random variates) from a specific distribution.\nnorm: This part of the name refers to the normal distribution, also known as the Gaussian distribution.\n\nThis naming convention is applied to other distributions as well. For example:\n\nrbinom: generates random numbers from a binomial distribution\ndexp: finds density value of an exponential distribution\nppois: calculates probability of a Poisson distribution\nqunif: determines the quantile value of a uniform distribution\n\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s_2.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_s_2.html",
    "title": "Geometry of Linear Classifiers",
    "section": "",
    "text": "Note: From now, we will temporarily ignore the bias term, \\(w_0\\), or assume it as \\(0\\). It will not influence our final conclusion. No worries. So, the basic classifier is represented as \\(y = \\text{Sign}(\\textbf{w}^{\\top}\\textbf{x})\\)\nPreviously, we explored the geometric understanding of linear classifiers, which is that the classifier determines a linear decision boundary. Next, let’s understand a linear classifier from another view of geometry. Suppose we have a classifier with two feature variables, \\(x_1\\) and \\(x_2\\), and the “reasonable” weight vector is \\(\\textbf{w} = (0.6, 0.8)^{\\top}\\). Look at the conceptual plot below.\n\n\n\n\n\n\n\n\n\nIt is easy to see that all the vectors (points) in blue form a sharp angle with the weights vector (black). By the property of inner product, (read about inner product) for any point \\(\\textcolor{blue}{\\textbf{x}} = (\\textcolor{blue}{x_1},\\textcolor{blue}{x_2})^{\\top}\\) standing on the direction pointed by the blue arrow, \\(\\textbf{w}^{\\top}\\textcolor{blue}{\\textbf{x}} \\propto \\cos(\\alpha) &gt; 0\\), i.e. all the cases on this direction will be classify as positive. On the contrary, all the vectors (points) in blue form a obtuse angle with the weights vector, and then \\(\\textbf{w}^{\\top}\\textcolor{red}{\\textbf{x}} \\propto \\cos(\\beta) &lt; 0\\), i.e. all the points standing on the direction pointed by a red vector will be classified as negative. With this observation, we can easily understand how does a “reasonable” linear classifier work.\nBased on this principle, let’s have look at a concrete example in the figure below.\n\n\n\nIn a binary classification problem, we have two feature variables, each with 10 cases, blue for positive and red for negative. We have a “reasonable” weight vector \\(\\textbf{w}\\) (orange arrow), which determines a linear decision boundary (purple line). \\(\\textbf{w}\\) is “reasonable” because it has an angle less than 90 degrees with all positive vectors, but an angle greater than 90 degrees with all negative vectors. Of course, a more direct understanding is that this linear classification boundary divides the entire feature space into two parts, with all positive cases at the bottom and all negative cases at the top. However, the first explation is more useful for understanding the proceptron algorithm.\n\n\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s_1.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_s_1.html",
    "title": "History",
    "section": "",
    "text": "Proceptron classifier is viewed as the foundation and building block of artificial neural network. For a historical introduction, see the figures below.\n\n\n\nThe Perceptron classifier was developed by Frank Rosenblatt in 1957 (LHS). Rosenblatt’s goal was to create a machine that could classify visual patterns, such as distinguishing between different shapes. At the time, he envisioned using large computers to simulate neural networks, inspired by how the brain processes information. His early experiments involved using a huge computer called the Mark I Perceptron (RHS), which attempted to recognize different shapes by adjusting weights based on input data. This work laid the foundation for modern neural networks and machine learning, despite initial limitations in its capacity to handle complex, non-linear problems.\n\n\nSuppose we are solving a binary classification problem with \\(p\\) feature variables. As discussed before, it can be represented as\n\\[\n  \\hat{y} = \\text{Sign}( \\textbf{w}^{\\top} \\textbf{x} + w_0  )\n\\]\nwhere \\(\\textbf{w} = (w_1, w_2, \\dots, w_p)^{\\top}\\), and \\(\\textbf{x} = (x_1, x_2, \\dots, x_p)^{\\top}\\). Different from before, here we represent the weighted sum of \\(p\\) feature variables, \\(w_1x_1+ \\dots + w_px_p\\), as the inner (dot) product of two vectors, i.e. \\(\\textbf{w} \\cdot \\textbf{x} = \\textbf{w}^{\\top} \\textbf{x}\\).\n\nNote: In order to understand proceptron algorithm, we need some basic knowledge about vector and its operations. If you are not familiar with it or need to refresh it, read about vector, operators, inner product before start reading the next.\n\nThe perceptron algorithm is about finding a set of reasonable weights. The key term here, “reasonable,” is easy to understand—it refers to a set of weights that can deliver good predictive performance. The core issue is how to find them.\n\nBrute-force idea: try all possible weight values and record the corresponding model performance, such as accuracy, and then choose the weights that yield the highest accuracy as the final model parameters.\n\nHowever, this idea is clearly not ideal. Even if we only have two feature variables, this would still not be a simple task. A smarter approach is to do it this way: we start with an initial guess for the weight values, and then gradually approach the most reasonable weights through some iterative mechanism. This mechanism is called the perceptron algorithm. Next, let’s dive into learning this magical mechanism—the perceptron algorithm.\nNext, the logic goes like this:\n\nFurther explore the geometric properties of linear classifiers\nUse geometry to understand how this algorithm works.\n\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1.html",
    "href": "Courses/c_mlwr1_2024/l1/l1.html",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "",
    "text": "In this lecture, we introduce machine learning to you. You will learn the basic elements in this field and an old, basic, but very interesting algorithm in machine learning.\nNext, I’ll begin with a review of key milestones in artificial intelligence. Then, we’ll delve into some metaphysical concepts, exploring the underlying logic of machine learning. By understanding the human learning process, we’ll gain insight into the entire machine learning process. Finally, after covering the ABCs of machine learning, we’ll focus on the fundamental forms of machine learning models."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1.html#mailstone-of-ai-alphago-2016",
    "href": "Courses/c_mlwr1_2024/l1/l1.html#mailstone-of-ai-alphago-2016",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "1.1 Mailstone of AI, AlphaGo, 2016",
    "text": "1.1 Mailstone of AI, AlphaGo, 2016\nMachine learning is not a new concept, but the latest version of the machine learning legend has indeed just happened recently. Have you heard about AlphaGo? AlphaGo is an artificial intelligence program developed by DeepMind that made history in 2016 by defeating a world champion Go player.\n\n\n\nL: Go is an ancient board game originating from China over 2,500 years ago, in which two players compete to capture territory using black and white stones on a grid. The game’s simple rules allow for deep strategic complexity, making it one of the most intellectually challenging games in the world. R: AlphaGo is an artificial intelligence program developed by DeepMind that became the first to defeat a professional human player, and eventually world champions, in the complex board game Go, showcasing a major milestone in AI’s capabilities in strategic thinking and decision-making. Source: Google search.\n\n\nBack in the year 2000, the computer program Deep Blue played to a draw against the world chess champion Garry Kasparov. Under this milestone, human couldn’t even fathom how to defeat top Go players on a Go board, and some believed it to be an impossible task forever. Indeed, the \\(19 \\times 19\\) Go board has 361 intersections, and the number of possible combinations is astronomical, making it a task that even the most powerful computers couldn’t handle. Furthermore, unlike chess, every Go piece (stone) has equal values and there is no difference between them, which makes it very difficult to evaluate the situation on the board and make decisions accordingly.\n\n\n\nUnlike in chess, each piece in Go starts with the same value, but its worth on the board isn’t fixed; it changes continuously with the evolving situation. This makes assessing the situation extraordinarily difficult, let alone writing a program to evaluate it. This resonates with life—though, unlike Go pieces, we have the power to determine our own path, at least to some extent.\n\n\nHowever, in just 16 years, this last bastion of human intelligence was breached by computer programs. In 2016, a computer program, AlphaGo, defeated top Go players from South Korea for the first time in a Go competition. In one year, the new version AlphaGo, Go Master, defeated the current world No. 1 ranking player from China. After that, the strongest version of AlphaZero could give three handicaps to top professional Go players. I consider this event to be a significant milestone in the history of AI. Among the many technologies behind AlphaGo, machine learning played a significant role. Following that, various applications of machine learning blossomed, and people started applying this computer technology to a wide range of fields.\n\n\n\nMachine learning applications: 1) AlphaGo, 2) Fraud detection, 3) Application in finance, 4) Pseudo CT image, 5) Face detection, 6) Spam filter, 7) Breast cancer automatical detection based on medical image.\n\n\nFor example, people use machine learning to generate CT scan images with potential side effects from harmless MRI scans; Machine learning trains programs to assist doctors in extracting essential information from medical images; More powerful spam filter is also trained by machine learning algorithm; When you take photos with your cellphone, automatic portrait recognition and focus are also achievements of machine learning; Up until now, you can communicate with ChatGPT seamlessly and obtain reliable information, all thanks to the dividends from machine learning."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1.html#philosophy-of-machine-learning",
    "href": "Courses/c_mlwr1_2024/l1/l1.html#philosophy-of-machine-learning",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "1.2 Philosophy of Machine Learning",
    "text": "1.2 Philosophy of Machine Learning\nLet’s first discuss some metaphysical matters. What is the philosophy behind machine learning? Let’s begin with the story about handwritten digit data. In the past, postal workers had to manually sort all mails according to handwritten post code. Aside from the high labor costs, we all would consider this an extremely tedious job with a high error rate. So, people wondered if they could scan handwritten post codes into computer and then let the computer recognize the digits. This is how handwritten digit data came into existence.\nAnother story is about medical imaging. In recent years, hospitals have introduced MRI technology for medical imaging. MRI excels in presenting soft tissue, and its use of magnetic fields results in minimal harm to the body. In contrast, traditional CT imaging relies on X-rays, which can have noticeable side effects on the human body. However, CT imaging is irreplaceable when it comes to displaying the solid tissues, like skeletal structure. Therefore, people have contemplated whether it’s possible to generate corresponding CT images from MRI image data, giving rise to the concept of generative CT images.\nThe two examples have a common feature, that is we aim to predict ‘expensive’ information using ‘cheap’ information. In the story of post office, the “cheap” information is the easily obtained image data, while the recognition of the postal code is considered “expensive” information. In the medical imaging story, the acquisition of MRI image data carries far less risk compared to the risks associated with CT images. From this perspective, MRI data is indeed much more cost-effective than CT data. Therefore, the basic idea of machine learning is to train a “machine” to transform “cheap” information into “expensive” information through data. In this way, expensive information is replaced by cheap information through machine learning models, thus avoiding high costs, unnecessary error costs, and additional risks.\n\n\n\nPhilosophy of machine learning. Obviously, water cannot be turned into oil—this is simply a metaphor. A successful project requires researchers to maintain an honest attitude; unrealistic ideas, like a washing machine, will only produce waste water."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1.html#machine-learning-process",
    "href": "Courses/c_mlwr1_2024/l1/l1.html#machine-learning-process",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "1.3 Machine Learning Process",
    "text": "1.3 Machine Learning Process\nWhat is the process of machine learning like? First, let me tell you about some observations I have made about my sons. After they learned to speak, they began asking me all sorts of questions. For example, when we were at the supermarket, he would point at apples and ask me, ‘What’s this, Daddy?’ I just simply answered them, “It is an apple.” After a few times, they would change their questioning style from special to general, like, ’Daddy, is this an apple? In about half months, they turned into high-precision apple classifiers. They could even recognize that the logo on my laptop is also an apple! Amazing! I must emphasize that I never taught them how to recognize apples.\n\n\n\nI have two boys at home. On the LHS, the boy wearing his pants frontside back is my elder son, Siyi, when he was three years old. He was earnestly planting flowers in the artificial soccer field. On the RHS, the guy who resembles a sloth is my younger son, Siqi. It is quite evident that he is a happy fellow. Actually, he is very quiet and cool.\n\n\nWe can summarize the human learning process from the example of my sons learning to recognize apples. First, they would accumulate experience through observation and questioning. Once they had enough experience, they would begin their own learning and distill this into “knowledge.” Subsequently, they would use general questions to validate their knowledge. Finally, they would use their validated knowledge to identify the Apple logo. This human learning process is summarized in the following figure (up).\nIn fact, if we just change the names of the components, this is also the process of machine learning. For computer programs, “experience” is essentially “data”, “learning” involves “training” with algorithms, for example proceptron algorithm, and the “knowledge” distilled is a type of “model”. We call the “self-exam process” as “validation” and “applying” it to new problems as “generalization”. The entire machine learning process is presented in the following figure (down). In this course, we focus on “training” and “validation” steps. For “training” step, we introduce several basic and fundamental algorithms for linear models and discuss several validation methods for “validation” step. See Figure below.\n\n\n\nThe human learning process (up) V.S. machine learning process (down)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1.html#machine-learning-abc",
    "href": "Courses/c_mlwr1_2024/l1/l1.html#machine-learning-abc",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "1.4 Machine Learning ABC",
    "text": "1.4 Machine Learning ABC\nIn machine learning, variables are often split into feature variables and target variables. Feature variables are the inputs to the model—information that helps the model make predictions—while target variables are the outcomes or labels the model is intended to predict. For instance, in the case of handwritten digit recognition, each pixel value in the image of a digit acts as a feature, providing the model with clues about the visual patterns, while the digit number itself (such as “5” or “9”) is the target variable. Similarly, in medical imaging applications like pseudo-CT imaging, the pixel values from an MRI image may serve as features, and the corresponding CT image’s pixel values become the target, as the model aims to predict CT values based on MRI inputs. There are many ways to categorize machine learning, with the most common being supervised and unsupervised learning. This categorization primarily depends on whether target variables are included in the research problem.\nIn supervised learning, a dataset includes known labels for each observations, which the model uses to learn relationships between features and targets. In a mathematical language, we try to find a map \\(f\\), or a function, that connect the features information and target information. For example, the Iris dataset is labeled with flower species (such as Setosa or Versicolor) based on measurements like petal length and width, which act as feature variables. The goal is for the model to learn these relationships so it can classify new, unseen examples accurately. This mapping, \\(f\\), also known as the model, has its functionality determined by model parameters, which are adjusted based on the data. The process of determining the “optimal” parameters is also called to ‘learning’. Once the optimal model parameters are set, the model is considered trained. In the case of the Iris example, for those who often confuse the three subspecies, the shape data of the flower can be used to predict the species. Many plant identification Apps work in this way. Supervised learning is often further divided into regression and classification problems, depending on the type of target variable. We will focus on this distinction in the following section.\nIn unsupervised learning, on the other hand, the dataset has no target variables, and the model’s task is to find underlying patterns or groupings in the data, such as clustering the Iris dataset’s measurements into groups without knowing the species in advance. In mathematical language, in unsupervised learning problem, we also want to learn a map \\(g\\) that connect feature variables and some “new” knowledge. In statistics, we often use the term “latent variable” or “latent information” to represent this “new” knowledge. The beautiful names Setosa, Versicolor, and Virginica did not exist before botanists classified and named them scientifically. This new knowledge emerged from analyzing data on the shapes of the flowers. In machine learning, we typically encounter two types of unsupervised learning problems: feature analysis and cluster analysis. In the first part of this course, we will not cover these topics.\n\nQuiz: In fact, we have encountered similar unsupervised learning problems in basic statistics. Do you know what they are?\n\n\n\n\nFeature variables V.S. Target Variable; Supervised Learning V.S. Unsupervised Learning"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1.html#regression-model-and-classification-model",
    "href": "Courses/c_mlwr1_2024/l1/l1.html#regression-model-and-classification-model",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "1.5 Regression Model and Classification Model",
    "text": "1.5 Regression Model and Classification Model\n\nRegression Model\nThe machine learning problem can be understood as regression problem when the target variable is a continuous variable. For example, predict the house price based on different feature variables; predict the pixel values of CT scans based on MRI scans; predict the stock price based on feature variables of market. A simple scenario displayed in the figure below, a basic regression model is a linear model, \\(y_i = w_0 + w_1x_i + \\epsilon_i\\). From a geometric perspective, a linear regression model can be seen as a straight line that passes through all sample observations. In the generalization stage, the target value can be predicted from feature variable through the regression model.\n\n\n\nRegression Problem\n\n\n\n\nClassification Model\nThe problem can be viewed as a classification problem when the target variable is categorical. We often refer to this type of target variable as labels. For example, in the classification with Iris data, the species variable is the label variable, and we aim for finding a good “function” taking 4 shaping variables as input to predict the labels based on data. This function is often refer to a classifier. So, what kind of function can perform this role? Let’s take a look at a real classifier first, a “coin sorter.” Its operation is quite simple, as it classifies coins based on their different diameters. Inside the machine, there are holes of varying sizes corresponding to different diameters, and through vibration, coins will fall into the holes that match their size. In essence, it’s classifying by comparing a variable to a threshold value. The idea is quite simple, but it is just the essential idea of machine learning classifier.\n\n\n\nLHS: Classification with iris data. RHS: A real classifier, coin sorter. The working principle: Variable (diameter) V.S. Threshold value.\n\n\nWell, usually we have multiple feature variables in a classification problem, then how do we apply this simple working principle to design a classifier? Let’s see another example. You might not know yet, in fact, teacher becomes a classifier after an exam. Well, to pass or not to pass is a classification problem. Suppose, in a secret exam, each student answers 5 questions and each question is worth 20 points. Student passes the exam if the total points are larger or equal to 60. I have corrected all the exams; the results are summarized in Table 1, and \\(1\\) indicating the question was correctly answered and \\(0\\) indicating not. Then, who can pass the exam?\n\n\n\n\n\nI believe it is a very simple problem, for example, Super girl correctly answered 4 questions and get 80 points that is above the threshold value 60, so she passed the exam! However, spiderman only got 20 points that is lower than 60, so he can’t pass. If we clearly write down the calculation process, we actually used the following formula to calculate the totol score, then compare the total score with the critical point, 60.\n\\[\n  20\\times Q_1 + 20\\times Q_2 + 20\\times Q_3 + 20\\times Q_4 + 20\\times Q_5 \\geq 60\n\\]\n\nNow, we know what a simple classifier looks like. Essentially, it is a two-step procedure. We create a single variable through the weighted sum of all feature variables first, then compare the resulting value with a threshold value. In formal, the classifier can be represented as\n\\[\n  y = \\text{Sign}(w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_p x_p)\n\\] where \\(\\text{Sign}(x)\\) is a sign function returning 1 if \\(x&gt;0\\) and 0 if \\(x&lt;0\\). We refer coefficients \\(w_1, \\dots, w_p\\) as weights, the weighted sum of feature variables \\(w_1 x_1 + w_2 x_2 + \\dots + w_p x_p\\) as scores and \\(w_0\\), the threshold value, as bias. If the score value is equal to 0, then this observation can’t be classified by this classifier, and the thing we can do best is flip a coin to make the decision. We call all the points that satisfy equation \\(w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_p x_p = 0\\) as the decision boundary. For example, in a 2D feature space, the decision boundary \\(w_0 + w_1x_1 + w_2x_2 =0\\) is just a straight line with a slope of \\(-w_1/w_2\\) and an intercept of \\(-w_0/w_2\\), see the figure below.\n\n\n\nIn this example, the 2D feature space is cut into two parts by the decision boundary (red line). For any unlabeled observations (blue dots), if it is above the decision boundary, then it will be classified as yellow , otherwise, green.\n\n\nThis kind of classifier is called linear classifier, since the decision boundary is presented by a linear function. It is a straight line in 2D space, a plane in 3D space, and hyper-plane in a higher dimension space. You might have already realized that in fact, a classifier is solely determined by its weights and bias, and machine learning algorithms tell us how to find the optimal weights and bias through data. There are several classical methods (algorithms) for learning a linear classifier which are perceptron algorithm, linear discriminant analysis, logistic regression, and maximum margin classifier. In this course, we will introduce all of them except maximum margin classifier.\nRemark: Just as all the rules of arithmetic start with \\(1+1\\), don’t underestimate this linear classifier. You will see that all complex classifiers are built upon them. For example, maximum margin classifier is the foundation of SVM (Support vector machine) which dominate machine learning world for 20 years, the perceptron algorithm is the starting point of artificial neural net works, and no matter how complex a neural network architecture may be, as long as it is a classifier, its final layer will inevitably be a logistic regression model.\n\nLecture 1 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_2.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_2.html",
    "title": "1.2 Philosophy of Machine Learning",
    "section": "",
    "text": "Let’s first discuss some metaphysical matters. What is the philosophy behind machine learning? Let’s begin with the story about handwritten digit data. In the past, postal workers had to manually sort all mails according to handwritten post code. Aside from the high labor costs, we all would consider this an extremely tedious job with a high error rate. So, people wondered if they could scan handwritten post codes into computer and then let the computer recognize the digits. This is how handwritten digit data came into existence.\nAnother story is about medical imaging. In recent years, hospitals have introduced MRI technology for medical imaging. MRI excels in presenting soft tissue, and its use of magnetic fields results in minimal harm to the body. In contrast, traditional CT imaging relies on X-rays, which can have noticeable side effects on the human body. However, CT imaging is irreplaceable when it comes to displaying the solid tissues, like skeletal structure. Therefore, people have contemplated whether it’s possible to generate corresponding CT images from MRI image data, giving rise to the concept of generative CT images.\nThe two examples have a common feature, that is we aim to predict ‘expensive’ information using ‘cheap’ information. In the story of post office, the “cheap” information is the easily obtained image data, while the recognition of the postal code is considered “expensive” information. In the medical imaging story, the acquisition of MRI image data carries far less risk compared to the risks associated with CT images. From this perspective, MRI data is indeed much more cost-effective than CT data. Therefore, the basic idea of machine learning is to train a “machine” to transform “cheap” information into “expensive” information through data. In this way, expensive information is replaced by cheap information through machine learning models, thus avoiding high costs, unnecessary error costs, and additional risks.\n\n\n\nPhilosophy of machine learning. Obviously, water cannot be turned into oil—this is simply a metaphor. A successful project requires researchers to maintain an honest attitude; unrealistic ideas, like a washing machine, will only produce waste water.\n\n\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_1.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_1.html",
    "title": "1.1 Mailstone of AI, AlphaGo, 2016",
    "section": "",
    "text": "Machine learning is not a new concept, but the latest version of the machine learning legend has indeed just happened recently. Have you heard about AlphaGo? AlphaGo is an artificial intelligence program developed by DeepMind that made history in 2016 by defeating a world champion Go player.\n\n\n\nL: Go is an ancient board game originating from China over 2,500 years ago, in which two players compete to capture territory using black and white stones on a grid. The game’s simple rules allow for deep strategic complexity, making it one of the most intellectually challenging games in the world. R: AlphaGo is an artificial intelligence program developed by DeepMind that became the first to defeat a professional human player, and eventually world champions, in the complex board game Go, showcasing a major milestone in AI’s capabilities in strategic thinking and decision-making. Source: Google search.\n\n\nBack in the year 2000, the computer program Deep Blue played to a draw against the world chess champion Garry Kasparov. Under this milestone, human couldn’t even fathom how to defeat top Go players on a Go board, and some believed it to be an impossible task forever. Indeed, the \\(19 \\times 19\\) Go board has 361 intersections, and the number of possible combinations is astronomical, making it a task that even the most powerful computers couldn’t handle. Furthermore, unlike chess, every Go piece (stone) has equal values and there is no difference between them, which makes it very difficult to evaluate the situation on the board and make decisions accordingly.\n\n\n\nUnlike in chess, each piece in Go starts with the same value, but its worth on the board isn’t fixed; it changes continuously with the evolving situation. This makes assessing the situation extraordinarily difficult, let alone writing a program to evaluate it. This resonates with life—though, unlike Go pieces, we have the power to determine our own path, at least to some extent.\n\n\nHowever, in just 16 years, this last bastion of human intelligence was breached by computer programs. In 2016, a computer program, AlphaGo, defeated top Go players from South Korea for the first time in a Go competition. In one year, the new version AlphaGo, Go Master, defeated the current world No. 1 ranking player from China. After that, the strongest version of AlphaZero could give three handicaps to top professional Go players. I consider this event to be a significant milestone in the history of AI. Among the many technologies behind AlphaGo, machine learning played a significant role. Following that, various applications of machine learning blossomed, and people started applying this computer technology to a wide range of fields.\n\n\n\nMachine learning applications: 1) AlphaGo, 2) Fraud detection, 3) Application in finance, 4) Pseudo CT image, 5) Face detection, 6) Spam filter, 7) Breast cancer automatical detection based on medical image.\n\n\nFor example, people use machine learning to generate CT scan images with potential side effects from harmless MRI scans; Machine learning trains programs to assist doctors in extracting essential information from medical images; More powerful spam filter is also trained by machine learning algorithm; When you take photos with your cellphone, automatic portrait recognition and focus are also achievements of machine learning; Up until now, you can communicate with ChatGPT seamlessly and obtain reliable information, all thanks to the dividends from machine learning.\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_4.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_4.html",
    "title": "1.4 Machine Learning ABC",
    "section": "",
    "text": "In machine learning, variables are often split into feature variables and target variables. Feature variables are the inputs to the model—information that helps the model make predictions—while target variables are the outcomes or labels the model is intended to predict. For instance, in the case of handwritten digit recognition, each pixel value in the image of a digit acts as a feature, providing the model with clues about the visual patterns, while the digit number itself (such as “5” or “9”) is the target variable. Similarly, in medical imaging applications like pseudo-CT imaging, the pixel values from an MRI image may serve as features, and the corresponding CT image’s pixel values become the target, as the model aims to predict CT values based on MRI inputs. There are many ways to categorize machine learning, with the most common being supervised and unsupervised learning. This categorization primarily depends on whether target variables are included in the research problem.\nIn supervised learning, a dataset includes known labels for each observations, which the model uses to learn relationships between features and targets. In a mathematical language, we try to find a map \\(f\\), or a function, that connect the features information and target information. For example, the Iris dataset is labeled with flower species (such as Setosa or Versicolor) based on measurements like petal length and width, which act as feature variables. The goal is for the model to learn these relationships so it can classify new, unseen examples accurately. This mapping, \\(f\\), also known as the model, has its functionality determined by model parameters, which are adjusted based on the data. The process of determining the “optimal” parameters is also called to ‘learning’. Once the optimal model parameters are set, the model is considered trained. In the case of the Iris example, for those who often confuse the three subspecies, the shape data of the flower can be used to predict the species. Many plant identification Apps work in this way. Supervised learning is often further divided into regression and classification problems, depending on the type of target variable. We will focus on this distinction in the following section.\nIn unsupervised learning, on the other hand, the dataset has no target variables, and the model’s task is to find underlying patterns or groupings in the data, such as clustering the Iris dataset’s measurements into groups without knowing the species in advance. In mathematical language, in unsupervised learning problem, we also want to learn a map \\(g\\) that connect feature variables and some “new” knowledge. In statistics, we often use the term “latent variable” or “latent information” to represent this “new” knowledge. The beautiful names Setosa, Versicolor, and Virginica did not exist before botanists classified and named them scientifically. This new knowledge emerged from analyzing data on the shapes of the flowers. In machine learning, we typically encounter two types of unsupervised learning problems: feature analysis and cluster analysis. In the first part of this course, we will not cover these topics.\n\nQuiz: In fact, we have encountered similar unsupervised learning problems in basic statistics. Do you know what they are?\n\n\n\n\nFeature variables V.S. Target Variable; Supervised Learning V.S. Unsupervised Learning\n\n\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_home.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_home.html",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "",
    "text": "In this lecture, we introduce machine learning and related basic concepts. It covers the following things:"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_home.html#lecture-notes",
    "href": "Courses/c_mlwr1_2024/l1/l1_home.html#lecture-notes",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "Lecture notes:",
    "text": "Lecture notes:\n\nIntroduction to Machine Learning:\n\nRead the integrated notes: here;\nRead the pagination notes: here;\nDownload the PDF notes: here.\n\nProceptron and Its Algorithm*:\n\nRead the integrated notes: here;\nRead the pagination notes: here;\nDownload the PDF notes: here."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_home.html#reading-guidelines-of-textbook",
    "href": "Courses/c_mlwr1_2024/l1/l1_home.html#reading-guidelines-of-textbook",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "Reading Guidelines of textbook:",
    "text": "Reading Guidelines of textbook:\nFor Lecture 1, it is recommended that you read the following sections in the textbook.\n\nChapter 1: Read pages 1 - 14. Extra attention to ‘Notation and Simple Matrix Algebra’ if necessary.\nChapter 2: Read sections 2.1, pages 15 - 28.\n\n\nCourse Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_home.html",
    "href": "Courses/c_mlwr1_2024/l6/l6_home.html",
    "title": "Lecture 6: Model Selection",
    "section": "",
    "text": "In this lecture,\nCourse Homepage"
  },
  {
    "objectID": "03_s_index.html",
    "href": "03_s_index.html",
    "title": "Skalds",
    "section": "",
    "text": "I don’t have the ability to create immortal poetry, but I would like to share some of my own short writings here. Of course, if I am diligent enough.\n\n\n\n\nSkalds were poets in Norse and Viking Age societies, known for composing and reciting poetry that celebrated heroes, gods, and important events. They played a significant role in preserving history and cultural traditions through oral storytelling, often in the courts of kings and chieftains across Scandinavia and Iceland. Source: google search."
  }
]