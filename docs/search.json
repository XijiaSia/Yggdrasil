[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Space!",
    "section": "",
    "text": "This is an important place for communication with my students. I store my main teaching materials here and share them with everyone. As I mentioned in the caption under the Yggdrasil image below, I hope that in the near future, I can weave all the learning materials into the world tree of data science.\n\nWhat can you find in my space?\n\nNorn’s Blessing: it lists my courses about statistics, data science, machine learning.\nMímisbrunnr: A math tool box. Students can be directed to the right items whenever they need.\nSkalds: Some of my short notes are listed here.\n\n\n\n\n\nYggdrasil is often referred to as the “world tree” in Norse mythology. It is an immense and central sacred tree that connects different worlds in Norse cosmology. My vision is to weave a Yggdrasil for data science. Source: Wikipidia"
  },
  {
    "objectID": "Skalds/20241028_Cracking the Code.html",
    "href": "Skalds/20241028_Cracking the Code.html",
    "title": "Cracking the Code",
    "section": "",
    "text": "In this section, we discuss a cryptography problem that Prof. Persi Diaconis introduced in his paper. He used this example to introduce the Markov Chain Monte Carlo (MCMC) approach. This is a beautiful example not only for introducing MCMC but also a nice example to demonstrate the “routine” of using data to solve real problems. We can also review some basic probability knowledge and concepts. In addition, it is a good programming exercise, and you may try to implement it by yourself.\nThe story takes place in a prison in the United States. One day, the police officer found many strange symbols on a piece of paper, see figure below. It turned out that some prisoners wanted to exchange secret information through ciphertext. The officer first visited a linguist and was told that the cipher text might be a simple substitution cipher. The linguists can’t understand the cipher either but recommend seeking help from statisticians."
  },
  {
    "objectID": "Skalds/20241028_Cracking the Code.html#problem-understanding",
    "href": "Skalds/20241028_Cracking the Code.html#problem-understanding",
    "title": "Cracking the Code",
    "section": "Problem Understanding",
    "text": "Problem Understanding\nFirst of all, let’s understand what is a substitution cipher. It is a very simple cipher using fixed symbols to replace letters, punctuation marks, numbers, and spaces. Then the key of this kind of cipher is a map from the symbol space to the letter space.\n\\[\n  f: \\{ \\text{code space} \\} \\to \\{ \\text{alphabet} \\}\n\\]\nWe use the letter itself instead of symbols to simplify the problem and make it easier to express. For example, suppose the key is\n\\[\n  f: \\{ \\text{d} \\to \\text{a}, \\text{a} \\to \\text{t}, \\text{t} \\to \\text{d}, \\dots \\}\n\\]\nAccording to this key, the regular English word “data” is encrypted as “atdt”. Next, same as the original paper, we take a small piece from the well-known section of Shakespeare’s Hamlet as a simple example.\n\n\nenter hamlet ham to be or not to be that is the question whe ther tis nobler in \nthe mind to suffer the slings and arrows of outrageous fortune or to take arms against \na sea of troubles and by opposing end.\n\n\nWe randomly generate a substitution cipher:\n\n\ntuntvmfeprtnmfepmnsmgtmsvmusnmnsmgtmnfenmjbmnftmatbnjsumdftnftvmnjbmusgrtv \nmjumnftmpjuymnsmbiitvmnftmbrjuhbmeuymevvsdbmsims nvehts bmisvn \nutmsvmnsmnextmevpb mehejubnmembtemsimnvs g rtbmeuymgwmskksbjuhmtuy\n\n\nOne can apply inverse mapping to translate the cipher and retrieve the secret information. In other words, one can understand the cipher if holds the key. Thus the problem is to find the key (mapping \\(f\\)). Well, it is also obviously not such an easy task. The size of the symbol space is too big to simply apply a brute force algorithm to search the key. The size of the symbol space is \\(27\\) even though we only consider letters in lowercase and space. In such a case, the total number of possible keys is \\(27! = 1.08\\times 10^{28}\\). So we have to figure out a smart approach that takes some prior information into account to search the key! The prior information leads to our assumptions and the model on top of it."
  },
  {
    "objectID": "Skalds/20241028_Cracking the Code.html#model-and-assumption",
    "href": "Skalds/20241028_Cracking the Code.html#model-and-assumption",
    "title": "Cracking the Code",
    "section": "Model and Assumption",
    "text": "Model and Assumption\nFor a natural language, we believe the alphabetical order should obey certain rules. For example, the probability of the letter ‘a’ being connected by the letter ‘b’ must be greater than that of the letter ‘x’. Now we translate this prior knowledge as an assumption. To do so, we view the sequence of letters in a text as a stochastic process. There could be \\(27\\) possible values for each time point (or position). We call the possible values (letters) as state. Then we can assume that a sequence of letters in a text should satisfy the first-order Markov properties.\n\\[\n  \\Pr(x_t | x_{1}, x_{2}, \\dots, x_{t-1}) = \\Pr(x_t | x_{t-1})\n\\]\nwhere \\(x_i\\) present the state at the \\(t\\)-th position in a text. In other words, the letter in \\(t\\)-th position only depends on the letter in the previous position. This assumption implies two things. First, the transition between two letters in a text should satisfy some transition probabilities and this can be summarized in a transition probability matrix,\n\\[\n    \\begin{pmatrix}\n        & a & d & t & \\dots \\\\\n        a & p_{11} & p_{12} & p_{13} & \\dots \\\\\n        d & p_{21} & p_{22} & p_{23} & \\dots \\\\\n        t & p_{31} & p_{32} & p_{33} & \\dots \\\\\n        \\vdots & \\vdots & \\vdots & \\vdots  & \\ddots\n    \\end{pmatrix}\n\\]\nThe second thing is the likelihood of a text can be factorized as the product of the corresponding transition probabilities. For example, the likelihood of ‘data’ is\n\\[\n  \\ell('data') = \\Pr(d\\to a)\\Pr(a\\to t)\\Pr(t\\to a) = p_{21}p_{13}p_{31},\n\\]\nbut the likelihood of ‘atdt’ is\n\\[\n  \\ell('atdt') = \\Pr(a\\to t)\\Pr(t\\to d)\\Pr(d\\to t) = p_{13}p_{32}p_{23}.\n\\]\nSince ‘atdt’ is much weirder than ‘data’, the likelihood of the former must be lower than the latter, \\(\\ell('atdt') &lt; \\ell('data').\\) Given the two thoughts, we can summarize our problem as an optimization problem and present it in a mathematical language,\n\\[\n  \\max_{f} \\ell( f( \\text{Cipher}) )\n\\]\nSo the problem is to find a key \\(f\\) that maximizes the likelihood of the translated text, \\(f(\\text{Cipher})\\). Great! We have obtained a promising idea, however, it is still not clear how to search for the key in a smart way. Before we discuss the final solution, we need to first discuss one practical issue, that is how to obtain the transition probability matrix."
  },
  {
    "objectID": "Skalds/20241028_Cracking the Code.html#model-estimation",
    "href": "Skalds/20241028_Cracking the Code.html#model-estimation",
    "title": "Cracking the Code",
    "section": "Model Estimation",
    "text": "Model Estimation\nAt this stage, we can finally invite our protagonist, data. So far, we have a probability model, the first-order Markov chain model. It is a parametric model and the parameter is the transition probability matrix. To be a little bit mysterious, we can’t know the real model coefficients set by God, we can only try to figure out his mind through the data generated by the model he created. Seriously speaking, we need to use the data to estimate the parameters of the model. For this problem, we can download classical books in English and that is our data. More specifically, we can count the frequencies of all \\(27^2\\) different transitions in a book. Then normalize all the frequencies by the total number of transitions as the estimation of the transition probabilities. You can see part of the estimation of the transition probability matrix bellow\n\\[\n  \\begin{matrix}\n& A & B & C & D & E & F & G & H & I & \\dots \\\\\nA & 0.000 & 0.017 & 0.034 & 0.055 & 0.001 & 0.008 & 0.017 & 0.002 & 0.043 & \\dots \\\\\nB & 0.093 & 0.007 & 0.000 & 0.000 & 0.328 & 0.000 & 0.000 & 0.000 & 0.029 & \\dots \\\\\nC & 0.110 & 0.000 & 0.017 & 0.000 & 0.215 & 0.000 & 0.000 & 0.184 & 0.042 & \\dots \\\\\nD & 0.021 & 0.000 & 0.000 & 0.011 & 0.116 & 0.001 & 0.004 & 0.000 & 0.066 & \\dots \\\\\nE & 0.042 & 0.001 & 0.017 & 0.091 & 0.026 & 0.010 & 0.007 & 0.002 & 0.011 & \\dots \\\\\nF & 0.071 & 0.001 & 0.000 & 0.000 & 0.084 & 0.052 & 0.000 & 0.000 & 0.085 & \\dots \\\\\nG & 0.066 & 0.000 & 0.000 & 0.001 & 0.115 & 0.000 & 0.009 & 0.117 & 0.049 & \\dots \\\\\nH & 0.164 & 0.000 & 0.000 & 0.000 & 0.450 & 0.000 & 0.000 & 0.000 & 0.156 & \\dots \\\\\nI & 0.017 & 0.008 & 0.049 & 0.050 & 0.049 & 0.022 & 0.025 & 0.000 & 0.001 & \\dots \\\\\n\\vdots  & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots  \n\\end{matrix}\n\\]\nOnce the transition probability is ready, then we can use it to evaluate the likelihood of a text. For example, the likelihood of the original text and the cipher is \\(-458.26\\) and respectively."
  },
  {
    "objectID": "Skalds/20241028_Cracking the Code.html#inference",
    "href": "Skalds/20241028_Cracking the Code.html#inference",
    "title": "Cracking the Code",
    "section": "Inference",
    "text": "Inference\nGreat! Now we can discuss how to find the optimal key. Imagine that all the \\(27!\\) possible keys form a huge solutions space. Similar to most optimization algorithms, we can start from a certain initial value, and then gradually approach the optimal solution through some iterative mechanism. (In fact, this is not very accurate, but we can understand it this way first, and later I will explain what our algorithm is doing.) Firstly, we randomly generate an initial key, for example\n\\[\n  f^{(0)}: \\{ \\text{a} \\to \\text{f}, \\text{b} \\to \\text{g}, \\text{c} \\to \\text{m}, \\dots \\},\n\\]\nand evaluate the likelihood of this key \\(\\ell(f^{(0)}(cipher))\\). Then we randomly move a small step to a new key \\(f^{(1)}\\) by making a random transposition of the values \\(f^{(0)}\\) assigned to two symbols, for example,\n\\[\n  f^{(1)}: \\{ \\text{a} \\to \\text{m}, \\text{b} \\to \\text{g}, \\text{c} \\to \\text{f}, \\dots \\}\n\\]\nIs the new \\(f^{(1)}\\) key a good proposal? We can calculate its likelihood value and compare it with the previous key. The simple situation is that if the likelihood value of the new key is higher than the previous one, \\(\\ell(f^{(1)}) &gt; \\ell(f^{(0)})\\), we should keep the new key. The interesting question is what if the new key is worse? Simply discard this new key? It seems to be a good way, but there is a risk of falling into the local area and finally missing the optimal key. What is the good way then? There are two situations. If the likelihood value of the new key is much lower than the previous key, then we should be inclined to abandon it; however, if the likelihood values of the two keys are almost the same, then we should tend to keep it. This plan sounds more reasonable, but the question is how exactly realize ‘tend to’? We can calculate the ratio, \\(\\frac{ \\ell(f^{(t)}) }{ \\ell( f^{(t-1)}}\\). This value, between 0 and 1, can be regarded as a probability that an action will be executed. If we generate a Bernoulli-distributed random number with this probability, and use it to determine whether to keep the new key, then the above operation can be achieved. We summarize this approach in the following algorithm\nInitialize the key f(0)\nfor(i in 1:R){\n  Step 1. Evaluate the likelihood value of f(t-1), likelihood(f(t-1))\n  Step 2. Propose a new key f(t) by making a random transposition of the values of f(t-1) assigned to two letters.\n  Step 3. Make decision, either keep the new key or stay with the old key\n  if(likelihood(f(t)) &gt; likelihood(f(t-1))){\n    keep f(t)\n  } else{\n    generate u ~ Ber(f(t)/f(t-1))\n    if(u == 1){\n      keep f(t)  \n    } else{\n      stay at f(t-1)\n    }\n  }\n}\nThe performance of our algorithm can be glimpsed from the following R outputs.\n\n\n\nR outputs"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_5.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_5.html",
    "title": "5. Regression Model and Classification Model",
    "section": "",
    "text": "The machine learning problem can be understood as regression problem when the target variable is a continuous variable. For example, predict the house price based on different feature variables; predict the pixel values of CT scans based on MRI scans; predict the stock price based on feature variables of market. A simple scenario displayed in the figure below, a basic regression model is a linear model, \\(y_i = w_0 + w_1x_i + \\epsilon_i\\). From a geometric perspective, a linear regression model can be seen as a straight line that passes through all sample observations. In the generalization stage, the target value can be predicted from feature variable through the regression model.\n\n\n\n\n\nFigure 8: Regression Problem"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_5.html#regression-model",
    "href": "Courses/c_mlwr1_2024/l1/l1_5.html#regression-model",
    "title": "5. Regression Model and Classification Model",
    "section": "",
    "text": "The machine learning problem can be understood as regression problem when the target variable is a continuous variable. For example, predict the house price based on different feature variables; predict the pixel values of CT scans based on MRI scans; predict the stock price based on feature variables of market. A simple scenario displayed in the figure below, a basic regression model is a linear model, \\(y_i = w_0 + w_1x_i + \\epsilon_i\\). From a geometric perspective, a linear regression model can be seen as a straight line that passes through all sample observations. In the generalization stage, the target value can be predicted from feature variable through the regression model.\n\n\n\n\n\nFigure 8: Regression Problem"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_5.html#classification-model",
    "href": "Courses/c_mlwr1_2024/l1/l1_5.html#classification-model",
    "title": "5. Regression Model and Classification Model",
    "section": "5.2 Classification Model",
    "text": "5.2 Classification Model\nThe problem can be viewed as a classification problem when the target variable is categorical. We often refer to this type of target variable as labels. For example, in the classification with Iris data, the species variable is the label variable, and we aim for finding a good “function” taking 4 shaping variables as input to predict the labels based on data. This function is often refer to a classifier. So, what kind of function can perform this role? Let’s take a look at a real classifier first, a “coin sorter.” Its operation is quite simple, as it classifies coins based on their different diameters. Inside the machine, there are holes of varying sizes corresponding to different diameters, and through vibration, coins will fall into the holes that match their size. In essence, it’s classifying by comparing a variable to a threshold value. The idea is quite simple, but it is just the essential idea of machine learning classifier.\n\n\n\n\nFigure 9: LHS: Classification with iris data. RHS: A real classifier, coin sorter. The working principle: Variable (diameter) V.S. Threshold value.\n\n\n\nWell, usually we have multiple feature variables in a classification problem, then how do we apply this simple working principle to design a classifier? Let’s see another example. You might not know yet, in fact, teacher becomes a classifier after an exam. Well, to pass or not to pass is a classification problem. Suppose, in a secret exam, each student answers 5 questions and each question is worth 20 points. Student passes the exam if the total points are larger or equal to 60. I have corrected all the exams; the results are summarized in Table 1, and \\(1\\) indicating the question was correctly answered and \\(0\\) indicating not. Then, who can pass the exam?\n\n\n\n\n\nI believe it is a very simple problem, for example, Super girl correctly answered 4 questions and get 80 points that is above the threshold value 60, so she passed the exam! However, spiderman only got 20 points that is lower than 60, so he can’t pass. If we clearly write down the calculation process, we actually used the following formula to calculate the totol score, then compare the total score with the critical point, 60.\n\\[\n  20\\times Q_1 + 20\\times Q_2 + 20\\times Q_3 + 20\\times Q_4 + 20\\times Q_5 \\geq 60\n\\]\n\nNow, we know what a simple classifier looks like. Essentially, it is a two-step procedure. We create a single variable through the weighted sum of all feature variables first, then compare the resulting value with a threshold value. In formal, the classifier can be represented as\n\\[\n  y = \\text{Sign}(w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_p x_p)\n\\]\nwhere \\(\\text{Sign}(x)\\) is a sign function returning 1 if \\(x&gt;0\\) and 0 if \\(x&lt;0\\). We refer coefficients \\(w_1, \\dots, w_p\\) as weights, the weighted sum of feature variables \\(w_1 x_1 + w_2 x_2 + \\dots + w_p x_p\\) as scores and \\(w_0\\), the threshold value, as bias. If the score value is equal to 0, then this observation can’t be classified by this classifier, and the thing we can do best is flip a coin to make the decision. We call all the points that satisfy equation \\(w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_p x_p = 0\\) as the decision boundary. For example, in a 2D feature space, the decision boundary \\(w_0 + w_1x_1 + w_2x_2 =0\\) is just a straight line with a slope of \\(-w_1/w_2\\) and an intercept of \\(-w_0/w_2\\), see the figure below.\n\n\n\n\nFigure 10: In this example, the 2D feature space is cut into two parts by the decision boundary (red line). For any unlabeled observations (blue dots), if it is above the decision boundary, then it will be classified as yellow , otherwise, green.\n\n\n\nThis kind of classifier is called linear classifier, since the decision boundary is presented by a linear function. It is a straight line in 2D space, a plane in 3D space, and hyper-plane in a higher dimension space. You might have already realized that in fact, a classifier is solely determined by its weights and bias, and machine learning algorithms tell us how to find the optimal weights and bias through data. There are several classical methods (algorithms) for learning a linear classifier which are perceptron algorithm, linear discriminant analysis, logistic regression, and maximum margin classifier. In this course, we will introduce all of them except maximum margin classifier.\nRemark: Just as all the rules of arithmetic start with \\(1+1\\), don’t underestimate this linear classifier. You will see that all complex classifiers are built upon them. For example, maximum margin classifier is the foundation of SVM (Support vector machine) which dominate machine learning world for 20 years, the perceptron algorithm is the starting point of artificial neural net works, and no matter how complex a neural network architecture may be, as long as it is a classifier, its final layer will inevitably be a logistic regression model.\n\nPrevious page | Lecture 1 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_0.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_0.html",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "",
    "text": "In this lecture, we introduce machine learning to you. You will learn the basic elements in this field and an old, basic, but very interesting algorithm in machine learning.\nNext, I’ll begin with a review of key milestones in artificial intelligence. Then, we’ll delve into some metaphysical concepts, exploring the underlying logic of machine learning. By understanding the human learning process, we’ll gain insight into the entire machine learning process. Finally, after covering the ABCs of machine learning, we’ll focus on the fundamental forms of machine learning models.\nOutline：\n\n1.1 Mailstone of AI, AlphaGo, 2016\n1.2 Philosophy of Machine Learning\n1.3 Machine Learning Process\n1.4 Machine Learning ABC\n1.5 Regression Model and Classification Model\n\n\nLecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_3.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_3.html",
    "title": "3. Machine Learning Process",
    "section": "",
    "text": "What is the process of machine learning like? First, let me tell you about some observations I have made about my sons. After they learned to speak, they began asking me all sorts of questions. For example, when we were at the supermarket, he would point at apples and ask me, ‘What’s this, Daddy?’ I just simply answered them, “It is an apple.” After a few times, they would change their questioning style from special to general, like, ’Daddy, is this an apple? In about half months, they turned into high-precision apple classifiers. They could even recognize that the logo on my laptop is also an apple! Amazing! I must emphasize that I never taught them how to recognize apples.\n\n\n\n\nFigure 5: I have two boys at home. On the LHS, the boy wearing his pants frontside back is my elder son, Siyi, when he was three years old. He was earnestly planting flowers in the artificial soccer field. On the RHS, the guy who resembles a sloth is my younger son, Siqi. It is quite evident that he is a happy fellow. Actually, he is very quiet and cool.\n\n\n\nWe can summarize the human learning process from the example of my sons learning to recognize apples. First, they would accumulate experience through observation and questioning. Once they had enough experience, they would begin their own learning and distill this into “knowledge.” Subsequently, they would use general questions to validate their knowledge. Finally, they would use their validated knowledge to identify the Apple logo. This human learning process is summarized in the following figure (up).\nIn fact, if we just change the names of the components, this is also the process of machine learning. For computer programs, “experience” is essentially “data”, “learning” involves “training” with algorithms, for example proceptron algorithm, and the “knowledge” distilled is a type of “model”. We call the “self-exam process” as “validation” and “applying” it to new problems as “generalization”. The entire machine learning process is presented in the following figure (down). In this course, we focus on “training” and “validation” steps. For “training” step, we introduce several basic and fundamental algorithms for linear models and discuss several validation methods for “validation” step. See Figure below.\n\n\n\n\nFigure 6: The human learning process (up) V.S. machine learning process (down)\n\n\n\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_s.html",
    "title": "Proceptron and Its Algorithm",
    "section": "",
    "text": "Here, we introduce an old but interesting algorithm to train a classifier. The main purpose is help you getting real feelings about taring algorithm in machine learning and have a deeper understanding of linear classifiers.\nNext, I’ll begin with a historical review of this algorithm and understanding the basic purpose of an machine learning algorithm. Then we use more algebra and geometry to show the intuitive idea behind of a linear classifier in general. In the end, the proceptron algorithm will be interpreted through algebra and geometry."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s.html#history",
    "href": "Courses/c_mlwr1_2024/l1/l1_s.html#history",
    "title": "Proceptron and Its Algorithm",
    "section": "History",
    "text": "History\nProceptron classifier is viewed as the foundation and building block of artificial neural network. For a historical introduction, see the figures below.\n\n\n\nThe Perceptron classifier was developed by Frank Rosenblatt in 1957 (LHS). Rosenblatt’s goal was to create a machine that could classify visual patterns, such as distinguishing between different shapes. At the time, he envisioned using large computers to simulate neural networks, inspired by how the brain processes information. His early experiments involved using a huge computer called the Mark I Perceptron (RHS), which attempted to recognize different shapes by adjusting weights based on input data. This work laid the foundation for modern neural networks and machine learning, despite initial limitations in its capacity to handle complex, non-linear problems.\n\n\nSuppose we are solving a binary classification problem with \\(p\\) feature variables. As discussed before, it can be represented as\n\\[\n  \\hat{y} = \\text{Sign}( \\textbf{w}^{\\top} \\textbf{x} + w_0  )\n\\]\nwhere \\(\\textbf{w} = (w_1, w_2, \\dots, w_p)^{\\top}\\), and \\(\\textbf{x} = (x_1, x_2, \\dots, x_p)^{\\top}\\). Different from before, here we represent the weighted sum of \\(p\\) feature variables, \\(w_1x_1+ \\dots + w_px_p\\), as the inner (dot) product of two vectors, i.e. \\(\\textbf{w} \\cdot \\textbf{x} = \\textbf{w}^{\\top} \\textbf{x}\\).\n\nNote: In order to understand proceptron algorithm, we need some basic knowledge about vector and its operations. If you are not familiar with it or need to refresh it, read about vector, operators, inner product before start reading the next.\n\nThe perceptron algorithm is about finding a set of reasonable weights. The key term here, “reasonable,” is easy to understand—it refers to a set of weights that can deliver good predictive performance. The core issue is how to find them.\n\nBrute-force idea: try all possible weight values and record the corresponding model performance, such as accuracy, and then choose the weights that yield the highest accuracy as the final model parameters.\n\nHowever, this idea is clearly not ideal. Even if we only have two feature variables, this would still not be a simple task. A smarter approach is to do it this way: we start with an initial guess for the weight values, and then gradually approach the most reasonable weights through some iterative mechanism. This mechanism is called the perceptron algorithm. Next, let’s dive into learning this magical mechanism—the perceptron algorithm.\nNext, the logic goes like this:\n\nFurther explore the geometric properties of linear classifiers\nUse geometry to understand how this algorithm works."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s.html#geometry-of-linear-classifiers",
    "href": "Courses/c_mlwr1_2024/l1/l1_s.html#geometry-of-linear-classifiers",
    "title": "Proceptron and Its Algorithm",
    "section": "Geometry of Linear Classifiers",
    "text": "Geometry of Linear Classifiers\nNote: From now, we will temporarily ignore the bias term, \\(w_0\\), or assume it as \\(0\\). It will not influence our final conclusion. No worries. So, the basic classifier is represented as \\(y = \\text{Sign}(\\textbf{w}^{\\top}\\textbf{x})\\)\nPreviously, we explored the geometric understanding of linear classifiers, which is that the classifier determines a linear decision boundary. Next, let’s understand a linear classifier from another view of geometry. Suppose we have a classifier with two feature variables, \\(x_1\\) and \\(x_2\\), and the “reasonable” weight vector is \\(\\textbf{w} = (0.6, 0.8)^{\\top}\\). Look at the conceptual plot below.\n\n\n\n\n\n\n\n\n\nIt is easy to see that all the vectors (points) in blue form a sharp angle with the weights vector (black). By the property of inner product, (read about inner product) for any point \\(\\textcolor{blue}{\\textbf{x}} = (\\textcolor{blue}{x_1},\\textcolor{blue}{x_2})^{\\top}\\) standing on the direction pointed by the blue arrow, \\(\\textbf{w}^{\\top}\\textcolor{blue}{\\textbf{x}} \\propto \\cos(\\alpha) &gt; 0\\), i.e. all the cases on this direction will be classify as positive. On the contrary, all the vectors (points) in blue form a obtuse angle with the weights vector, and then \\(\\textbf{w}^{\\top}\\textcolor{red}{\\textbf{x}} \\propto \\cos(\\beta) &lt; 0\\), i.e. all the points standing on the direction pointed by a red vector will be classified as negative. With this observation, we can easily understand how does a “reasonable” linear classifier work.\nBased on this principle, let’s have look at a concrete example in the figure below.\n\n\n\nIn a binary classification problem, we have two feature variables, each with 10 cases, blue for positive and red for negative. We have a “reasonable” weight vector \\(\\textbf{w}\\) (orange arrow), which determines a linear decision boundary (purple line). \\(\\textbf{w}\\) is “reasonable” because it has an angle less than 90 degrees with all positive vectors, but an angle greater than 90 degrees with all negative vectors. Of course, a more direct understanding is that this linear classification boundary divides the entire feature space into two parts, with all positive cases at the bottom and all negative cases at the top. However, the first explation is more useful for understanding the proceptron algorithm."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s.html#proceptron-algorithm",
    "href": "Courses/c_mlwr1_2024/l1/l1_s.html#proceptron-algorithm",
    "title": "Proceptron and Its Algorithm",
    "section": "Proceptron Algorithm",
    "text": "Proceptron Algorithm\nWith the discussion above, at least for me, the weights vector of a perceptron classifier is like a sword of judgment, wielded to evaluate all cases. So, I just name proceptron algorithm as finding sword of judgment algorithm. You may remember the brute force idea, so the next question is whether we have a clever way to quickly find the sword of judgment instead of trying all possibilities.\n\n\n\nSword of judgment: it is a powerful and iconic weapon in the Transformers universe, often wielded by the noble Autobot leader, Optimus Prime. This sword embodies the principles of justice and righteousness, serving as a symbol of hope in the battle against evil. Now, you understand why blue presents positive? Because The Autobots have blue eyes.\n\n\nBefore we begin to understand how this algorithm works, we must first reach a consensus that no matter how we look for it, we must have a starting point, and this starting point is preferably random. If you agree with me, let’s randomly pick up one initial guess of the sword (weights vector) in the following conceptual example.\n\n\n\n\n\nThis initial guess is not too bad since it only made one mistake, see the 2nd column of the image. This case is a positive case but wrongly predicted as negative. So, we need to correct this weights vector such that the corresponding decision boundary will be rotated clockwise slightly. From the 3rd column of the image we can see that, this aim can be achieved by updating the weights vector as \\[\n  \\textcolor{red}{\\textbf{w}}^{\\text{new}} = \\textcolor{green}{\\textbf{w}}^{\\text{old}} + \\textcolor{blue}{\\textbf{x}}\n\\]\nBy the Parallelogram Law (read about vector addition), the old weights vector will be rotated clockwise, thereafter the decision boundary is corrected properly.\nSuppose the mistake by the initial guess of sword (\\(\\textbf{w}\\)) happened for a negative case, see figure below. In this case, we need to update the weights vector as\n\\[\n  \\textcolor{red}{\\textbf{w}}^{\\text{new}} = \\textcolor{green}{\\textbf{w}}^{\\text{old}} - \\textcolor{red}{\\textbf{x}}\n\\]\nsuch that the decision boundary will be anticlockwise slightly.\n\n\n\n\n\nNotice that the target variable \\(y = -1\\) or \\(+1\\), therefore we can integrate the two updating rule as one:\n\\[\n  \\textcolor{red}{\\textbf{w}}^{\\text{new}} = \\textcolor{green}{\\textbf{w}}^{\\text{old}} + y \\textbf{x}\n\\]\nNow, we have actually found a clever way to search for the sword of judgment. Basically, we can do this. First, number all cases, and then randomly pick an initial guess for \\(\\textbf{w}\\). After these are prepared, we start to use this sword to test cases in the order of numbers. If we find a wrong judgment, such as number 3, then we correct this sword according to the updating formula above. After the correction, continue to judge in order, that is, start judging from number 4, and continue to correct the error. Repeat this process, and when we find a real sword that can correctly judge all cases, we stop the search process. This is the so called Proceptron Algorithm:\nProceptron Algorithm:\nInputs: y nx1 vector taking vaules -1 or 1, X nxp matrix\nInitialization: weight vector w randomly initialized \n\nWhile(All cases are correctly classified){\n  for(i in 1:n){\n    if(case i is misclassified){\n      w = w + y[i]X[i, ]      \n    }\n  }\n}\nBelow, you can see an animation presenting the Proceptron Algorithm in action.\n\n\n\nAnimation of running perceptron algorithm\n\n\nRemarks:\n\nThis algorithm works, that is, as long as we follow this algorithm, no matter where we start, we can always find the sword of judgment.\nHowever, as you may have realized, this algorithm has a prerequisite, that is, our training samples are linearly separable. In other words, we can find a straight line (2D case) to split the sample space, all positive and negative examples stand on each side, or we will not make any mistakes in the training samples.\nIn addition, you may also find that the “reasonable” \\(\\textbf{w}\\) obtained by this algorithm is not unique, that is, there exist many swords of judgment, and which one we finally get depends on our initial guess.\n\n\nLecture 1 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s_0.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_s_0.html",
    "title": "Proceptron and Its Algorithm",
    "section": "",
    "text": "Here, we introduce an old but interesting algorithm to train a classifier. The main purpose is help you getting real feelings about taring algorithm in machine learning and have a deeper understanding of linear classifiers.\nNext, I’ll begin with a historical review of this algorithm and understanding the basic purpose of an machine learning algorithm. Then we use more algebra and geometry to show the intuitive idea behind of a linear classifier in general. In the end, the proceptron algorithm will be interpreted through algebra and geometry.\nOutline：\n\n\nHistorical review of Proceptron\n\n\nMore geometrical insights of linear classifier\n\n\nProceptron algorithm\n\n\n\nLecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s_3.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_s_3.html",
    "title": "Proceptron Algorithm",
    "section": "",
    "text": "With the discussion above, at least for me, the weights vector of a perceptron classifier is like a sword of judgment, wielded to evaluate all cases. So, I just name proceptron algorithm as finding sword of judgment algorithm. You may remember the brute force idea, so the next question is whether we have a clever way to quickly find the sword of judgment instead of trying all possibilities.\n\n\n\nSword of judgment: it is a powerful and iconic weapon in the Transformers universe, often wielded by the noble Autobot leader, Optimus Prime. This sword embodies the principles of justice and righteousness, serving as a symbol of hope in the battle against evil. Now, you understand why blue presents positive? Because The Autobots have blue eyes.\n\n\nBefore we begin to understand how this algorithm works, we must first reach a consensus that no matter how we look for it, we must have a starting point, and this starting point is preferably random. If you agree with me, let’s randomly pick up one initial guess of the sword (weights vector) in the following conceptual example.\n\n\n\n\n\nThis initial guess is not too bad since it only made one mistake, see the 2nd column of the image. This case is a positive case but wrongly predicted as negative. So, we need to correct this weights vector such that the corresponding decision boundary will be rotated clockwise slightly. From the 3rd column of the image we can see that, this aim can be achieved by updating the weights vector as \\[\n  \\textcolor{red}{\\textbf{w}}^{\\text{new}} = \\textcolor{green}{\\textbf{w}}^{\\text{old}} + \\textcolor{blue}{\\textbf{x}}\n\\]\nBy the Parallelogram Law (read about vector addition), the old weights vector will be rotated clockwise, thereafter the decision boundary is corrected properly.\nSuppose the mistake by the initial guess of sword (\\(\\textbf{w}\\)) happened for a negative case, see figure below. In this case, we need to update the weights vector as\n\\[\n  \\textcolor{red}{\\textbf{w}}^{\\text{new}} = \\textcolor{green}{\\textbf{w}}^{\\text{old}} - \\textcolor{red}{\\textbf{x}}\n\\]\nsuch that the decision boundary will be anticlockwise slightly.\n\n\n\n\n\nNotice that the target variable \\(y = -1\\) or \\(+1\\), therefore we can integrate the two updating rule as one:\n\\[\n  \\textcolor{red}{\\textbf{w}}^{\\text{new}} = \\textcolor{green}{\\textbf{w}}^{\\text{old}} + y \\textbf{x}\n\\]\nNow, we have actually found a clever way to search for the sword of judgment. Basically, we can do this. First, number all cases, and then randomly pick an initial guess for \\(\\textbf{w}\\). After these are prepared, we start to use this sword to test cases in the order of numbers. If we find a wrong judgment, such as number 3, then we correct this sword according to the updating formula above. After the correction, continue to judge in order, that is, start judging from number 4, and continue to correct the error. Repeat this process, and when we find a real sword that can correctly judge all cases, we stop the search process. This is the so called Proceptron Algorithm:\nProceptron Algorithm:\nInputs: y nx1 vector taking vaules -1 or 1, X nxp matrix\nInitialization: weight vector w randomly initialized \n\nWhile(All cases are correctly classified){\n  for(i in 1:n){\n    if(case i is misclassified){\n      w = w + y[i]X[i, ]      \n    }\n  }\n}\nBelow, you can see an animation presenting the Proceptron Algorithm in action.\n\n\n\nAnimation of running perceptron algorithm\n\n\nRemarks:\n\nThis algorithm works, that is, as long as we follow this algorithm, no matter where we start, we can always find the sword of judgment.\nHowever, as you may have realized, this algorithm has a prerequisite, that is, our training samples are linearly separable. In other words, we can find a straight line (2D case) to split the sample space, all positive and negative examples stand on each side, or we will not make any mistakes in the training samples.\nIn addition, you may also find that the “reasonable” \\(\\textbf{w}\\) obtained by this algorithm is not unique, that is, there exist many swords of judgment, and which one we finally get depends on our initial guess.\n\n\nPrevious page | Lecture 1 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_home.html",
    "href": "Courses/c_mlwr1_2024/l7/l7_home.html",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "",
    "text": "In this lecture, we will study the most important classifier, logistic regression classifier. I believe that in the near future, you will realize that this classifier plays an important role in the entire field of machine learning, and even in AI as a whole. We will cover the following things:"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_7.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_7.html",
    "title": "2.7 Other Useful Things",
    "section": "",
    "text": "Alright, we now have a basic understanding of the most fundamental operations in R programming, most of which are things we will frequently use in this course. Next, we’ll introduce a few more useful concepts and some commonly used functions.\n\n2.7.1 Workspace\nIn R, the workspace refers to the environment where all objects (such as variables, functions, and data) are stored during an R session. It acts as a storage area that retains the data and objects you create, allowing you to work with them without needing to re-import or redefine them every time you start R. The most common scenario is when you’ve worked hard all day and want to take a break, but if you close R, all the objects in your working environment (memory) will disappear. In this case, you can save your current working environment as a workspace file, which has a .RData extension.\nThere are two ways to save your working environment as a workspace file. First, by mouse actions, you can click Session -&gt; Save Workspace As.... Or you can do it by command\nsave.image(\"FileName.RData\")\nThe next day, after enjoying the morning sunshine (if conditions permit) and your coffee, you can load this file and continue your hard work!\n\n\n2.7.2 Packages\nIf R could only be used for scientific computing, it would undoubtedly be overshadowed by numerous other scientific computing programs. The true strength of R lies in its extensibility, which is achieved through R packages. Initially, R packages were primarily written by statisticians to implement new methods, such as lme4 for fitting generalized linear mixed-effects models; survival for conducting survival analysis; psych for psychological research, and so on. However, writing packages is not exclusive to statisticians; an increasing number of non-statistical application packages have also been developed. Today, R has become incredibly versatile through the extension of various packages, for example this website is written by quarto package. Below, we will briefly illustrate how to install and load packages using examples.\ninstall.packages(\"kernelab\") \n# to install a new package. Note: the quotation marks are essential.\n\nlibrary(kernelab) \n# you can import a package by function `library`\n\n\n2.7.3 Useful Functions\nNext, some useful functions are introduced. These functions were extremely useful back when I was a student. However, in the era of RStudio, their usefulness has been greatly reduced. Nonetheless, they are still quite necessary for those who prefer keyboard operations or need to work on a server. In addition, these functions can, to some extent, enhance R users’ understanding of R programming.\n\nls function: it can list all the objects in the workspace or current environment.\nrm function: it can help us to remove objects from the workspace or current environment.\n\n# Example 1\nx = 1\nrm(x) \n# Example 2\nrm(list = ls()) # Danger Warning: This command will remove all objects listed by `ls`\n\nstr function: it displays the structure of an object.\n\n# Example 1\nx = list()\nx[[1]] = 1:10\nx[[2]] = letters[4:10]\nstr(x)\n# Example 2\nres = t.test(rnorm(30)) # do one sample t-test and save results in `res`\nstr(res) \n# You can see that the testing results are saved in a list of 10.\n# if you want to extract elements from it, the information coveryed by ´str´ is ideal.\n\nsummary function: it helps us to summarize useful information from an R objects. The information extracted depends on the type of the object. For examples\n\n# Example 1\ndat = iris[,-5] # we use the first 4 variable from iris data\nsummary(dat) # the type of ´dat´ is dataframe, then the summarized informations are...\n# Example 2\nres = t.test(rnorm(30))\nsummary(res) \n# the type of ´res´ is results of t test. The designer of this function decided \n# to show the names of all the elements in ´res´, similiar to the output of ´str´\n\nunique and table functions: they are useful when you want to check all possible values in a variable and the frequency of different possible values.\n\n# First, we create a small demo dataset\ntreatment = c(1,1,0,0,1,1,0,0)\nblock = c(1,1,1,1,2,2,2,2)\nsex = c(\"F\",\"M\",\"M\",\"M\",\"F\",\"F\",\"M\",\"F\")\nage = c(19,20,28,22,21,19,23,20)\noutcome = c(20,19,33,12,54,87,98,84)\nDat = data.frame(treatment, block, sex, age, outcome)\nhead(Data, 8)\n\n# Example 1:\nunique(Dat$sex)\ntable(Dat$sex)\nunique(Dat$age)\ntable(Dat$age)\n\n# Example 2:\ntable(Data$sex, Data$treatment) # do you know the name of the outputs?\n\nwhich function: it finds the index of elements that satisfy some conditions in a vector, or matrix, or data frame.\n\n# Example: Use the same demo data above\nwhich(Dat$sex == \"M\")\nwhich(Dat$age &lt; 21)\n\napply function: it is used to perform operations on rows or columns of matrices, data frames, or higher-dimensional arrays. It allows you to apply a function across the rows or columns without needing to use loops, making code more concise and often more efficient.\n\n# Syntax: \napply(X, margin, fun)\n# `margin` is an integer specifying whether to apply the `fun` across rows (1) or columns (2) \n# Examples: Use the demo data but ignore the variable `sex`\nDat = Dat[, -3]\napply(Dat, 2, mean)\nNext, show some useful functions for graphics. The ggplot2 package is definitely the top choice for plotting, but sometimes the following functions are more practical and convenient for data visualization. I will only list them below, and you are already strong enough to investigate them by yourself :)\n\nhist function: it can help use check the distribution of a variable.\nplot function: it is usually used to show the scatter plot of two variables.\npairs function: it shows the pairwise scatter plot of many variables.\n\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_1.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_1.html",
    "title": "2.1 Overview of R language",
    "section": "",
    "text": "R is a powerful and versatile programming language primarily used for statistical computing, data analysis, and graphical representation. Developed in the early 1990s by Ross Ihaka and Robert Gentleman at the University of Auckland, R has since evolved into a robust tool that supports a wide range of applications in various fields, including data science, bioinformatics, and social sciences.\nMain features of R:\n\nFor machine learning, data mining, and statistical analysis: R provides a comprehensive suite of statistical functions, making it ideal for conducting complex data analyses. It supports various statistical methods and techniques, including linear and nonlinear modeling, time-series analysis, classification, clustering, and machine learning. R facilitates the implementation of machine learning algorithms for predictive modeling and data mining.\nFor data Visualization: R excels at creating high-quality graphics and visualizations. With packages like ggplot2, users can generate intricate plots and charts to effectively communicate insights and findings.\nFor data Handling: R has powerful data manipulation capabilities, especially with packages like dplyr and tidyr. These tools allow for efficient data cleaning, transformation, and reshaping, making it easier to prepare datasets for analysis.\nEfficient matrix computing: R is inherently designed for matrix operations and linear algebra, making it particularly well-suited for tasks involving matrix computations. This feature allows users to perform complex mathematical calculations efficiently, which is essential in statistics and data analysis.\nCommunity Support: R has a vibrant and active community, offering extensive resources, tutorials, and forums for users to seek help and share knowledge. This community-driven approach fosters continuous improvement and innovation within the R ecosystem.\n\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_lab_home.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_lab_home.html",
    "title": "Lab 1: Exercises of R programming",
    "section": "",
    "text": "In this lab, we practice on R programming. Warm-Up, Strength Training, and Extreme Cardio exercises of varying intensities will be provided. You can choose your starting level based on your own situation. As mentioned earlier, the best way to learn any language is through practice. So, let’s get our brains and fingers moving!\nNote: Programming training is inseparable from mathematics and various algorithms. Here, we don’t require you to complete all tasks; you can decide based on your own situation. There is no time limit, you can come back to the challenge whenever you have time. The more you solve, the more you practice, and the more you master. In addition, you will also learn more knowledge in math and statistics BTW."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_lab_home.html#warm-up",
    "href": "Courses/c_mlwr1_2024/l2/l2_lab_home.html#warm-up",
    "title": "Lab 1: Exercises of R programming",
    "section": "1. Warm-Up",
    "text": "1. Warm-Up\n\nTask 1: Embark!\nInstall R-studio and R-base. Launch Rstudio, install package tidyverse. It integrates multiple packages and has revolutionary significance in data processing.\nImport a data set, FCourse.txt. You can download it here. This data set includes students’ preference levels for different subjects. Do you have any comments? Delete the last 10 rows of the data set and save it as a txt file in your disk. The function you need is write.table.\n\n\nTask 2: A small test of skill\nTask 2.1: Write a program to calculate the sum of integer from 1 to 100. I’m not sure if you’ve heard the story about the great mathematician Gauss, who was the first pupil to finish this calculation and the first to go home for dinner in his class room. I’m sure you must be even faster than him!\nTask 2.2: After this, modify your code such that you can calculate the factorial of 100 (the product of integers from 1 to 100) by the program.\n\n\nTask 3: Some Basic Plots\nR is excellent at graphics, especially taking power of the ggplot2 package into account. We don’t have time to study this package, but will do some simple exercises.\nTask 3.1: One can visualize a math function by the following code\nx = seq(-pi,pi,0.01)\nplot(x, sin(x), type = \"l\")\nabline(h = 0)\nabline(v = 0)\nNow, it is your turn. Visualize the density function of the normal distribution with mean 5 and  sd  2.\nTask 3.2: Learn and practice the following basic plotting functions, hist, boxplot, and pie.\n\n\nTask 4: Data frame\nUsing the following code to generate the example data for this task. BTW, you may also have a closer look at the functions that you have not seen before or you are not familiar with.\n\nn = 100 # sample size\ntreatment = rbinom(n, 1, 0.5)\nblock = sample(c(1,2), n, replace = T)\nsex = sample(c(\"F\", \"M\"), n, replace = T)\nage = round( runif(n, 18, 40) )\noutcome = round( rnorm(n, 30, 10) )\nDat = data.frame(treatment, block, sex, age, outcome)\n\nTask 4.1: Sort the data set by variable age in ascending order; Filter out all the rows of female observations; Find out the values of variables age and outcome for all the rows that belongs to treatment 1 and block 2.\nTask 4.2: Randomly draw a sample with 80 observations from the data set and set them as sub-dataset 1 and the rest of them as sub-dataset 2."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_lab_home.html#strength-training",
    "href": "Courses/c_mlwr1_2024/l2/l2_lab_home.html#strength-training",
    "title": "Lab 1: Exercises of R programming",
    "section": "2. Strength Training",
    "text": "2. Strength Training\n\nTask 5: About Numbers\nIn this task, we solve some problems about numbers\nTask 5.1: Finding prime numbers Write a program to find all prime numbers up to 100. A prime number is a number that has only two factors, that is, 1 and the number itself.\nTask 5.2: Write a function that can convert a binary number to a decimal integer.\nTask 5.3: Write a function that can convert a decimal integer to a binary number.\n\n\nTask 6: Law of Large Numbers\nLaw of large numbers (LLN) is one of the foundations of probability theory. It states that as the number of trials or observations increases, the average of the results approaches the expected value. Simply put, suppose you have a fair coin. Each flip has an equal chance—50/50—of landing heads or tails. If you flip the coin repeatedly and track the cumulative proportion of heads, this proportion will get closer and closer to 0.5 as the number of flips increases.\nIn this task, we will “prove” the LLN by doing a small simulation. Let the computer mimic flipping a fair coin (generate a random number from Bernoulli distribution with \\(p=0.5\\)). Draw a graph to show that as the number of flips increases, the cumulative proportion of getting heads or tails converges to 0.5.\n\n\nTask 7: Central Limit Theorem\nThe Central Limit Theorem (CLT) is a key principle in statistics that states: for a sufficiently large sample size, the distribution of the sample mean approaches a normal (bell-shaped) distribution, regardless of the shape of the original population distribution. This means that if you take repeated random samples from any population with a finite mean and variance, the means of those samples will tend to follow a normal distribution as the sample size grows.\nIn this task, we will “demonstrate” the CLT through a simulation. We’ll repeatedly draw random samples, each with a sample size of 50, from a uniform distribution between 0 and 1. For each sample, we’ll calculate and record the average value. After repeating this process 1,000 times, plot a histogram to visualize the distribution of these 1,000 sample averages.\n\n\nTask 8: Box-Muller’s Algorithm\nWe roughly mentioned about Pseudo-random numbers in the lecture. Pseudo-random numbers are a sequence of numbers which are generated by some algorithm from an initial number, and they can mimic the behaviour of a random sample of uniform random variables. Once the pseudo uniform random number is ready, different algorithms can be applied to them to generate random numbers from certain distributions. Here, you implement Box and Muller’s algorithm to generate random numbers from an arbitrary Normal distribution. Write the implementation of this algorithm in a function, such that you can apply this function to simulate Normal random sample, just like function `rnorm``\nBox-Muller’s Algorithm\n\nStep 1: Randomly generate \\(u\\) and \\(v\\) from \\(U(0,1)\\), uniform distribution between 0 and 1\nStep 2: Set \\(x = \\sqrt{ -2 \\log (u) } \\cos (2\\pi v)\\) and \\(y = \\sqrt{ -2 \\log (u) } \\sin (2\\pi v)\\)\n\nBased on this procedure, the resulting values of \\(x\\) and \\(y\\) are independent normal distributed with mean 0 and variance 1.\nToDo4Sia: write a note to explain BM algorithm.\n\n\nTask 9: Newton Raphson Algorithm\nDo you know Newton Raphson’s optimization algorithm? The main idea of this algorithm is to find successively better approximations to the roots of a real-valued function. More specifically, assuming \\(f(x)\\) is differentiable and starting from an initial guess \\(x_0\\), the root of \\(f(x)=0\\) can be iteratively approximated as\n\\[\n  x_{n+1}=x_{n}-\\frac{f(x_{n})}{f'(x_{n})}\n\\] Now, you are required to apply this algorithm to find an approximated root of \\(x^3-x-1=0\\).\n\n\nTask 10: Bootstrap Algorithm\nThe bootstrap algorithm is a statistical technique used to estimate the distribution of a sample statistic by resampling the observed data with replacement. In essence, it involves repeatedly drawing samples (typically of the same size as the original sample) from the dataset, calculating the desired statistic (e.g., mean, median, or standard deviation) for each resample, and then aggregating these results. This method allows for estimating the variability or confidence intervals of statistics without requiring complex mathematical formulas, making it especially useful when traditional parametric assumptions (like normality) are not met. The well known machine learning algorithm random forest was just developed based on this algorithm.\nPrepare the data set: Simulate \\(x_i,i = 1,2,...,30\\) from uniform distribution \\(U(0,5)\\) by function ’runif’ and \\(\\epsilon_i,i=1,2,...,30\\) from the standard Gaussian distribution. Calculate \\(y_i =0.5+1.5x_i+\\epsilon_i\\).\nNext, we will apply bootstrap algorithm to estimate the confidence interval (CI) of the regression coefficient, and compare it with the CI calculated by formula.\nTask 10.1: Calculate the CI by formula. Employ function ’lm’ to estimate the regression model and use the output to calculate the confidence interval of the slop term of the regression model.\nTask 10.2: Calculate the CI of the estimation of slop term by Bootstrap Algorithm.\nFirst, we generate a bootstrap sample from the data. The bootstrap sample is resampled from the simulated data with a replacement. Second, estimate the regression model with the bootstrap sample and record the estimation of the slope term. Repeat the two steps 1000 times. The bootstrap confidence interval is the upper and lower quantile values of all the estimations of the slope term. Compare the results with 1). This procedure is summarized in the following pseudo algorithm\nB = 1000\nfor(i in 1:B){\n  #Step 1: draw a bootstap sample from the data set\n  #Step 2: Apply `lm` to estimate the model with the bootstrap sample\n  #Step 3: Save the estimation of slop term\n}\n# Calculate the quantile values of 1000 estimation of slop term."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_lab_home.html#extreme-cardio",
    "href": "Courses/c_mlwr1_2024/l2/l2_lab_home.html#extreme-cardio",
    "title": "Lab 1: Exercises of R programming",
    "section": "3. Extreme Cardio",
    "text": "3. Extreme Cardio\n\nTask 11: Proceptron Algorithm\nWe have discussed this algorithm in lecture 1. Now, let’s implement it in R.\nTask 11.1: Use the following code to generate the data\nset.seed(201606)\nN = 20\nx1 = runif(N,-1,1); x2 &lt;- runif(N,-1,1); X &lt;- cbind(x1,x2)\ny = ifelse(x2&gt;x1,-1,1); id &lt;- 1:N\nt = seq(-1.5,1.5,0.1)\ndat = cbind(y, x1, x2)\nHere, you can ignore the bias (constant) term in the classifier, just like what we discussed in lecture 1. Write your function and use it to find the sword of judgment!\nTask 11.2: Use the following code to get the data\n# Here, we will train a Proceptron algorithm to a subset of iris data\nX = iris[1:100,1:2]\ny = as.numeric(iris[1:100,5])\ndat = cbind(y, X)\nWrite your function and use it to find the sword of judgment!\n\n\nTask 12: Decipher Problem\nDo you know substitution cipher? A substitution cipher is a method of encryption where each letter in a text is replaced by another letter according to a specific system. For example, we use H to present D, \\(H \\to D\\), and similarly \\(A \\to T\\), and \\(U \\to A\\), then the substitution cipher of word DATA is HUAU. Next, I ask you a secrete question in a text and encrypt it using a substitution cipher. My cipher is simple; it only includes the 26 lowercase letters and space. The task is to write a program that implement the algorithm illustrated in the notes to crack my cipher and answer my secrete question.\n\nMy ciphertext: download it here: download it here\nThe transition probability matrix: download it here\n\n\n\nCongratulations! You have become so powerful!\n\n\nLecture 2 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_3.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_3.html",
    "title": "2.3 Love at First Sight?",
    "section": "",
    "text": "In R, we can enter commands in the console to have the computer perform the corresponding tasks. For example, we want to print ‘welcome to R’\nprint(\"welcome to R\")\n\n[1] \"welcome to R\""
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_3.html#the-main-actor-function",
    "href": "Courses/c_mlwr1_2024/l2/l2_3.html#the-main-actor-function",
    "title": "2.3 Love at First Sight?",
    "section": "2.3.1 The main actor: function",
    "text": "2.3.1 The main actor: function\nAs a computing language specifically designed for statisticians, the most essential elements in R are functions, since almost all tasks are accomplished through various functions. Similar to most programming languages, the general form of a function in R is\nFunctionName([input 1], [input 2], ..., [argument 1], [argument 2], ...)\ni.e. it consists of a function name, inputs and arguments enclosed in parentheses. For example, you can type the following math functions in the console,\n\nsin(pi) # 'pi' is built-in constant representing the mathematical value of Pi\n\n[1] 1.224647e-16\n\nexp(1) # It returns the natural logarithm\n\n[1] 2.718282\n\nlog(10, base = 10) # The logarithm of 10 to the base 10\n\n[1] 1\n\n\nRemark: As you can see from the code above, we use # sign to comment the code, in other words, characters after # are not considered as part of the command.\nR has extensive and well-organized help documentation. You can access help for specific functions using the ? or help() function. For example:\n\n?chisq.test\n\nYou will see the help document of this function in Rstudio."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_3.html#functions-for-generating-a-sequence-of-values",
    "href": "Courses/c_mlwr1_2024/l2/l2_3.html#functions-for-generating-a-sequence-of-values",
    "title": "2.3 Love at First Sight?",
    "section": "2.3.2 Functions for generating a sequence of values",
    "text": "2.3.2 Functions for generating a sequence of values\nThe first function is c function, which can create a sequence of numbers based on your inputs and store in in memory. For example, we want to create a sequence of the integers from 1 to 10 and store it in a variable x.\nx = c(1,2,3,4,5,6,7,8,9,10)\nYou also can include characters in x by function c, for example\nx = c(letters)\n(x = c(\"I\", \"like\", \"R\", \"How about you?\"))\nRemark: In R, if you use parentheses to enclose a command, then the outputs will be printed automatically.\nTyping the integers from 1 to 10 can be done by another function seq\nseq(1,10)\nor simply by colon operator :\n1:10\nWell, the simple colon operator is neat but limited to integer sequence and increments of 1 (or -1, try 10:1 in your console). Use seq when you need more control over the sequence, for example, try the following code in your console\nseq(1, 10, 2) \nseq(1, 10, length.out = 5)\nseq(0, 1, 0.1)\nThe last function for creating sequence is rep, which can create copies of your inputs, for example\n# guess what will we get by the following commands?\nrep(1, 10)\nrep(1:4, 4)\nrep(rep(1:4,4), 2)\nrep(\"I like R\", 3)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_3.html#custom-function",
    "href": "Courses/c_mlwr1_2024/l2/l2_3.html#custom-function",
    "title": "2.3 Love at First Sight?",
    "section": "2.3.3 Custom function",
    "text": "2.3.3 Custom function\nIn R, you are allowed to encapsulate specific tasks or calculations that you can reuse throughout your code. The syntax for defining a function is\n# Syntax of custom function\nfunction_name = function(inputs, arguments) {\n  # Function body: code to execute\n  # Optionally return a value\n  return(value)\n}\nFor example, we want a function to calculate the sum of two input values\nmySum = function(x = 1, y = 1){\n    res = x+y\n    return(res)\n}\nmySum(2,3)\nmySum()\n\nIn the parentheses after the syntax key word function, the values assigned to x and y are default values, namely R will take two ones as input automatically if we don’t enter any inputs into function mySum, for example, the result of the last line above should be 2.\nIf we don’t use return to indicate the results should be returned, then the results of the last line in the function will be returned as output.\n\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_home.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_home.html",
    "title": "Lecture 2: Introduction to R Programming",
    "section": "",
    "text": "In this lecture, we provide you a genital introduction to R programming. It covers the following things:"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_home.html#lecture-notes",
    "href": "Courses/c_mlwr1_2024/l2/l2_home.html#lecture-notes",
    "title": "Lecture 2: Introduction to R Programming",
    "section": "Lecture notes:",
    "text": "Lecture notes:\n\nIntroduction to R programming:\n\nRead the integrated notes: here;\nRead the paginated notes: here;\nDownload the PDF notes: here."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_home.html#reading-guidelines-of-textbook",
    "href": "Courses/c_mlwr1_2024/l2/l2_home.html#reading-guidelines-of-textbook",
    "title": "Lecture 2: Introduction to R Programming",
    "section": "Reading Guidelines of textbook:",
    "text": "Reading Guidelines of textbook:\nFor Lecture 2, it is recommended that you read the following sections in the textbook.\n\nChapter 2: Read sections 2.3, pages 42 - 51."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_home.html#prepare-before-the-qa-session",
    "href": "Courses/c_mlwr1_2024/l2/l2_home.html#prepare-before-the-qa-session",
    "title": "Lecture 2: Introduction to R Programming",
    "section": "Prepare before the Q&A session",
    "text": "Prepare before the Q&A session\n\nRead and study the materials of lecture 1 and 2 as much as you can.\nYou may have some questions, and you’re welcome to ask me. If you prefer, you can also email me or send a message via Canvas to inquire.\nThink about the following questions：\n\nImagine an idea for a machine learning project in your field. If you’re having trouble coming up with one, you can revisit our discussion in the philosophy of machine learning section.\nIf you don’t have a good idea for question 1, learn about a potential machine learning project idea.\nBased on your answers to questions 1 or 2, think about what your target variable is, what potential feature variables you have, and whether your problem is a regression problem or a classification problem.\nConsider if you have any questions about Lectures 1 and 2."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_home.html#lab-exercises",
    "href": "Courses/c_mlwr1_2024/l2/l2_home.html#lab-exercises",
    "title": "Lecture 2: Introduction to R Programming",
    "section": "Lab exercises:",
    "text": "Lab exercises:\nLaboratory Entrance\nSolutions\nLab Queue Entrance\n\nCourse Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_5_0.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_5_0.html",
    "title": "2.5 Flow Control",
    "section": "",
    "text": "Flow control refers to the mechanisms and structures used in programming to dictate the order in which instructions are executed in a program. It allows for making decisions, repeating actions, and controlling the flow of execution based on certain conditions or logic. In R and most programming languages, flow control is essential for creating dynamic and responsive programs. Here, we will mainly introduce loops and conditional statement.\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_5_3.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_5_3.html",
    "title": "2.5.3 While Loop",
    "section": "",
    "text": "Different from a for loop, a while loop continues to execute a block of code as long as a specified condition remains true. This means that the number of iterations is not predetermined; instead, it depends on the state of the condition being evaluated. The syntax is\n# Syntax of while loop\nwhile (CONDITION) {\n    # Code block to be executed\n}\nThe CONDITION could be a logical value, or a command resulting logical value. The same example of for loop above, example 4, also can be implemented through a while loop.\n# Example 7\n# Initialize the starting number\nnumber = 3\n# While loop to print numbers divisible by 3 up to 50\nwhile(number &lt;= 50){\n  print(number)      # Print the current number\n  number = number + 3  # Move to the next multiple of 3\n}\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_8.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_8.html",
    "title": "Final Words",
    "section": "",
    "text": "Alright, our short journey with R programming has come to an end. I hope you now have a basic understanding of this software, its basic operations, and various syntax. As I mentioned earlier, the goal of this lecture, or even this course, is not solely to train you in R but rather to give you a quick introduction to this simple programming language so that we can use it to study machine learning concepts later on. The R programming language is a powerful tool in the field of data science. You can continue exploring various advanced techniques afterward.\nLastly, I want to say that the best way to learn any language is to engage in conversation; the same applies to programming languages. Only by repeatedly typing commands and scripts into the computer will you truly master and become familiar with this programming language.\n\nPrevious page | Lecture 2 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_4_3.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_4_3.html",
    "title": "2.4.3 Data Frame",
    "section": "",
    "text": "As a programming language originally designed for statisticians, importing data and setting a specific structure for it is essential. It is so called data frame. In R, we can use various functions to read in different types of data, such as txt, csv, xlsx, and more. For example, you can apply read.table function to import data saved in a txt file. You can download Boston data here.\n# Prepare a txt file, 'Boston.txt', in a director\nsetwd(\"dir containing your data\") \n# we set the dir containg your data as the working director.\ndat = read.table(\"Boston.txt\", sep=\",\", header = T)\nRemark: Setting working director (WD) is always useful since it can simplify many things, for example, if we don’t set the WD as the folder containing ‘Boston.txt’, then you have to specify the dir in the first argument of the read.table function. Setting a WD can be done by function setwd, and for checking the current WD, you can use function getwd. In Rstudio, this action also can be done using mouse actions, see figure below.\n\n\n\nFirst, go to the right folder. Second, in tab ‘Files’, click the gear icon, then you will find it.\n\n\nData frame is a fundamental data structure used for storing tabular data, where each column can hold different types of data (e.g., numeric, character, or factor). Data frame can be created by function data.frame. For example:\n(X = cbind(x1,x2))\n(dat = data.frame(x1,x2)) \nclass(X)\nclass(dat) # it seems there is no difference between a matrix and a dataframe\nX%*%t(X) # try this \ndat%*%t(dat) # try this -&gt; matrix multiplication is not allowed.\nSo, usually, the operations and functions for a matrix are not allowed to apply to a data frame. Including different types of data is the main difference between data frame and matrix. For example:\n# with the same demo data above\nx3 = letters[1:3] # define another variable \nX = cbind(x1, x2, x3)\ndat = data.frame(x1, x2, x3)\nX\ndat # compare `X` and `dat`, draw yoru conclusions.\nFor a data frame, we still can use the same method as for matrix to slice. Another more practical way is using $ to slice. For examples:\n# with the same example above\ndat[,3] \ndat$x3\nSome useful functions for data frames\n\nhead and tail functions: they can help us to check the first and last few lines respectively. For examples:\n\ndat = iris # iris is a pre-stored data set in R which includes 150 iris flowers\nhead(dat)\ntail(dat)\nhead(dat, 10)\n\nnames function: it can help us quickly check the names of all variables.\nattach and detach functions: people feel very inconvenient to use $ to slice a data frame, but want to use the variable names directly. In this case, ´attach´ function can help us go into such kind of mode, and apparently detach function can cease this mode. For examples:\n\ndat = iris\nnames(iris) # [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"  \nSpecies # you can't find it\ndat$Species # works\n\nattach(dat)\nSpecies # also works\n\ndetach(dat)\nSpecies\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_4_0.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_4_0.html",
    "title": "2.4 Objects in R",
    "section": "",
    "text": "In R, data types (structure) define the nature of data that can be stored and manipulated. The main data types include numeric, character, logical, factor, array, matrix, data frame, list, function, each serving different purposes in data analysis and programming. Function class can check the type of a variable in the memory.\nNext, we list the simple types first and then illustrate more complex structures in details.\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_home.html",
    "href": "Courses/c_mlwr1_2024/l5/l5_home.html",
    "title": "Lecture 5: Regression Models",
    "section": "",
    "text": "In this lecture, we will discuss regression problems. We will start by reviewing linear models, then move on to nonlinear extensions, and finally address an important issue in machine learning: the overfitting problem. It covers the following things:"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4.html",
    "href": "Courses/c_mlwr1_2024/l4/l4.html",
    "title": "Lecture 4: Gaussian Discriminant Analysis",
    "section": "",
    "text": "In this lecture, we will learn about the first set of machine learning models, which are also the first group of classifiers, namely Gaussian Discriminant Analysis. In addition, we will also learn how to evaluate the performance of a classifier. We will introduce several different statistics to assess a classifier from different perspectives, thereby enriching model evaluation methods beyond just accuracy.\nToDo4Sia: write about generative models"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html",
    "href": "Courses/c_mlwr1_2024/l3/l3.html",
    "title": "Lecture 3: Basic Knowledge in Probability Theory",
    "section": "",
    "text": "Machine learning is essentially about finding methods for making decisions, and the best way to make decisions is based on assessing the probability (or likelihood) of potential outcomes. Therefore, probability theory has undoubtedly become a fundamental tool in this field.\nIn this lecture, we will cover the fundamental concepts of probability theory. To make this note self-contained, I will start from scratch. The benefit of this approach is that we can gradually build on each concept in a coherent manner. However, there may be some topics that we won’t use in this course, so I will list the most important aspects separately at the end. If you have already studied probability theory, you can treat this as a quick review. I will do my best to provide you with an intuitive picture of probability theory. If you feel this isn’t necessary, you can also skip ahead to the exercises.\nBefore we begin, I want to emphasize something important: probability models exist solely within our rational world. They are summaries of patterns in observational data, rather than replicas of real-world phenomena. No probability model can perfectly replicate reality, but it can help us solve real-world problems. As the statistician Box famously said, “All models are wrong, but some are useful.” I highly recommend you take a moment to read a mini note, it may open a door to a whole new perspective for you.\n(WBPA) The tools provided in this lesson are a type of soft tool. They may not help you directly with how to use the software, but they can offer you a way of thinking. A good understanding of them will make things easier for your studies not only in this course but also for all your data science knowledge."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#expected-value",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#expected-value",
    "title": "Lecture 3: Probability Theory and Statistics",
    "section": "Expected Value",
    "text": "Expected Value\nWe have introduced the most basic elements of probability theory, namely random variables and their distributions. So if there are two independent random variables, can we compare them? For example, suppose we have two uneven coins, and use \\(X_1 \\sim \\text{Ber}(0.4)\\) and \\(X_2 \\sim \\text{Ber}(0.6)\\) to denote the result of flipping two coins It seems that the value of \\(X_2\\) should be generally higher than the value of \\(X_1\\). However, how can I explain it more clearly in a mathematical language? We then need to introduce another important concept, the expected value of a random variable. The expected value of a random variable is very similar to the concept that you are very familiar with, that is average value. If I ask you how often do you go to IKSU per week? Then most likely, you will answer me that you go to IKSU, on average, 3 times per week, since the number of visiting IKSU (the largest sports center in Umeå) per week is not a certain number (you often go there 3 times, but not always, for example, you are sick or have an important exam to prepare). Let me show you the number of my IKSU visits in the last 10 weeks. \\[\n  3,5,3,2,2,4,5,3,3,4\n\\]\nWe all know that the average value can be \\[\n  \\frac{3+5+3+2+2+4+5+3+3+4}{10} = 3.4\n\\] Of course, it is a super easy calculation, but let’s have a close look at it. This calculation can be represented as \\[\n  \\frac{2 \\times 2 + 4\\times3 + 2\\times4 +2\\times 5 }{10} = 0.2 \\times 2 + 0.4 \\times 3 + 0.2 \\times 4 + 0.2 \\times 5 = 3.4\n\\]\nNotice that the decimal in front of each integer, the possible value, is the percentage of the corresponding value that happened in the last ten weeks. In the rational world, if you still remember it, the percentage is replaced by the probability. Therefore, the definition of expected value is defined as the weighted sum of all possible values, and the weights are the corresponding probabilities. In a mathematical notation, the expected value of a random variable is presented as \\[\n  \\text{E}(X) = \\sum_{k} k \\Pr (X = k)\n\\] We can see that the expected value of a binary distributed random variable and a binomial distributed random variable is \\(p\\) and \\(Np\\) respectively. It is a good exercise to verify it. Now, we can turn back to the question at the beginning. [TODO]\nThe expected value satisfies linearity. Suppose \\(a\\) and \\(b\\) are constant numbers and \\(X\\) is a random variable, then \\(\\text{E}(aX+b) = a \\text{E}(X) + b\\). In other words, linearity means the expectation operator and the linear operator (scalar multiplication and addition) are exchangeable.\n\n3.3.2 Variance\nThe expected value can help us determine the size of the “common” value of a random variable so that we can compare two random variables. One can also compare two random variables from another dimension, which is “value stability”. For example, we have two coins, one is even and the other is so uneven that there is a very high probability, 90%, of getting Heads. Then imagine that if we flip two coins repeatedly, we will get many heads for the uneven coin and occasionally get Tails; but for the even coin, we will get the same number of heads and tails with high probability. From the perspective of taking values, the values of uneven coins are very stable, while those of uniform coins are not. The stability of a random also refers to variation. High variation means low stability. The two things can be quantified by the variance.\nThe variance of a random variable \\(X\\) is defined as \\[\n  \\text{Var}(X) = \\text{E}(X - \\text{E}(X))^2\n\\] Based on this definition, one can easily verify the variance of a binary distributed random variable and a binomial distributed random variable is \\(p(1-p)\\) and \\(np(1-p)\\) respectively. In the example above, the variance of the even coin is \\(0.25\\), but the uneven is \\(0.09\\).\nDifferent from the expected value, variance doesn’t satisfy the linearity, i.e. the variance operator and the linear operator are not exchangeable. However, it satisfies the following rules, \\[\n  \\text{Var}(aX+b) = a^2\\text{Var}(X)  \n\\]\nBased on the results above, we can easily find that a special linear combination, \\(\\left( X-\\text{E}(X) \\right)/\\sqrt{\\text{Var}(X)}\\), produces a standardized random variable, i.e. mean zero and variance 1."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#variance",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#variance",
    "title": "Lecture 3: Probability Theory and Statistics",
    "section": "Variance",
    "text": "Variance\nThe expected value can help us determine the size of the “common” value of a random variable so that we can compare two random variables. One can also compare two random variables from another dimension, which is “value stability”. For example, we have two coins, one is even and the other is so uneven that there is a very high probability, 90%, of getting Heads. Then imagine that if we flip two coins repeatedly, we will get many heads for the uneven coin and occasionally get Tails; but for the even coin, we will get the same number of heads and tails with high probability. From the perspective of taking values, the values of uneven coins are very stable, while those of uniform coins are not. The stability of a random also refers to variation. High variation means low stability. The two things can be quantified by the variance.\nThe variance of a random variable \\(X\\) is defined as \\[\n  \\text{Var}(X) = \\text{E}(X - \\text{E}(X))^2\n\\] Based on this definition, one can easily verify the variance of a binary distributed random variable and a binomial distributed random variable is \\(p(1-p)\\) and \\(np(1-p)\\) respectively. In the example above, the variance of the even coin is \\(0.25\\), but the uneven is \\(0.09\\).\nDifferent from the expected value, variance doesn’t satisfy the linearity, i.e. the variance operator and the linear operator are not exchangeable. However, it satisfies the following rules, \\[\n  \\text{Var}(aX+b) = a^2\\text{Var}(X)  \n\\]\nBased on the results above, we can easily find that a special linear combination, \\(\\left( X-\\text{E}(X) \\right)/\\sqrt{\\text{Var}(X)}\\), produces a standardized random variable, i.e. mean zero and variance 1."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#sum-rule-and-product-rule",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#sum-rule-and-product-rule",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Sum rule and product rule",
    "text": "Sum rule and product rule\nLet’s think about a question like this. Suppose there are two boxes, red and blue, and the red box contains two apples and six oranges, while the blue box contains three apples and one orange. Now we want to randomly pick a box and then randomly take out a fruit from it, then what is the probability that the fruit taken out is an apple? We can randomly pick the box by throwing a dice. If we get a number less than 5 then we choose the red box, or we choose the blue box.\n\n\n\nBox and Furits Problem\n\n\nThis example is a bit more complicated than the previous one because there are two randomized actions involved here. The result of this action involves a combination of the color of the box and the type of fruits. Let’s start by considering the probability that the red box is drawn while an apple is picked up. Again, we can use the previous formula to calculate this probability. There are six numbers corresponding to the dice, and the number of fruits inside the red box is 8, so all the possibilities are \\(6 \\times 8\\). But there are only numbers 1 through 4 and two apples, so there are only \\(4 \\times 2\\) possibilities that qualify. So the probability is \\[\n  \\frac{4\\times2}{6\\times8} = \\frac{4}{6} \\times \\frac{2}{8} = \\frac{1}{6}\n\\] It is easy to see that \\(4/6\\) is the probability of getting a number less than \\(5\\), i.e. the red box is selected. But what is the meaning of the second part, \\(2/8\\)? Since \\(8\\) is the number of fruits and \\(2\\) is the number of apples in the red box, it can be understood as the probability of getting an apple when the red box was selected. We refer to this probability as conditional probability and present it as \\(\\Pr(\\text{Apple} | \\text{Red}) = 2/8\\). This discussion can be summarized as \\[\n  \\Pr ( \\text{Red AND Apple} ) = \\Pr( \\text{Apple} | \\text{Red} ) \\Pr ( \\text{Red} )\n\\] We can also easily get the probability of getting an apple under the other possibility, i.e. \\[\n  \\Pr(\\text{Blue AND Apple}) = \\Pr(\\text{Apple} | \\text{Blue} ) \\Pr( \\text{Blue} ) = \\frac{3}{4} \\times \\frac{2}{6} = \\frac{1}{4}\n\\] “AND” corresponds to the “product rule”. \\[\n  \\Pr(E_1 \\text{ AND } E_2) = \\Pr(E_1 | E_2)\\Pr(E_2) = \\Pr(E_2 | E_1)\\Pr(E_1)\n\\] Going back to our original question, what is the probability of getting an apple? This random event can be labeled as “Red AND Apple OR Blue AND Apple”. Here, we have the second rule, i.e. “sum rule”, when considering the “OR” operator between two events that don’t happen simultaneously. \\[\n  \\Pr(E_1 \\text{ OR } E_2) = \\Pr(E_1) + \\Pr(E_2)\n\\]\nBased on the sum rule, the probability of getting an apple is calculated as \\[\n  \\Pr( \\text{Apple} ) = \\Pr( \\text{Red AND Apple} ) + \\Pr( \\text{Blue AND Apple} ) = \\frac{5}{12}\n\\]\nRemark: we can compare it with the sum-product rule in permutation and combinatorics. When there are different types of solutions for one thing, the total number of possible solutions is the sum of the number of possible solutions for each type. When there are different steps in doing one thing, the total number of solutions is the product of the number of possible solutions in each step."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#joint-distribution-and-marginal-distribution",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#joint-distribution-and-marginal-distribution",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Joint distribution and marginal distribution",
    "text": "Joint distribution and marginal distribution\nSimilarly, you can verify the probabilities when orange is considered, and summarise them in a 2 by 2 table\n\\begin{table}[h]\n    \\centering\n    \\begin{tabular}{c|c|c|c}\n         & Red & Blue &  \\\\ \\hline\n        Apple & $1/6$ & $1/4$ & $5/12$ \\\\ \\hline\n        Orange & $1/2$ & $1/12$ & $7/12$ \\\\ \\hline\n         & $2/3$ & $1/3$ & \n    \\end{tabular}\n\\end{table}\nIf we use random variables to present the events, for example, the random variable \\(X\\) presents the box selected, \\(1\\) indicates red, and \\(0\\) indicates blue; the random variable \\(Y\\) presents the fruit drew, then the table can be viewed as the joint distribution of two random variables. The last column is called the marginal distribution of random variable \\(Y\\), and the last row is the marginal distribution of random variable \\(X\\)."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#posterior-probability",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#posterior-probability",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Posterior probability",
    "text": "Posterior probability\nWith the example above, the last interesting question is “What is the probability we chose the blue box if we get an orange?”. It is a conditional probability, \\(\\Pr(X = 0 | Y = 0)\\). According to the product rule, we know that this conditional probability is the ratio between \\(\\Pr(X=0 \\text{ and } Y=0)\\) and \\(\\Pr(Y=0)\\). The first second has been calculated before and that is \\(7/12\\). Well, the first can be calculated by product rule again, i.e. \\(\\Pr( Y = 0 | X = 0 ) \\Pr(X=0)\\). Summarise, \\[\n  \\Pr(X = 0 | Y = 0) = \\frac{\\Pr( Y = 0 | X = 0 ) \\Pr(X=0)}{\\Pr(Y=0)}\n\\] This probability is referred to as the posterior probability in the sense that we use the observation in the second step to update the probability of the first step. Correspondingly, the probability of drawing a blue box at the first step is called prior probability. The formula above is well known as Bayes formula."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#statistically-independent",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#statistically-independent",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Statistically independent",
    "text": "Statistically independent\nFrom the joint distribution, the probability of drawing an apple \\(\\Pr(Y = 1)\\) is \\(5/12\\). It is different from the probability of drawing an apple under the condition that the red box was selected, \\(\\Pr(Y=1 | X = 1) = 2/8\\). This fact implies that the value of random variable \\(Y\\) depends on the value of \\(X\\), or random variable \\(X\\) and \\(Y\\) are dependent. If we add 1 apple and 11 oranges to the blue box, then \\(\\Pr(Y=1) = \\Pr(Y=1 | X = 1)\\). In this case, the value of random variable \\(Y\\) doesn’t depend on the value of \\(X\\), i.e. they are independent."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#covariance",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#covariance",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Covariance",
    "text": "Covariance\nFor two random variables \\(X\\) and \\(Y\\), we can use covariance to quantify the degress of association between two random variables. The covariance is defined as \\[\n  \\text{Cov}(X, Y) = \\text{E}(X-\\text{E}(X))(Y-\\text{E}(Y))\n\\] The mean and variance of a linear combination of two random variables are \\[\n  \\text{E}(aX+bY) = a\\text{E}(X) + b\\text{E}(Y)\n\\] and \\[\n  \\text{Var}(aX+bY) = a^2\\text{Var}(X) + 2ab\\text{Cov}(X,Y) + b^2\\text{Var}(Y)\n\\] respectively."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#continuous-distribution",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#continuous-distribution",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Continuous distribution",
    "text": "Continuous distribution\nContinuous random variables are not difficult to understand, they are nothing more than random variables that take real numbers, but the problem is how to describe their distribution. Let us still abstract mathematical concepts from reality. Let’s consider such a background problem, assuming that I have height data for all boys in middle school. Height is obviously not a categorical variable, but we can still use grouping to describe the distribution of height from a discrete perspective. Specifically, we can evenly divide the possible range of height into several groups, and then calculate the percentage of the number of people in each group to the total number of people. Yes, if you are familiar with basic statistics, you can tell that this is a histogram at a glance.\n\n\n\nHistogram\n\n\nSuch an approach has obvious flaws. For example, on the left-top of the plot, it is difficult for us to distinguish the probability of height being less than 175 and greater than 170 because these two values are combined into one group. How to do it? Very simple, we can split each large group into two small groups, and then calculate the frequency of each group to represent the distribution of height, for example, on the right-top of the plot. If we still cannot distinguish the above probability, we can continue to split each group into two groups. Doing this we can see that the histogram is more detailed. If we have a large amount of data, we can continue to subdivide the height group and know the probability that we can distinguish the above two events. Suppose we put all living, dead, and unborn men in the world into this histogram, and each group can be subdivided infinitely. We can imagine that the upper edge of the histogram will be a smooth curve. We call this smooth curve the density function and denote it as \\(f(x)\\). A valid density function has to inherit two conditions from the probability mass function of a discrete random. First, the density value must be positive, $f(x) &gt; 0, $ and the integral on the whole domain should be 1, \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\). With this function, we can calculate the probability of many events, as well as the expectation and variance. \\[\n  \\Pr(X&lt;b) = \\int_{-\\infty}^bf(x)dx\n\\] \\[\n  \\text{E}(X) =  \\int_{-\\infty}^{\\infty} xf(x) dx\n\\]\nOne can compare the formula above with the expected value of a discrete random variable, \\(\\text{E}(X) = \\sum_k k\\Pr(x=k)\\). You can see similar patterns, they all are the “sum” of all possible values times the corresponding probability or density values. Keep in mind that the integral symbol is an elongated S which indicates “sum”."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#normal-distribution",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#normal-distribution",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Normal distribution",
    "text": "Normal distribution\nA continuous random variable is Normally distributed, \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), if its density function is\n\\[\n  f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} } e^{- \\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma}\\right)^2 }\n\\] The normal distribution is determined by two parameters, location parameter \\(\\mu\\) and shape parameter \\(\\sigma^2\\). Density functions of normal distribution with different parameters are displayed in the following picture.\n\n\n\n\n\nLHS: Normal distribution with fixed shape parameter (sigma = 1) and different location parameters, orange: mu = -4, blue: mu = 0, red: mu = 2. RHS: Normal distribution with fixed location parameter (mu = 0) and different shape parameters, orange: sigma = 3, blue: sigma = 1, red: sigma = 0.5."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#likelihood-value-v.s.-probability",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#likelihood-value-v.s.-probability",
    "title": "Lecture 3: Probability Theory Review",
    "section": "Likelihood value V.S. Probability",
    "text": "Likelihood value V.S. Probability\nWe have build up the concept of probability. In simple terms, probability is a mathematical model used to quantify the likelihood of an event. The discussion of the possibility of this event is limited to the realm of rationality. For example, we can use probability to discuss the following questions. “ Imagine you are standing in a square in the city center, close your eyes for 30 seconds, then open your eyes and catch the first man you see. How likely is his height above 180 cm?”\nLet’s have a look at another scenario, “You went downtown last weekend. You stood in the square and closed your eyes, then after 30 seconds you opened your eyes and grabbed the man you saw first and measured his height. His height is 230 and you think it is unbelievable.” In this case, we have a concrete observation of a random variable and want to evaluate the possibility of this observation. In this case, a proper word is likelihood value.\n\\begin{table}[]\n  \\begin{tabular}{c|cccc}\n  \\hline\n                       & Concept              & Object       & Discrete Variable  & Continuous Variable  \\\\  \\hline\n      Probability      & Mathematical Concept & Events       & $\\Pr(X = 1)$ p.m.f. & $\\Pr(X&lt;b) = \\int_{-\\infty}^bf(x)dx$ \\\\\n      Likelihood value & Statistical Concept  & Observations & $\\Pr(X = 1)$ p.m.f. & $f(x)$ p.d.f.              \\\\     \n  \\hline\n  \\end{tabular}\n\\end{table}"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#the-secrete-message-delivered-by-normal-density-function",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#the-secrete-message-delivered-by-normal-density-function",
    "title": "Lecture 3: Probability Theory Review",
    "section": "The secrete message delivered by normal density function",
    "text": "The secrete message delivered by normal density function\nNotice that \\(\\left(\\frac{x-u}{\\sigma}\\right)^2\\) is the normalized distance between observation \\(x\\) and the center point \\(\\mu\\). The value of density function is the likelihood value of one observation, so Normal distribution tells us that the likelihood of an observation \\(x_0\\) is inversly propotion to the normalized distance between observed value \\(x_0\\) and the mean value \\(\\mu\\)."
  },
  {
    "objectID": "MathToolBox/la/la_08.html",
    "href": "MathToolBox/la/la_08.html",
    "title": "Review and Outlook",
    "section": "",
    "text": "Let’s review linear combinations in 2D space from a view of the inner product.\n\\[\n  \\textbf{y} = x_1 \\cdot \\textbf{a}_1 + x_2 \\cdot \\textbf{a}_2\n\\]\nThe vector \\(\\textbf{y}\\) is represented as the linear combination of the two basis vectors \\(\\textbf{a}_1, \\textbf{a}_2\\). It can be reviewed as the inner product of two “vectors”, one is the vector of coefficients, \\((x_1, x_2)^{\\top}\\), that we are familiar with, and another is a vector of two vectors, \\((\\textbf{a}_1, \\textbf{a}_2)^{\\top}\\). Based on this thinking, we can rewrite it as\n\\[\n  \\textbf{y} = (\\textbf{a}_1, \\textbf{a}_2) \\begin{pmatrix}\nx_1\\\\\nx_2\n\\end{pmatrix}\n\\]\nLet’s focus on \\((\\textbf{a}_1, \\textbf{a}_2)\\). It is a 2 by 2 rectangle array and we denote it by a bold capital letter, \\(\\textbf{A}\\), and name it matrix!\n\n\n\n\n\n\nPrevious page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_18.html",
    "href": "MathToolBox/la/la_18.html",
    "title": "Orthonormal Matrix",
    "section": "",
    "text": "Previous page | LA4DS Homepage"
  },
  {
    "objectID": "MathToolBox/la/la_02.html",
    "href": "MathToolBox/la/la_02.html",
    "title": "Basic Operators",
    "section": "",
    "text": "In a space of vectors, we need to define some operations for studying the relations among vectors. In this section, we focus on two basic operations, i.e. scalar multiplication and addition, and their geometric meanings."
  },
  {
    "objectID": "MathToolBox/la/la_02.html#scalar-multiplication",
    "href": "MathToolBox/la/la_02.html#scalar-multiplication",
    "title": "Basic Operators",
    "section": "Scalar multiplication",
    "text": "Scalar multiplication\nFor a vector \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n\\right)^{\\top}\\) and a scalar \\(k\\), \\(k\\cdot \\textbf{x} = \\left( kx_1, kx_2, \\dots, kx_n\\right)^{\\top}\\). In data science, scalar multiplication is very common, the simplest example would be we want to change the units of a variable. In the Swedish men’s height example, the units is M, if we want to change it to CM, then we actually calcluate\n\\[\n  100\\times(1.78, 1.85, 1.74, 1.82, 1.90, 1.88)^{\\top}\n\\] Scalar multiplication often appears in the form of weights as well. We will see more examples in the linear combination.\nThis operation can only change the magnitude (norm) of the vector, and you can verify the following equation \\[\n  \\|k\\textbf{x}\\| = k\\|\\textbf{x}\\|\n\\] From the geometric point of view, after doing this operation, the vector will be stretched.\n\n\n\n\n\nAlso, \\(k\\textbf{x}\\) represents a line in the direction of and crossing the original point.\nGiven a vector \\(\\textbf{x}\\) there is a special scalar multiplication if set \\(k = \\|\\textbf{x}\\|^{-1}\\) since the length of the scaled vector is \\(1\\) and it is called the unit vector."
  },
  {
    "objectID": "MathToolBox/la/la_02.html#addition",
    "href": "MathToolBox/la/la_02.html#addition",
    "title": "Basic Operators",
    "section": "Addition",
    "text": "Addition\nSo far we have only considered single vectors and will consider the first operation on two vectors, addition. For vectors \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n\\right)^{\\top}\\) and \\(\\textbf{y} = \\left( y_1, y_2, \\dots, y_n\\right)^{\\top}\\), \\(\\textbf{x} + \\textbf{y} = \\left( x_1 + y_1, x_2+y_2, \\dots, x_n + y_n\\right)^{\\top}\\)\nFrom a geometric point of view, addition follows the so-called ‘parallelogram rule’:\n\nTwo vectors complete a parallelogram, then the sum of the two vectors is the directed diagonal of the parallelogram.\n\n\n\n\n\n\nThe difference between two vectors can be viewed as the first vector adding the second vector which is rescaled by \\(-1\\)\n\n\n\n\n\nThe last example essentially shows that the two vectors create the resulting purple vector through the two basic operations. ‘Create’ is the key word here, introducing the next concept, linear combination.\n\nPrevious page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_17.html",
    "href": "MathToolBox/la/la_17.html",
    "title": "Eigen Decomposition",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_home.html",
    "href": "MathToolBox/la/la_home.html",
    "title": "Linear Algebra for Data Science",
    "section": "",
    "text": "To some extent, machine learning is simply multivariate statistics focused on prediction; and statistics, especially multivariate statistics, is, in a sense, an advanced application of linear algebra. This illustrates the importance of linear algebra as a mathematical tool. It not only plays a key role in computation but is also crucial for a deep understanding of algorithms.\nSo, how can you tell if your knowledge of linear algebra is sufficient? Take a look at the diagram below. If you can understand the logical relationships between the modules, then you’re good to go. If not, you may need to spend a bit more time reviewing.\nLinear algebra isn’t very beginner-friendly for many people. If you start with calculating determinants, finding inverse matrices, or computing eigenvalues and eigenvectors, you will likely be overwhelmed by its tedious calculations and the vast number of concepts to memorize. Even if you manage to get through it with sheer mental effort, the knowledge you acquire will be lifeless—it won’t serve you well, let alone blossom into something new.\nSo, what is the key to learning linear algebra? It’s understanding the geometric meaning. The idea of combining numbers and shapes can offer you a tangible lifeline in the depths of abstract concepts. There are plenty of helpful tutorials on linear algebra online, and among them, Professor Strang’s MIT OpenCourseWare is a must. Everyone should listen to his lectures at least once. In the era of social media, there’s no shortage of interesting creators explaining linear algebra with vivid animations, like 3Blue1Brown. Here, I provide a concise summary of linear algebra knowledge tailored for data science. The logic behind my summary follows the mind map above. I hope it helps you."
  },
  {
    "objectID": "MathToolBox/la/la_home.html#notes",
    "href": "MathToolBox/la/la_home.html#notes",
    "title": "Linear Algebra for Data Science",
    "section": "Notes:",
    "text": "Notes:\n\nIntegrated Notes\nPagination notes"
  },
  {
    "objectID": "MathToolBox/la/la_home.html#list-of-items",
    "href": "MathToolBox/la/la_home.html#list-of-items",
    "title": "Linear Algebra for Data Science",
    "section": "List of items:",
    "text": "List of items:\n\n\nVector Part:\n1. Vectors\n2. Basic Operators\n3. Linear Combination\n4. Linearly Independent\n5. Basis\n6. Inner Product\n7. Orthogonal and Projection\n8. Review and Outlook\n\nMatrix Part:\n9. Matrix, its true color\n10. Rank\n11. Multiplication\n12. Square Matrix\n13. Transpose, Inverse, Determinant, and Trace\n14. Symmetric Matrix\n15. Quadratic Form\n16. Eigen-value and Eigen-vector\n17. Eigen Decomposition\n18. Orthonormal Matrix"
  },
  {
    "objectID": "MathToolBox/la/la_01.html",
    "href": "MathToolBox/la/la_01.html",
    "title": "Vector",
    "section": "",
    "text": "What is a vector? First, a vector is an array of numbers. For example,\n\\[\n  \\textbf{x} = \\left( 38, 1, 170, 67, 2, 0 \\right).\n\\]\nConventionally, we use a lowercase letter in bold to denote a vector. There are two ways to arrange these numbers, horizontally or vertically. If we stack the numbers vertically, then I get a column vector, like this\n\\[\n  \\textbf{x} =  \\begin{pmatrix}\n38\\\\\n1\\\\\n170\\\\\n67\\\\\n2\\\\\n0\n\\end{pmatrix}\n\\] Conventionally, again, it defaults to a column vector when we say a vector. However, a column vector takes much space, therefore people place it flat as a row vector and add a little “T” in the upper right corner.\n\\[\n  \\textbf{x} = \\left( 38, 1, 170, 67, 2, 0 \\right)^{\\top}.\n\\]\nThe little “T” denotes an operation, transpose, which means changing the size of the array from (1,7) to (7,1). Thus vector is still a column vector.\nIn data science, there usually are two ways to understand a vector. First, it can be viewed as a single case with observations on several variables. We call this vector as an observation, or a case, or an example. For example, the vector above presents a 38 year old guy’s (1 indicates male) basic information. Hight is 170, weight is 67, he has two kids, and the number of pub visits per month is 0. He is probably a home guy.\nAnother example is displayed in the following 3D scatter plot. You can move your cursor to a certain point, and you’ll find a string of numbers. Each string of numbers represents a flower, and all the flowers are blooming in this 3D world, which we usually call the feature space.\n\n\n\n\n\n\nSecond, it also can be understood as a sample of a single variable. For example, \\[\n  (1.78, 1.85, 1.74, 1.82, 1.90, 1.88)^{\\top}\n\\] it is a sample of height of audlt Swedish men."
  },
  {
    "objectID": "MathToolBox/la/la_01.html#array-view",
    "href": "MathToolBox/la/la_01.html#array-view",
    "title": "Vector",
    "section": "",
    "text": "What is a vector? First, a vector is an array of numbers. For example,\n\\[\n  \\textbf{x} = \\left( 38, 1, 170, 67, 2, 0 \\right).\n\\]\nConventionally, we use a lowercase letter in bold to denote a vector. There are two ways to arrange these numbers, horizontally or vertically. If we stack the numbers vertically, then I get a column vector, like this\n\\[\n  \\textbf{x} =  \\begin{pmatrix}\n38\\\\\n1\\\\\n170\\\\\n67\\\\\n2\\\\\n0\n\\end{pmatrix}\n\\] Conventionally, again, it defaults to a column vector when we say a vector. However, a column vector takes much space, therefore people place it flat as a row vector and add a little “T” in the upper right corner.\n\\[\n  \\textbf{x} = \\left( 38, 1, 170, 67, 2, 0 \\right)^{\\top}.\n\\]\nThe little “T” denotes an operation, transpose, which means changing the size of the array from (1,7) to (7,1). Thus vector is still a column vector.\nIn data science, there usually are two ways to understand a vector. First, it can be viewed as a single case with observations on several variables. We call this vector as an observation, or a case, or an example. For example, the vector above presents a 38 year old guy’s (1 indicates male) basic information. Hight is 170, weight is 67, he has two kids, and the number of pub visits per month is 0. He is probably a home guy.\nAnother example is displayed in the following 3D scatter plot. You can move your cursor to a certain point, and you’ll find a string of numbers. Each string of numbers represents a flower, and all the flowers are blooming in this 3D world, which we usually call the feature space.\n\n\n\n\n\n\nSecond, it also can be understood as a sample of a single variable. For example, \\[\n  (1.78, 1.85, 1.74, 1.82, 1.90, 1.88)^{\\top}\n\\] it is a sample of height of audlt Swedish men."
  },
  {
    "objectID": "MathToolBox/la/la_01.html#geometry-view",
    "href": "MathToolBox/la/la_01.html#geometry-view",
    "title": "Vector",
    "section": "Geometry View",
    "text": "Geometry View\nAnother understanding is to understand a vector from a geometric point of view as a directed line segment starting from the origin in the coordinate space and ending at the point of this string of numbers in the space. Here is an example of a vector in a 2D space.\n\n\n\n\n\nThis way of understanding allows us to learn linear algebra intuitively from a geometric point of view. For example, we can quantify a vector from two geometric perspectives, i.e. magnitude and direction. The magnitude of a vector can be quantified by the length of the corresponding arrow and we call it as norm. For a general vector \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n \\right)^{\\top}\\), its norm is \\(\\|\\textbf{x}\\|=\\sqrt{ \\sum_{i=1}^n x^2_i }\\).\n\n\n\n\n\nDirection is another important characteristic of a vector. The angle between the vector and x-axes can determine it. How can the angle be quantified? We will return to it when discussing an important operator in linear algebra, the inner product.\n\n\n\n\n\nLet’s go ahead and study more from a geometric point of view.\n\nPrevious page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_14.html",
    "href": "MathToolBox/la/la_14.html",
    "title": "Symmetric Matrix",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_04.html",
    "href": "MathToolBox/la/la_04.html",
    "title": "Linear Independent",
    "section": "",
    "text": "Next, I will show you one simple counter-example. Considering two vectors pointing in the same direction, we know that one vector can be represented as another vector scaled by some scalar. Therefore, the linear combination of the two vectors is equivalent to a scalar multiplication of arbitrary one vector. In this case, the resulting vector only stays in one direction or presents a line but not the 2D plane. Thus, we can’t create arbitrary vectors by the two vectors through linear combinations. It is also the case when the two vectors point in opposite directions.\n\n\n\n\n\nIn simple words, for two overlapping vectors, we can only create a new vector in the direction of two vectors. In a mathematical language, we can use a linear combination of two vectors with non-zero coefficients to get the zero vector. In this case, we say the two vectors are linearly dependent.\n\n\n\n\n\nOppositely, for two non-overlapping vectors, we can’t use a linear combination of them to create the zero vector unless the coefficients all are zeros, and we say the two vectors are linearly independent.\n\n\n\n\n\nThis definition can be extended to a more general scenario and leads to the general definition of linear dependence.\n\n\n\n\n\n\nFor 2D space, we can maximumly have two linearly independent vectors. For k-D space, we can maximumly have k linearly independent vectors. Is that true?\n\nGreat! It is a good time to introduce the next concept, basis, which is not only important in linear algebra but also in data analysis.\n\nPrevious page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_11.html",
    "href": "MathToolBox/la/la_11.html",
    "title": "Multiplication",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_13.html",
    "href": "MathToolBox/la/la_13.html",
    "title": "Transpose, Inverse, Determinant, and Trace",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_06.html",
    "href": "MathToolBox/la/la_06.html",
    "title": "Inner Product",
    "section": "",
    "text": "So far, we have defined one operation on two vectors, i.e. addition, and can see the power of geometry. In order to use more intuitive geometric ideas, we need to introduce another operation on two vectors, that is the inner product."
  },
  {
    "objectID": "MathToolBox/la/la_06.html#inner-product-at-first-sight",
    "href": "MathToolBox/la/la_06.html#inner-product-at-first-sight",
    "title": "Inner Product",
    "section": "Inner product, at first sight",
    "text": "Inner product, at first sight\nThe inner product is also well known as the dot product. Here we use the inner product notations. For two vectors \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n\\right)^{\\top}\\) and \\(\\textbf{y} = \\left( y_1, y_2, \\dots, y_n\\right)^{\\top},\\) the inner product is \\(\\langle \\textbf{x}, \\textbf{y} \\rangle = \\textbf{x}^{\\top}\\textbf{y} = \\sum_{i=1}^n x_iy_i\\). In textbooks in certain disciplines, the inner product is also represented as \\(\\textbf{x} \\cdot \\textbf{y}\\).\n\n\n\n\n\nRemark 1: Since the inner product of two vectors is a scalar, the result doesn’t depend on the order of two vectors.\n\n\n\n\n\nRemark 2: A special case is the inner product of a vector and itself. We actually have seen it before and it is the square of norm of this vector."
  },
  {
    "objectID": "MathToolBox/la/la_06.html#inner-product-the-geometric-view",
    "href": "MathToolBox/la/la_06.html#inner-product-the-geometric-view",
    "title": "Inner Product",
    "section": "Inner product, the geometric view",
    "text": "Inner product, the geometric view\nIn a 2D space, two non-overlapping vectors can form a triangle. Based on the cosine theorem, one can show that the inner product of two vectors is equal to the cosine value of the angle between two vectors times the product of the length of two vectors, i.e.\n\\[\n  \\langle \\textbf{x}, \\textbf{y} \\rangle =  \\textbf{x}^{\\top}\\textbf{y} = cos(\\theta)\\cdot ||\\textbf{x} ||||\\textbf{y}||\n\\]\nIf the two vectors all are unit vectors, then the inner product is equal to the cosine value of the angle between the two vectors. In one word, the inner product of two vectors is proportional to the cosine value of the angle between two vectors, i.e. \\(\\langle \\textbf{x}, \\textbf{y} \\rangle =  \\textbf{x}^{\\top}\\textbf{y} \\propto   cos(\\theta).\\) Now, the connection between geometry and algebra has been established.\n\n\n\n\n\nBased on this geometrical idea, one may have realized that the inner product quantifies the similarity of two vectors since the angle indicates if the two vectors are close to each other. This idea is very important in data science and it has been applied in a proptotpye algorithm in machine learning, that is the well-known algorithm, perceptron algorithm. In statistics, one famous statistic is just based on the inner product. Do you know what is that?\n\n\n\n\n\nWe have seen that there is a strong connection between inner product and the angle between two vectors. In our real life and mathematics, an angle of 90 degrees is special and the most useful one and it leads to the next concept, orthogonal.\n\nPrevious page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "04_about_me.html",
    "href": "04_about_me.html",
    "title": "About me",
    "section": "",
    "text": "My name is Xijia Liu (刘西嘉) or Sia.\n\n\n\nTL: I am originally from China. This is the landmark of my hometown, Beijing-the temple of heaven. This is the place where I played as a child. My parents still live near the Heaven temple park. BL: In Sweden, people usually pronounce my name as ‘Sia’. I love this somewhat sweet nickname because it’s also the name of a popular ice cream brand in the Nordics. TM: This is the landmark of Uppsala, Uppsala Cathedral, which is said to be the largest Eastern Orthodox church in the Nordic countries. I spent five wonderful years in Uppsala. The campus culture and the colorful autumn left a deep impression on me. BM: This is the emblem of Uppsala University. I completed my Ph.D. study in statistics at Uppsala University. TR: Currently, I live in Umeå with my wife and two naughty boys. This is a vibrant city; the Northern Lights are a common sight; it’s a paradise for winter sports enthusiasts. BR: I work as a senior lecturer at the Department of Statistics of Umeå University.\n\n\n\nAcademic Interests:\n\nMultivariate data analysis\nMachine Learning\nFunction data analysis\nStatistical modelling in general\n\nHobbies:\n\nWeiqi: its English name is Go. An ancient board game from China. With only two rules, you can build your universe with black and white stones. I really enjoy playing Go, but my level isn’t very high, just an amateur 2-dan.\nBadminton: In recent years, I have started playing badminton. After several training sessions with my son, I fell in love with this sport. Its charm lies in the fact that on the court, you need both abundant physical energy and a clear mind."
  },
  {
    "objectID": "02_m_index.html",
    "href": "02_m_index.html",
    "title": "Mímisbrunnr",
    "section": "",
    "text": "On this page, you can see the mathematical concepts and tools I’ve collected for my courses. You can think of it as a small dictionary of math for data science. In different courses, if needed, you will be directed to a specific item to refresh or learn it.\n\n\nLinear Algebra\nProbability\nCalculus and Optimization\nAdvance Probability Theory\n\n\n\n\n\nMímisbrunnr, or Mimir’s Well, is a significant source of wisdom in Norse mythology. It is located beneath one of the roots of Yggdrasil, the World Tree, and is guarded by Mimir, a figure renowned for his vast knowledge and wisdom. The well contains immense wisdom, and it is said that those who drink from it gain great knowledge and understanding. Odin drinks from Mímisbrunnr, he sacrificed one of his eyes to the wellspring in exchange for a drink. Odin tells us with his actions that there’s truly no free lunch, or, in other words, no pain no gain. Source: Wikipidia."
  },
  {
    "objectID": "MathToolBox/la/la_12.html",
    "href": "MathToolBox/la/la_12.html",
    "title": "Square Matrix",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_07.html",
    "href": "MathToolBox/la/la_07.html",
    "title": "Orthogonal and Projection",
    "section": "",
    "text": "In linear algebra, if the angle between two vectors is 90 degrees, then we say they are orthogonal.\nRecall your knowledge in high school, the cosine value of 90 degrees is 0. This fact provides a simple method to determine whether two vectors are orthogonal, i.e. two vectors are orthogonal if and only if their inner product is zero.\n\n\n\n\n\nOrthogonal is an important concept in many ways. In statistics, there is a simple and good example. From the previous section, we learned that the correlation between two variables is just the cosine value of the angle between two variables. So if two variables are orthogonal, then they are also uncorrelated. Another reason is that the concept of orthogonal leads to the next important concept, projection."
  },
  {
    "objectID": "MathToolBox/la/la_05.html",
    "href": "MathToolBox/la/la_05.html",
    "title": "Basis",
    "section": "",
    "text": "Before, we emphasized a keyword, ‘create’, and say an arbitrary vector can be created by two non-overlapping vectors in 2D space. Now, I want to replace this keyword with ‘represented’, maybe a more formal one. With this new keyword, we move our attention from “resulting vector” to “the two original linearly independent vectors”. We call them a set of basis in the sense that they are the backbone of the space constituted by all possible vectors.\n\n\n\nI have a metaphor that may not particularly fit. The basis is like the various departments under the student union of a university, such as the study department, the culture and sports department, the life department, and so on. The students in these departments are by no means all students in this university, but they can represent all students very well. If a student is dissatisfied with the university, it is best not to quarrel with the dean alone. The smartest way is to negotiate with the institute through the student union. Keep this in mind! Creator: Alejandro A. Alvarez | Credit: Alejandro A. Alvarez / Staff Photographer\n\n\nLet’s summarize. In a 2D space, a pair of non-overlapped vectors can be a basis. So the angle between the two vectors is very essential. Then how do we deal with angles in linear algebra? Another question is can we find a better basis in the sense that the coefficients of linear combination can be easily obtained? These questions lead to the next important operation, the inner product.\n\n\n\n\n\n\nPrevious page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_10.html",
    "href": "MathToolBox/la/la_10.html",
    "title": "Rank",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_00.html",
    "href": "MathToolBox/la/la_00.html",
    "title": "Linear Algebra for Data Science",
    "section": "",
    "text": "Linear algebra is a basic tool in many disciplines, and multivariate statistical analysis is inseparable from it. If you asked me to give this course another name, I think it would be “Applied Linear Algebra”. In this section, I will provide you with a comprehensive review combining some statistical ideas. In addition, I believe the best way of learning linear algebra is through geometry, so many animations are applied to visualize the geometry. I hope it can help you recall everything and even have a deeper understanding of linear algebra.\nBefore we start, I have one question for you. Who is the protagonist of linear algebra? You may think it is the Matrix since usually we start from many complicated calculations on matrices in most linear algebra courses. However, I don’t think so and believe the leading man in linear algebra is Vector!\n\n\n\nNeo, the leading man in film ‘The Matrix’. Source: Google.\n\n\n\nLA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_15.html",
    "href": "MathToolBox/la/la_15.html",
    "title": "Quadratic Form",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_03.html",
    "href": "MathToolBox/la/la_03.html",
    "title": "Linear Combination",
    "section": "",
    "text": "Inspired by subtraction, it is obvious that if one changes the coefficients, then we will end up with another vector.\n\n\n\n\n\nInversely, we also can find two proper coefficients such that an arbitrary vector (purple) can be represented as the sum of two given vectors (red and blue).\n\n\n\n\n\nSo, the linear combination of two vectors is \\(c_1\\textbf{x}_1 + c_2\\textbf{x}_2.\\) In general, the linear combination can be defined as the weighted sum of a group of vectors. For \\(\\textbf{x}_1, \\textbf{x}_2, \\dots, \\textbf{x}_n\\), and n scalars \\(c_1, c_2, \\dots, c_n\\), the linear combination is presented as \\(\\sum_{i=1}^n c_i\\textbf{x}_i\\).\nIt seems that we can create all the vectors as long as hold two vectors in a 2D space. However, that is not always true.\n\nPrevious page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_16.html",
    "href": "MathToolBox/la/la_16.html",
    "title": "Eigen-value and Eigen-vector",
    "section": "",
    "text": "Previous page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la.html",
    "href": "MathToolBox/la/la.html",
    "title": "Linear Algebra for Data Science",
    "section": "",
    "text": "Linear algebra is a basic tool in many disciplines, and multivariate statistical analysis is inseparable from it. If you asked me to give this course another name, I think it would be “Applied Linear Algebra”. In this section, I will provide you with a comprehensive review combining some statistical ideas. In addition, I believe the best way of learning linear algebra is through geometry, so many animations are applied to visualize the geometry. I hope it can help you recall everything and even have a deeper understanding of linear algebra.\nBefore we start, I have one question for you. Who is the protagonist of linear algebra? You may think it is the Matrix since usually we start from many complicated calculations on matrices in most linear algebra courses. However, I don’t think so and believe the leading man in linear algebra is Vector!"
  },
  {
    "objectID": "MathToolBox/la/la.html#vector",
    "href": "MathToolBox/la/la.html#vector",
    "title": "Linear Algebra for Data Science",
    "section": "1.1 Vector",
    "text": "1.1 Vector\n\nArray View:\nWhat is a vector? First, a vector is an array of numbers. For example,\n\\[\n  \\textbf{x} = \\left( 38, 1, 170, 67, 2, 0 \\right).\n\\]\nConventionally, we use a lowercase letter in bold to denote a vector. There are two ways to arrange these numbers, horizontally or vertically. If we stack the numbers vertically, then I get a column vector, like this\n\\[\n  \\textbf{x} =  \\begin{pmatrix}\n38\\\\\n1\\\\\n170\\\\\n67\\\\\n2\\\\\n0\n\\end{pmatrix}\n\\] Conventionally, again, it defaults to a column vector when we say a vector. However, a column vector takes much space, therefore people place it flat as a row vector and add a little “T” in the upper right corner.\n\\[\n  \\textbf{x} = \\left( 38, 1, 170, 67, 2, 0 \\right)^{\\top}.\n\\]\nThe little “T” denotes an operation, transpose, which means changing the size of the array from (1,7) to (7,1). Thus vector is still a column vector.\nIn data science, there usually are two ways to understand a vector. First, it can be viewed as a single case with observations on several variables. We call this vector as an observation, or a case, or an example. For example, the vector above presents a 38 year old guy’s (1 indicates male) basic information. Hight is 170, weight is 67, he has two kids, and the number of pub visits per month is 0. He is probably a home guy.\nAnother example is displayed in the following 3D scatter plot. You can move your cursor to a certain point, and you’ll find a string of numbers. Each string of numbers represents a flower, and all the flowers are blooming in this 3D world, which we usually call the feature space.\n\n\n\n\n\n\nSecond, it also can be understood as a sample of a single variable. For example, \\[\n  (1.78, 1.85, 1.74, 1.82, 1.90, 1.88)^{\\top}\n\\] it is a sample of height of audlt Swedish men.\n\n\nGeometry View\nAnother understanding is to understand a vector from a geometric point of view as a directed line segment starting from the origin in the coordinate space and ending at the point of this string of numbers in the space. Here is an example of a vector in a 2D space.\n\n\n\n\n\nThis way of understanding allows us to learn linear algebra intuitively from a geometric point of view. For example, we can quantify a vector from two geometric perspectives, i.e. magnitude and direction. The magnitude of a vector can be quantified by the length of the corresponding arrow and we call it as norm. For a general vector \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n \\right)^{\\top}\\), its norm is \\(\\|\\textbf{x}\\|=\\sqrt{ \\sum_{i=1}^n x^2_i }\\).\n\n\n\n\n\nDirection is another important characteristic of a vector. The angle between the vector and x-axes can determine it. How can the angle be quantified? We will return to it when discussing an important operator in linear algebra, the inner product.\n\n\n\n\n\nLet’s go ahead and study more from a geometric point of view."
  },
  {
    "objectID": "MathToolBox/la/la.html#basic-operators",
    "href": "MathToolBox/la/la.html#basic-operators",
    "title": "Linear Algebra for Data Science",
    "section": "1.2 Basic Operators",
    "text": "1.2 Basic Operators\nIn a space of vectors, we need to define some operations for studying the relations among vectors. In this section, we focus on two basic operations, i.e. scalar multiplication and addition, and their geometric meanings.\n\nScalar multiplication\nFor a vector \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n\\right)^{\\top}\\) and a scalar \\(k\\), \\(k\\cdot \\textbf{x} = \\left( kx_1, kx_2, \\dots, kx_n\\right)^{\\top}\\). In data science, scalar multiplication is very common, the simplest example would be we want to change the units of a variable. In the Swedish men’s height example, the units is M, if we want to change it to CM, then we actually calcluate\n\\[\n  100\\times(1.78, 1.85, 1.74, 1.82, 1.90, 1.88)^{\\top}\n\\] Scalar multiplication often appears in the form of weights as well. We will see more examples in the linear combination.\nThis operation can only change the magnitude (norm) of the vector, and you can verify the following equation \\[\n  \\|k\\textbf{x}\\| = k\\|\\textbf{x}\\|\n\\] From the geometric point of view, after doing this operation, the vector will be stretched.\n\n\n\n\n\nAlso, \\(k\\textbf{x}\\) represents a line in the direction of and crossing the original point.\nGiven a vector \\(\\textbf{x}\\) there is a special scalar multiplication if set \\(k = \\|\\textbf{x}\\|^{-1}\\) since the length of the scaled vector is \\(1\\) and it is called the unit vector.\n\n\n\n\n\n\n\nAddition\nSo far we have only considered single vectors and will consider the first operation on two vectors, addition. For vectors \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n\\right)^{\\top}\\) and \\(\\textbf{y} = \\left( y_1, y_2, \\dots, y_n\\right)^{\\top}\\), \\(\\textbf{x} + \\textbf{y} = \\left( x_1 + y_1, x_2+y_2, \\dots, x_n + y_n\\right)^{\\top}\\)\nFrom a geometric point of view, addition follows the so-called ‘parallelogram rule’:\n\nTwo vectors complete a parallelogram, then the sum of the two vectors is the directed diagonal of the parallelogram.\n\n\n\n\n\n\nThe difference between two vectors can be viewed as the first vector adding the second vector which is rescaled by \\(-1\\)\n\n\n\n\n\nThe last example essentially shows that the two vectors create the resulting purple vector through the two basic operations. ‘Create’ is the key word here, introducing the next concept, linear combination."
  },
  {
    "objectID": "MathToolBox/la/la.html#linear-combination",
    "href": "MathToolBox/la/la.html#linear-combination",
    "title": "Linear Algebra for Data Science",
    "section": "1.3 Linear Combination",
    "text": "1.3 Linear Combination\nInspired by subtraction, it is obvious that if one changes the coefficients, then we will end up with another vector.\n\n\n\n\n\nInversely, we also can find two proper coefficients such that an arbitrary vector (purple) can be represented as the sum of two given vectors (red and blue).\n\n\n\n\n\nSo, the linear combination of two vectors is \\(c_1\\textbf{x}_1 + c_2\\textbf{x}_2.\\) In general, the linear combination can be defined as the weighted sum of a group of vectors. For \\(\\textbf{x}_1, \\textbf{x}_2, \\dots, \\textbf{x}_n\\), and n scalars \\(c_1, c_2, \\dots, c_n\\), the linear combination is presented as \\(\\sum_{i=1}^n c_i\\textbf{x}_i\\).\nIt seems that we can create all the vectors as long as hold two vectors in a 2D space. However, that is not always true."
  },
  {
    "objectID": "MathToolBox/la/la.html#linearly-independent",
    "href": "MathToolBox/la/la.html#linearly-independent",
    "title": "Linear Algebra for Data Science",
    "section": "1.4 Linearly Independent",
    "text": "1.4 Linearly Independent\nNext, I will show you one simple counter-example. Considering two vectors pointing in the same direction, we know that one vector can be represented as another vector scaled by some scalar. Therefore, the linear combination of the two vectors is equivalent to a scalar multiplication of arbitrary one vector. In this case, the resulting vector only stays in one direction or presents a line but not the 2D plane. Thus, we can’t create arbitrary vectors by the two vectors through linear combinations. It is also the case when the two vectors point in opposite directions.\n\n\n\n\n\nIn simple words, for two overlapping vectors, we can only create a new vector in the direction of two vectors. In a mathematical language, we can use a linear combination of two vectors with non-zero coefficients to get the zero vector. In this case, we say the two vectors are linearly dependent.\n\n\n\n\n\nOppositely, for two non-overlapping vectors, we can’t use a linear combination of them to create the zero vector unless the coefficients all are zeros, and we say the two vectors are linearly independent.\n\n\n\n\n\nThis definition can be extended to a more general scenario and leads to the general definition of linear dependence.\n\n\n\n\n\n\nFor 2D space, we can maximumly have two linearly independent vectors. For k-D space, we can maximumly have k linearly independent vectors. Is that true?\n\nGreat! It is a good time to introduce the next concept, basis, which is not only important in linear algebra but also in data analysis."
  },
  {
    "objectID": "MathToolBox/la/la.html#basis",
    "href": "MathToolBox/la/la.html#basis",
    "title": "Linear Algebra for Data Science",
    "section": "1.5 Basis",
    "text": "1.5 Basis\nBefore, we emphasized a keyword, ‘create’, and say an arbitrary vector can be created by two non-overlapping vectors in 2D space. Now, I want to replace this keyword with ‘represented’, maybe a more formal one. With this new keyword, we move our attention from “resulting vector” to “the two original linearly independent vectors”. We call them a set of basis in the sense that they are the backbone of the space constituted by all possible vectors.\n\n\n\nI have a metaphor that may not particularly fit. The basis is like the various departments under the student union of a university, such as the study department, the culture and sports department, the life department, and so on. The students in these departments are by no means all students in this university, but they can represent all students very well. If a student is dissatisfied with the university, it is best not to quarrel with the dean alone. The smartest way is to negotiate with the institute through the student union. Keep this in mind! Creator: Alejandro A. Alvarez | Credit: Alejandro A. Alvarez / Staff Photographer\n\n\nLet’s summarize. In a 2D space, a pair of non-overlapped vectors can be a basis. So the angle between the two vectors is very essential. Then how do we deal with angles in linear algebra? Another question is can we find a better basis in the sense that the coefficients of linear combination can be easily obtained? These questions lead to the next important operation, the inner product."
  },
  {
    "objectID": "MathToolBox/la/la.html#inner-product",
    "href": "MathToolBox/la/la.html#inner-product",
    "title": "Linear Algebra for Data Science",
    "section": "1.6 Inner Product",
    "text": "1.6 Inner Product\nSo far, we have defined one operation on two vectors, i.e. addition, and can see the power of geometry. In order to use more intuitive geometric ideas, we need to introduce another operation on two vectors, that is the inner product.\n\nInner product, at first sight\nThe inner product is also well known as the dot product. Here we use the inner product notations. For two vectors \\(\\textbf{x} = \\left( x_1, x_2, \\dots, x_n\\right)^{\\top}\\) and \\(\\textbf{y} = \\left( y_1, y_2, \\dots, y_n\\right)^{\\top},\\) the inner product is \\(\\langle \\textbf{x}, \\textbf{y} \\rangle = \\textbf{x}^{\\top}\\textbf{y} = \\sum_{i=1}^n x_iy_i\\). In textbooks in certain disciplines, the inner product is also represented as \\(\\textbf{x} \\cdot \\textbf{y}\\).\n\n\n\n\n\nRemark 1: Since the inner product of two vectors is a scalar, the result doesn’t depend on the order of two vectors.\n\n\n\n\n\nRemark 2: A special case is the inner product of a vector and itself. We actually have seen it before and it is the square of norm of this vector.\n\n\n\n\n\n\n\nInner product, the geometric view\nIn a 2D space, two non-overlapping vectors can form a triangle. Based on the cosine theorem, one can show that the inner product of two vectors is equal to the cosine value of the angle between two vectors times the product of the length of two vectors, i.e.\n\\[\n  \\langle \\textbf{x}, \\textbf{y} \\rangle =  \\textbf{x}^{\\top}\\textbf{y} = cos(\\theta)\\cdot ||\\textbf{x} ||||\\textbf{y}||\n\\]\nIf the two vectors all are unit vectors, then the inner product is equal to the cosine value of the angle between the two vectors. In one word, the inner product of two vectors is proportional to the cosine value of the angle between two vectors, i.e. \\(\\langle \\textbf{x}, \\textbf{y} \\rangle =  \\textbf{x}^{\\top}\\textbf{y} \\propto   cos(\\theta).\\) Now, the connection between geometry and algebra has been established.\n\n\n\n\n\nBased on this geometrical idea, one may have realized that the inner product quantifies the similarity of two vectors since the angle indicates if the two vectors are close to each other. This idea is very important in data science and it has been applied in a proptotpye algorithm in machine learning, that is the well-known algorithm, perceptron algorithm. In statistics, one famous statistic is just based on the inner product. Do you know what is that?\n\n\n\n\n\nWe have seen that there is a strong connection between inner product and the angle between two vectors. In our real life and mathematics, an angle of 90 degrees is special and the most useful one and it leads to the next concept, orthogonal."
  },
  {
    "objectID": "MathToolBox/la/la.html#orthogonal-and-projection",
    "href": "MathToolBox/la/la.html#orthogonal-and-projection",
    "title": "Linear Algebra for Data Science",
    "section": "1.7 Orthogonal and Projection",
    "text": "1.7 Orthogonal and Projection\n\nOrthogonal\nIn linear algebra, if the angle between two vectors is 90 degrees, then we say they are orthogonal.\nRecall your knowledge in high school, the cosine value of 90 degrees is 0. This fact provides a simple method to determine whether two vectors are orthogonal, i.e. two vectors are orthogonal if and only if their inner product is zero.\n\n\n\n\n\nOrthogonal is an important concept in many ways. In statistics, there is a simple and good example. From the previous section, we learned that the correlation between two variables is just the cosine value of the angle between two variables. So if two variables are orthogonal, then they are also uncorrelated. Another reason is that the concept of orthogonal leads to the next important concept, projection.\n\n\nProjection\nA projection can be viewed as a vector created by two vectors through the following procedure. Suppose we have two non-overlapping vectors \\(\\textbf{x}\\) and \\(\\textbf{y}\\), a beam of light is incident from a direction perpendicular to \\(\\textbf{y}\\) cause a shadow of \\(\\textbf{x}\\) on \\(\\textbf{y}\\) and the shadow is called the projection of \\(\\textbf{x}\\) onto \\(\\textbf{y}\\).\n\n\n\n\n\nOne interesting problem is how to represent the shadow \\(\\textbf{x}_1\\) by the two existing vectors. You may have realized that \\(\\textbf{x}_1\\) overlaps with vector \\(\\textbf{y}\\), therefore \\(\\textbf{x}_1\\) should be scaled \\(\\textbf{y}\\), i.e. \\(\\textbf{x}_1 = k \\cdot \\textbf{y}\\). So the problem becomes finding the coefficient \\(k\\). Based on the orthogonal properties, we can find that\n\\[\n  k = \\frac{\\textbf{x}^{\\top} \\textbf{y}}{\\textbf{y}^{\\top} \\textbf{y}} \\text{, and }  \\textbf{x}_1 = \\frac{\\textbf{x}^{\\top} \\textbf{y}}{\\textbf{y}^{\\top} \\textbf{y}} \\textbf{y}.\n\\] through the following derivations.\n\n\n\n\n\nRemark: The scalar \\(k\\) is just the length of projection and it is called scalar projection. One can go further and find that the scalar projection is obtained by the inner product of \\(\\textbf{x}\\) and the unit vector in \\(\\textbf{y}\\) direction.\n\n\n\n\n\n\n\nOrthonormal Basis\nIn 2D space, there is a pair of two special and nice unit vectors, \\((1,0)^{\\top}\\) and \\((0,1)^{\\top}\\). One can easily get the scalar projection of arbitrary vectors on them, isn’t it? Obviously, they also form a basis of 2D space. This basis is very nice since they are not only unit vectors but also orthogonal to each other. We call this kind of basis an orthonormal basis."
  },
  {
    "objectID": "MathToolBox/la/la.html#review-and-outlook",
    "href": "MathToolBox/la/la.html#review-and-outlook",
    "title": "Linear Algebra for Data Science",
    "section": "1.8 Review and Outlook",
    "text": "1.8 Review and Outlook\nLet’s review linear combinations in 2D space from a view of the inner product.\n\\[\n  \\textbf{y} = x_1 \\cdot \\textbf{a}_1 + x_2 \\cdot \\textbf{a}_2\n\\]\nThe vector \\(\\textbf{y}\\) is represented as the linear combination of the two basis vectors \\(\\textbf{a}_1, \\textbf{a}_2\\). It can be reviewed as the inner product of two “vectors”, one is the vector of coefficients, \\((x_1, x_2)^{\\top}\\), that we are familiar with, and another is a vector of two vectors, \\((\\textbf{a}_1, \\textbf{a}_2)^{\\top}\\). Based on this thinking, we can rewrite it as\n\\[\n  \\textbf{y} = (\\textbf{a}_1, \\textbf{a}_2) \\begin{pmatrix}\nx_1\\\\\nx_2\n\\end{pmatrix}\n\\]\nLet’s focus on \\((\\textbf{a}_1, \\textbf{a}_2)\\). It is a 2 by 2 rectangle array and we denote it by a bold capital letter, \\(\\textbf{A}\\), and name it matrix!"
  },
  {
    "objectID": "MathToolBox/la/la.html#matrix-its-true-colors",
    "href": "MathToolBox/la/la.html#matrix-its-true-colors",
    "title": "Linear Algebra for Data Science",
    "section": "2.1 Matrix, its true colors",
    "text": "2.1 Matrix, its true colors\n\nMatrix, at first sight\nThe most straightforward definition of a matrix is the rectangle array. One needs to use two numbers to describe the size (shape) of a matrix, the numbers of rows and columns. In the following example, the matrix consists of 3 rows and 4 columns. We say it is a 3 by 4 matrix. One neat but informative notation is \\(\\textbf{A} = \\left\\{ a_{ij} \\right\\}_{3 \\times 4}\\). In this course, keep in mind, we always use \\(i\\) indicates for the row index and \\(j\\) for column index.\n\n\n\n\n\nYou may remember the idea in the previous section that a matrix can be viewed as a “row vector” of column vectors. In this example, suppose we can find 3 linearly independent vectors from the 4 column vectors, then they can be viewed as a basis and generate a space. Similarly, the matrix also can be viewed as a “column vector” of row vectors, and they also can generate a space if they are linearly independent. In statistics, especially MDA, we mainly work with a data matrix. Let’s talk about it later on.\n\n\n\n\n\n\n\nMatrix, true colors\nThe definition of the matrix above is very straightforward and very helpful when we understand a data matrix in MDA. However, the disadvantage of this definition is that it is too dry, and we cannot understand it from a functional point of view as well as a geometric point of view. Let’s take a look at what its true color. Let’s turn back to the concept of ‘linear combination’ and watch the following animation.\n\n\n\n\n\nFrom this animation, we can see that the \\(n \\times p\\) matrix \\(\\textbf{A}\\) determines a map (function) from the \\(\\mathbb{R}^p\\) space to the \\(\\mathbb{R}^n\\) space. We call this map or function a linear transformation. In some sense, the matrix can be viewed as a bridge between two spaces.\nRemark: we also want to emphasize that the meaning of an action that pre-multiply a vector by a matrix, \\(\\textbf{Ax}\\), even though we have not defined matrix multiplication yet. Keep in mind that this action means that one uses the elements of \\(\\textbf{x}\\) as coefficients to calculate the linear combination of column vectors of the matrix \\(\\textbf{A}\\). It is very helpful when we discuss the next important concept, the rank of a matrix."
  },
  {
    "objectID": "MathToolBox/la/la.html#rank",
    "href": "MathToolBox/la/la.html#rank",
    "title": "Linear Algebra for Data Science",
    "section": "2.2 Rank",
    "text": "2.2 Rank\nLet’s have a look at some examples.\n\n\n\n\n\nIn this example, we transform a 2D vector \\(\\textbf{x}\\) to a 3D space through the matrix \\(\\textbf{A}\\). However, is the new space “richer” than the original one? It seems not since the degree of freedom of taking values in the resulting vector is 2, and not different from the input vector \\(\\textbf{x}\\). What does that mean? Let’s understand it from the geometric point of view.\n\n\n\n\n\nFrom the animation, we can see that if one puts all the vectors in the original space (blue) into the conveyor belt, matrix \\(\\textbf{A}\\), then the output vectors will constitute the new space (purple). Therefore the output space is just a subspace of the \\(\\mathbb{R}^3\\). Notice that there are 2 linearly independent vectors in the matrix \\(\\textbf{A}\\). Does this number, 2, determine the degree of freedom of taking values in the output space? Yes, it is true, and let’s see one more example.\n\n\n\n\n\nIn this example, the second column vector of \\(\\textbf{A}\\) is two times the first column vector, and the degree of taking values in the output vector is exactly 1. One can imagine what happened from the geometrical point of view. Two overlapped vectors stand in the \\(\\mathbb{R}^3\\) space and one wants to use the vector in \\(\\mathbb{R}^2\\) as coefficients to do a linear combination with them. All vectors will be pinned to the dotted line where the column vectors stand.\n\n\n\n\n\nLet’s summarize. Even though the vectors can be transformed to a higher dimension space, the “size” of the output space can only be determined by the number of linearly independent column vectors, and this number is called the rank of the matrix."
  },
  {
    "objectID": "MathToolBox/la/la.html#matrix-multiplication",
    "href": "MathToolBox/la/la.html#matrix-multiplication",
    "title": "Linear Algebra for Data Science",
    "section": "2.3 Matrix Multiplication",
    "text": "2.3 Matrix Multiplication\nIn the previous section, we mentioned that \\(\\textbf{A}_{n\\times p}\\textbf{x}_{p \\times 1}\\) is a kind of multiplication, a vector pre-multiplied by a matrix. Since a vector also can be viewed as a special matrix with only one column, can we define the multiplication between matrices? If so, what is the meaning of matrix multiplication? Let’s think about these questions from the linear transformation point of view. Watch the following animation first.\n\n\n\n\n\nIn this example, matrices \\(\\textbf{A}\\) and \\(\\textbf{B}\\) determine two linear transformations. One can transform a \\(p \\times 1\\) vector from \\(\\mathbb{R}^p\\) to \\(\\mathbb{R}^n\\) through matrix \\(\\textbf{A}\\) and then transform the resulting vector to \\(\\mathbb{R}^q\\) by matrix \\(\\textbf{B}\\). If we consider the two steps as one, it can be represented as \\(\\textbf{BAx}\\). Note that the order of calculation is from right to left. Now the question is can we calculate it from left to right but still get the same results? The answer is yes.\n\n\n\n\n\nWe define the multiplication as \\(\\textbf{C} = \\left\\{ c_{i,j} \\right\\} = \\left\\{ \\sum_{k=1}^n b_{i,k} a_{k,j}  \\right\\}\\), note that the elements in the resulting matrix, \\(\\sum_{k=1}^n b_{i,k} a_{k,j}\\), is the inner product between the \\(i\\)th row vector of \\(\\textbf{B}\\) and the \\(j\\)th column vector of \\(\\textbf{A}\\). Based on the definition of matrix multiplication, the resulting matrix \\(\\textbf{C}\\) is desirable to our original aim, i.e. combine the two steps as one. In other words, transforming a vector by \\(\\textbf{A}\\) then \\(\\textbf{B}\\) is equivalent to transforming the vector by the multiplication result \\(\\textbf{C}\\). Great! We have answered the second question, i.e. the meaning of matrix multiplication is to composite two maps as one. However, about the first question, the answer depends on the size of the two matrices, for example,\n\n\n\n\n\nIn other words, the order matters in matrix multiplication. You can fly from Umeå to Stockholm first, then change fly to Beijing, however, it is ridiculous if you change the order of two flys."
  },
  {
    "objectID": "MathToolBox/la/la.html#square-matrix",
    "href": "MathToolBox/la/la.html#square-matrix",
    "title": "Linear Algebra for Data Science",
    "section": "2.4 Square Matrix",
    "text": "2.4 Square Matrix"
  },
  {
    "objectID": "MathToolBox/la/la.html#transpose-inverse-determinant-and-trace",
    "href": "MathToolBox/la/la.html#transpose-inverse-determinant-and-trace",
    "title": "Linear Algebra for Data Science",
    "section": "2.5 Transpose, Inverse, Determinant, and Trace",
    "text": "2.5 Transpose, Inverse, Determinant, and Trace"
  },
  {
    "objectID": "MathToolBox/la/la.html#symmetric-matrix",
    "href": "MathToolBox/la/la.html#symmetric-matrix",
    "title": "Linear Algebra for Data Science",
    "section": "2.6 Symmetric Matrix",
    "text": "2.6 Symmetric Matrix"
  },
  {
    "objectID": "MathToolBox/la/la.html#quadratic-form",
    "href": "MathToolBox/la/la.html#quadratic-form",
    "title": "Linear Algebra for Data Science",
    "section": "2.7 Quadratic Form",
    "text": "2.7 Quadratic Form"
  },
  {
    "objectID": "MathToolBox/la/la.html#eigen-values-and-eigen-vector",
    "href": "MathToolBox/la/la.html#eigen-values-and-eigen-vector",
    "title": "Linear Algebra for Data Science",
    "section": "2.8 Eigen-values and Eigen-vector",
    "text": "2.8 Eigen-values and Eigen-vector"
  },
  {
    "objectID": "MathToolBox/la/la.html#eigen-decomposition",
    "href": "MathToolBox/la/la.html#eigen-decomposition",
    "title": "Linear Algebra for Data Science",
    "section": "2.9 Eigen Decomposition",
    "text": "2.9 Eigen Decomposition"
  },
  {
    "objectID": "MathToolBox/la/la.html#orthonormal-matrix",
    "href": "MathToolBox/la/la.html#orthonormal-matrix",
    "title": "Linear Algebra for Data Science",
    "section": "2.10 Orthonormal Matrix",
    "text": "2.10 Orthonormal Matrix\n\nLA4DS Homepage"
  },
  {
    "objectID": "MathToolBox/la/la_09.html",
    "href": "MathToolBox/la/la_09.html",
    "title": "Matrix, its true color",
    "section": "",
    "text": "The most straightforward definition of a matrix is the rectangle array. One needs to use two numbers to describe the size (shape) of a matrix, the numbers of rows and columns. In the following example, the matrix consists of 3 rows and 4 columns. We say it is a 3 by 4 matrix. One neat but informative notation is \\(\\textbf{A} = \\left\\{ a_{ij} \\right\\}_{3 \\times 4}\\). In this course, keep in mind, we always use \\(i\\) indicates for the row index and \\(j\\) for column index.\n\n\n\n\n\nYou may remember the idea in the previous section that a matrix can be viewed as a “row vector” of column vectors. In this example, suppose we can find 3 linearly independent vectors from the 4 column vectors, then they can be viewed as a basis and generate a space. Similarly, the matrix also can be viewed as a “column vector” of row vectors, and they also can generate a space if they are linearly independent. In statistics, especially MDA, we mainly work with a data matrix. Let’s talk about it later on."
  },
  {
    "objectID": "01_c_index.html",
    "href": "01_c_index.html",
    "title": "Norns’ Blessing",
    "section": "",
    "text": "On this page, you can find a list of my courses. May the three goddesses of fate bless our journey through data, models, and predictions.\n\n\nMachine Learning with R, Part 1\nMachine Learning with R, Part 2\nMultivariate Data Analysis\nComputational Statistics\nUsing R to Learn Basic Mathematics for Data Science\n\n\n\n\n\nIn Norse mythology, the three Norns—Urd, who governs the past; Verdandi, who governs the present; and Skuld, who governs the future—perfectly align with the perspectives we take as statisticians: data reflecting the past, models capturing the present, and predictions shaping the future. Figure: the Norns spin the threads of fate at the foot of Yggdrasil, the tree of the world. Source: google search."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_home.html",
    "href": "Courses/c_mlwr1_2024/l3/l3_home.html",
    "title": "Lecture 3: Basic Knowledge in Probability Theory",
    "section": "",
    "text": "In this lecture, we will cover the fundamental concepts of probability theory. It covers the following things:"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_home.html",
    "href": "Courses/c_mlwr1_2024/l4/l4_home.html",
    "title": "Lecture 4: Gaussian Discrimination Analysis",
    "section": "",
    "text": "In this lecture, we will explore the first models for classification, developed using probability theory. Additionally, we will introduce several methods for evaluating classification models. It covers the following things:"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html",
    "title": "Machine Learning with R, Part 1",
    "section": "",
    "text": "This is a public course designed for master’s level students in Umeå University, and it is given at the department of statistics, Umeå University. Students need to have elementary programming knowledge and some understanding of basic statistics, at most understanding regression analysis. In the first part of this course, we will focus on familiarizing students with the basic concepts of machine learning through basic linear models and preparing them for the second part of the course.\n\n\n\nIn this new era, learning various skills in data science is as straightforward as learning to get a driver’s license, but not everyone can become an engineer or technician for the Ferrari racing team. My visions: First, every student can get their license and add new tools to their data analysis toolbox. Second, I hope this course can provide some helps to those who want to understand the engine behind the tools. If that sounds like you, you’ll need to invest some time and a bit of patience :)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#introduction",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#introduction",
    "title": "Machine Learning with R, Part 1",
    "section": "",
    "text": "This is a public course designed for master’s level students in Umeå University, and it is given at the department of statistics, Umeå University. Students need to have elementary programming knowledge and some understanding of basic statistics, at most understanding regression analysis. In the first part of this course, we will focus on familiarizing students with the basic concepts of machine learning through basic linear models and preparing them for the second part of the course.\n\n\n\nIn this new era, learning various skills in data science is as straightforward as learning to get a driver’s license, but not everyone can become an engineer or technician for the Ferrari racing team. My visions: First, every student can get their license and add new tools to their data analysis toolbox. Second, I hope this course can provide some helps to those who want to understand the engine behind the tools. If that sounds like you, you’ll need to invest some time and a bit of patience :)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#course-design",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#course-design",
    "title": "Machine Learning with R, Part 1",
    "section": "Course Design",
    "text": "Course Design\nIn this course, after understanding the basic concepts of machine learning, we start with useful tools, the elementary probability knowledge and an introduction to the R language. In lecture 4, 5, and 7, we focus on linear solutions to machine learning problems. Two linear classifiers, Linear Discriminant Analysis and Logistic regression, and linear regression models. During this period, we will also introduce a simple idea of nonlinear extension, which will lead to an important concept and challenge in machine learning, namely the overfitting problem. By this, we consider avoiding overfitting as a model selection problem, and the corresponding methods are discussed."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#textbook",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#textbook",
    "title": "Machine Learning with R, Part 1",
    "section": "Textbook",
    "text": "Textbook\nWe use ‘An Introduction to statistical learning’ as our textbook. The website of the book: https://www.statlearning.com. On this website, you can not only get an electronic copy of this book, but also find a lot of useful information."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#teaching-methods",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#teaching-methods",
    "title": "Machine Learning with R, Part 1",
    "section": "Teaching Methods",
    "text": "Teaching Methods\nAs an online course, we naturally choose the flipped classroom teaching method. That is, students first read and study the materials and textbooks we provided, and then conduct laboratory lessons after discussions in a question-and-answer (Q&A) session."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#examination-method",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#examination-method",
    "title": "Machine Learning with R, Part 1",
    "section": "Examination method",
    "text": "Examination method\nWe use a combination of take home exams and oral interviews to assess students’ mastery of course knowledge."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#list-of-lectures",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#list-of-lectures",
    "title": "Machine Learning with R, Part 1",
    "section": "List of lectures",
    "text": "List of lectures\n\nLecture 1. Introduction to Machine Learning\nLecture 2. Introduction to R Programming\nLecture 3. Basic Knowledge in Probability Theory\nLecture 4. Gaussian Discriminant Analysis\nLecture 5. Regression Models\nLecture 6. Model Validation and Selection\nLecture 7. Logistic Regression Classifier"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html",
    "href": "Courses/c_mlwr1_2024/l5/l5.html",
    "title": "Lecture 5: Regression Models",
    "section": "",
    "text": "In this lecture, we discuss regression problems. First, we review the training methods of linear regression models from different perspectives. Then, a basic nonlinear extension idea is introduced, feature mapping, and use it to introduce the first nonlinear regression model, polynomial regression. Afterward, we understand polynomial regression from the perspective of basis functions, and then introduce another commonly used nonlinear model, spline regression. At the end of this lecture, we introduce a new concept, the overfitting problem. It is a core challenge in machine learning from the training perspective, and we focus on it in the next lecture."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#least-square-method",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#least-square-method",
    "title": "Lecture 5: Regression Models",
    "section": "5.1.1 Least Square Method",
    "text": "5.1.1 Least Square Method\nLeast Square method was proposed by the famous mathematician Gauss. He applied this method for data analysis to accurately predict the time of the second appearance of the asteroid, Ceres. His idea is quite simple: to use data to find the optimal line, represented by two coefficients, in order to minimize prediction errors, see the plot below. \n\n\n\n\n\nSimple Linear Regression Model: Red circles are observations (x,y), Black dashline is the regression model, Blue Dots on the dash line are prediction y_hat, the lenght of segment in purple is the prediction error.\n\n\n\n\nSuppose that we have a set of paired observations, \\((y_1,x_1), \\dots, (y_n, x_n)\\), then the mathematical formulation is \\[\n  \\hat{w}_0, \\hat{w}_1 = \\arg\\min_{w_0, w_1} \\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n\\] where \\(\\hat{y}_i = w_0 + w_1x_i\\).\nThe solution of this optimization problem is \\(\\widehat{w}_1=\\frac{\\sum_{i=1}^N(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^N(x_i-\\overline{x})^2}\\), and \\(\\widehat{w}_0=\\overline{y}-\\widehat{w}_1\\overline{x}\\)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#maximum-likelihood-method",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#maximum-likelihood-method",
    "title": "Lecture 5: Regression Models",
    "section": "5.1.3 Maximum Likelihood Method",
    "text": "5.1.3 Maximum Likelihood Method\nDifferent from least square method, next, we are going to reexamine the regression model from the perspective of probability models. To do so, we assume the error term \\(\\epsilon\\) is normally distributed, \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\). Based on this assumption, the target variable is normally distributed conditional on feature variables. Therefore, we essentially predict the expected value of the target variable conditional on \\(X_1, \\dots, X_p\\) as a linear model, i.e.\n\n\n\nRegression Model: the expected value of target variable is a linear function of X1 and X2\n\n\n\\[\n  \\text{E}(Y | X_1, \\dots, X_p) = w_0 + w_1 X_1 + w_2 X_2 + \\dots + w_p X_p\n\\] Based on the normality assumption, another estimation method, MLE, for coefficients can be discussed.\nMLE of regression model: Under the normality assumption, we have \\(y_i \\sim \\mathcal{N}( w_0 + w_1x_1 , \\sigma^2)\\). If you remember the secrete message behind the normal distribution, the likelihood of each observation, \\(y_i\\), is inversely proportionally to the distance to the expected value, i.e.  \\[\n   f( y_i | w_0, w_1, \\sigma^2 ) \\propto -(y_i - (w_0 + w_1x_1 ) )^2\n\\] Therefore the likelihood function of the sample \\(\\left\\{ y_i, x_i \\right\\}_{i=1}^n\\) is \\[\n  \\log \\left( L( w_0, w_1, \\sigma^2 | (y_i, x_i) ) \\right) \\propto -\\sum_{i=1}^n (y_i - (w_0 + w_1x_1 ) )^2\n\\]\nNotice that, on the LHS, it is sum square of residual. Therefore, minimize sum square of residuals is equivalent to maximize the log likelihood function. In other words, the two methods are equivalent."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#basic-ideas-of-non-linear-extension",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#basic-ideas-of-non-linear-extension",
    "title": "Lecture 5: Regression Models",
    "section": "5.2.1 Basic ideas of Non-linear Extension",
    "text": "5.2.1 Basic ideas of Non-linear Extension\nNonlinear models are not the focus of our course, however, here we can explore the basic approach to finding nonlinear models, that is feature mapping. Feature mapping involves the basic idea of introducing new variables by transforming the original feature variables with functions. This expands the original feature space, allowing the exploration of potential linear solutions within the augmented feature space. Let’s start with the toy example of classification problem above.\nIn the classification problem, we can consider three new variables \\(\\left(h_1 = x_1^2, h_2 = \\sqrt{2}x_1x_2, h_3 = x_2^2 \\right)\\) instead of the two original variables \\(x_1, x_2\\). The new data set is visualized in the following 3D plot.\n\n\n\n\n\n\nIf you rotate the 3D scatter plot above, you may notice that the same set of observed cases becomes linearly separable in the new feature space, see the LHS of plot below. For example, we can train an LDA classifier using three new feature variables, \\(\\left(h_1, h_2, h_3 \\right)\\). The decision boundary of this classifier would correspond to the gray plane in the 3D space. By building the new feature space, the previously non-linearly separable data points may now become linearly separable, allowing the LDA classifier to effectively separate the two classes. Also, if we change the direction of our view, the linear model in the augmented feature space is eventually a nonlinear model in the original space, see the LHS of the plots below.\n\n\n\nAugmented Feature Space\n\n\nWe can refer to this idea as the feature mapping idea. In simple terms, we need to find an appropriate new space, which we call the augmented feature space, and train our linear model within it. This augmented feature space is entirely determined by a transformation function, \\(\\phi(\\textbf{x}) : \\text{p-D space} \\to \\text{q-D space}\\) which we refer to as feature mappings.\nThis concept plays a significant role in machine learning, and almost all advanced nonlinear models are based on this idea. For example:\n\nBefore deep learning dominated AI, the Support Vector Machine (SVM) applied this idea indirectly through the kernel function. The kernel allows the SVM to operate in a higher-dimensional space without explicitly computing the coordinates in that space.\nIn the world of ensemble methods, which dominate structured data tasks, each single model can be seen as a form of feature mapping.\nIn the foundation deep learning model, the neural network, the chain of transformations through neurons can also be seen as a feature transformation.\n\nThis idea of transforming data into a higher-dimensional space (whether directly or indirectly) enables models to handle complex, nonlinear relationships that would otherwise be difficult to capture in the original space."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#polynomial-regression-1",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#polynomial-regression-1",
    "title": "Lecture 5: Regression Models",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nIn the regression problem, we also can consider an augmented feature space \\((x, x^2, x^3)\\) according to the data visualization. In other words, we consider the true model as \\(y = w_0 + w_1x + w_2x^2 + w_3x^3\\) which is the 3rd order polynomial function of \\(x\\). We call this regression model as polynomial regression model, however, it is an essentially a linear regression in the augmented feature space."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_4_1.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_4_1.html",
    "title": "2.4.1 Basic Data Types",
    "section": "",
    "text": "Numeric: Represents real numbers (both integers and decimals).\n\nx = 3.14; y = 2\nclass(x)\nclass(y)\nThis is a sample sentence with an annotated text that shows additional information on hover.\n\nInteger ( NE ): Represents whole numbers explicitly (defined with L).\n\nx = 5L; y = c(1L, 2L)\nclass(x)\nclass(y)\n\nComplex ( NE ): Represents complex numbers with real and imaginary parts.\n\nx = 1 + 2i\nclass(x)\n\nCharacter: Represents strings of text enclosed in quotes.\n\nx = \"Machine Learning\"\nclass(x)\n\nLogical: Represents Boolean values: TRUE or FALSE.\n\nx = TRUE; y = FALSE\nclass(x)\nclass(y)\nx & y # '&' is the AND operator\nx | y # '|' is the OR operator\nx + y # Can we calculate it?\nx/y # And this?\nRemark: if you apply basic arithmetic operators to logical values, then they will be transformed to numeric first, and then execute the calculations.\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_4_2.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_4_2.html",
    "title": "2.4.2 Array",
    "section": "",
    "text": "Array is a data structure that can hold multiple values in a grid-like format, organized into dimensions (like rows, columns, and layers). Arrays can be one-dimensional (like a vector), two-dimensional (like a matrix), or multi-dimensional (with three or more dimensions)."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_4_2.html#vector",
    "href": "Courses/c_mlwr1_2024/l2/l2_4_2.html#vector",
    "title": "2.4.2 Array",
    "section": "Vector:",
    "text": "Vector:\nIn R, a vector is one-dimensional array that contains data with same type. The outputs of the functions c, seq, and rep are essentially vectors, with the only difference being that they are viewed as a sequence with a length, but do not have dimensional attributes. For example,\nx = seq(1,10, by = 0.1)\nclass(x) # output is numeric\nlength(x) # output is 91\ndim(x) # output is NULL\nIn some programming language, such kind of objects without dimensional attributes are called tuple. This means that we can slice this type of sequence object to extract the desired elements. We will introduce slicing in the discussion about vectors below.\nOne can use the array function to embed the dimension attribute into such an object and then obtain a real vector, for example,\nx = seq(0,1,0.2)\nx = array(x, dim = length(x)) \n# length(x) is the default value for 'dim'\nclass(x)\nlength(x)\ndim(x)\nOne can use [index] to slice an array, for examples\n# Guess the outputs first, verify them in R\nx = seq(0,1,by=0.2)\nx[1] # the first element in an array\nx[length(x)] \nx[1:3]  \nx[c(1,2,5)]\nx[rep(2:4, 3)]\nx[-1]\nx[-c(1,2,5)]\nSome operations, examples\nx = seq(0,1,by=0.2)\n2*x + 1 # elementwise operations\nt(x) # transpose\ndim(t(x)) # column vector\ndim(t(t(x))) # row vector"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_4_2.html#matrix",
    "href": "Courses/c_mlwr1_2024/l2/l2_4_2.html#matrix",
    "title": "2.4.2 Array",
    "section": "Matrix:",
    "text": "Matrix:\nMatrices in R are the same as matrices in linear algebra, that is matrix is a rectangle array. So, we can apply function array to create matrix as well, but need to specify two numbers for the argument dim. For example,\nX = array(1:10, dim = c(2,5))\nX\nFrom the output of the example above we can see that we can only fill in the elements by columns. If we want to fill in the elements by rows, then we have to use another function matrix. For example,\nX = matrix(1:10, nrow = 2, ncol = 5, byrow = FALSE) \nX\nX = matrix(1:10, nrow = 2, ncol = 5, byrow = TRUE)\nX\nFor slicing of matrix, see examples\n# Guess the outputs first, verify them in R\nX = matrix(1:9,3,3,byrow=T)\nX[1,2]\nX[1,]\nX[,3]\nX[1:2,2:3]\nX[ array(c(1,2), dim = c(1,2)) ]\nX[ array(c(1,2,3,2), dim = c(2,2)) ]\nMore operations for matrix, see examples\nX = matrix(1:9,3,3,byrow=T)\nX*X # elementwise multiplication\nX%*%X # matrix multiplication (in linear algebra)\nt(X) # transpose \ndet(X) # calculate determinant\n\nX = matrix(c(1,2,2,1),2,2)\neigen(X) # eigen decomposition of a square matrix\nTwo useful functions, cbind and rbind, which can combine objects by column and row, respectively. See examples,\nx1 = 1:3\nx2 = 3:5\n(X = cbind(x1,x2))\n(X = rbind(x1,x2))\nclass(X) # check the type of outputs\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_4_4.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_4_4.html",
    "title": "2.4.4 List",
    "section": "",
    "text": "The last type of data structure I want to introduce to you is the list. Compared to matrices and data frames, its frequency of use in programming is not very high, but it is still very useful and reflects a higher level of understanding of R data structures. It is a versatile and powerful data structure that can hold an ordered collection of elements, which can be of different types. Unlike vectors, matrices, and data frames, lists can contain mixed data types, including other lists, vectors, data frames, and even functions. conversely, a data frame is essentially a type of list. Let’s see some examples\n# Example 1 \ny = list()\ny[[1]] = 1:10 # In a list, we use double square brackets to slice. \ny[[2]] = letters[2:7]\ny[[3]] = function(x){2*x}\ny\ny[[3]](2) # what is this?\n# Example 2\ny = list()\ny$x = 1:10 # In a list, we also can use `$` to slice.\ny$letters = letters[2:7]\ny$double = function(x){2*x}\ny\ny$double(2) # Now, you understand why data frame is also a list.\nRemark: Function is alo a kind of data type or structure in R. It has been investigated before, so we don’t discuss it here again.\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_5_2.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_5_2.html",
    "title": "2.5.2 For Loop",
    "section": "",
    "text": "For loop repeats a block of code a specified number of times. The syntax is\n# Syntax of for loop\nfor (i in SEQUENCE) {\n    # Code block to be executed\n}\nIn loop syntax, the code block will be executed with respect to each elements in SEQUENCE. For example, we want to print all numbers within 50 that are divisible by 3.\n# Example 4\nfor(i in 1:50){\n  if (i %% 3 == 0){ # %% is modulo operator, i.e. returns the remiainder of i/3\n    print(i)\n  }\n}\nRemark: The form of SEQUENCE can vary in many ways and doesn’t even need to be a numeric array. See the examples below.\n# Example 5: print all the even number within 50\nfor(i in seq(2, 50, by = 2)){\n  print(i)\n}\n# Example 6: say hello to famous mathematicians \nname_list = c(\"Gauss\", \"Euler\", \"Fourier\", \"Cantor\")\nfor(i in name_list){\n  print(paste0(\"Hello, \", i, \"!\"))\n}\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_5_1.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_5_1.html",
    "title": "2.5.1 If/Else",
    "section": "",
    "text": "The if/else statement executes a block of code based on a specified condition. The syntax is:\n# Syntax of If/Else statement\nif (CONDITION) {\n    # Code block to be executed if CONDITION is TRUE\n} else {\n    # Code block to be executed if CONDITION is FALSE\n}\nIn this syntax, the code block will be executed based on whether the CONDITION is TRUE or FALSE. For example:\n# Example 1\nx = 6\nif(x &gt; 5){\n  print(\"x is greater than 5\")\n}else{\n  print(\"x is 5 or less\")\n}\nIn R, else is not mandatory, especially for simple conditional checks. R allows you to use if to evaluate a condition and execute code if the condition is true, without requiring an else block to handle other cases. For example,\n# Example 2\nx = 6\nif(x &gt; 5){\n  print(\"x is greater than 5\")\n}\nThe ifelse function in R is used to execute one of two values based on a specified condition, element-wise across a vector. The syntax is:\n# Syntax of function 'ifelse'\nifelse(CONDITION, VALUE_IF_TRUE, VALUE_IF_FALSE)\nIn this syntax, ifelse evaluates each element of CONDITION. If the element meets the CONDITION (is TRUE), VALUE_IF_TRUE is returned; otherwise, VALUE_IF_FALSE is returned. For example:\n# Example 3\nx = c(3, 9, 1, 6, 5)\nresult = ifelse(x &gt; 5, \"grater than 5\", \"not grater than 5\")\nprint(result)\nIn this example, the code will check each element in x to see if it is greater than 5. If true, it will return “greater than 5”; otherwise, it will return “not greater than 5”.\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html",
    "href": "Courses/c_mlwr1_2024/l2/l2.html",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "",
    "text": "Machine learning is a discipline based on data and algorithms, so naturally, we need a programming language to implement algorithms and conduct experiments. For many reasons, the R language is a good choice, with its open-source nature and simple syntax being the primary ones. It’s important to note that this course does not focus on advanced applications of R, so we aim to minimize the learning curve, allowing students to quickly understand and master the language for convenient experimentation. In short, our ultimate goal is to understand models and algorithms through experimentation."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#an-overview-of-r-language",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#an-overview-of-r-language",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "2.1 An Overview of R Language",
    "text": "2.1 An Overview of R Language\nR is a powerful and versatile programming language primarily used for statistical computing, data analysis, and graphical representation. Developed in the early 1990s by Ross Ihaka and Robert Gentleman at the University of Auckland, R has since evolved into a robust tool that supports a wide range of applications in various fields, including data science, bioinformatics, and social sciences.\nMain features of R:\n\nFor machine learning, data mining, and statistical analysis: R provides a comprehensive suite of statistical functions, making it ideal for conducting complex data analyses. It supports various statistical methods and techniques, including linear and nonlinear modeling, time-series analysis, classification, clustering, and machine learning. R facilitates the implementation of machine learning algorithms for predictive modeling and data mining.\nFor data Visualization: R excels at creating high-quality graphics and visualizations. With packages like ggplot2, users can generate intricate plots and charts to effectively communicate insights and findings.\nFor data Handling: R has powerful data manipulation capabilities, especially with packages like dplyr and tidyr. These tools allow for efficient data cleaning, transformation, and reshaping, making it easier to prepare datasets for analysis.\nEfficient matrix computing: R is inherently designed for matrix operations and linear algebra, making it particularly well-suited for tasks involving matrix computations. This feature allows users to perform complex mathematical calculations efficiently, which is essential in statistics and data analysis.\nCommunity Support: R has a vibrant and active community, offering extensive resources, tutorials, and forums for users to seek help and share knowledge. This community-driven approach fosters continuous improvement and innovation within the R ecosystem."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#a-prowerful-weapon-rstudio",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#a-prowerful-weapon-rstudio",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "2.2 A Prowerful Weapon, Rstudio",
    "text": "2.2 A Prowerful Weapon, Rstudio\nWhen I was a student, we could only program using R’s rudimentary built-in interface, and many tasks required our direct involvement. After I graduated with my Ph.D., RStudio became increasingly popular due to its powerful and powerful features. We also inevitably fell into it. Nowadays, while enjoying the convenience it brings, I can’t help but feel nostalgic when I look back on those youthful days.\nRStudio is a powerful integrated development environment (IDE) specifically designed for R programming. It provides a user-friendly interface that enhances the R programming experience, making it easier for users to write code, visualize data, and manage projects. Whether you are a beginner or an experienced programmer, RStudio offers a range of features that streamline the data analysis process.\nThe main features of Rstudio are introduced in the following Figure. More advanced operations await your exploration.\n\n\n\nThe entire workspace is divided into four main areas. Top Left: This is where we write code. After writing the code, you can save it for future use. Top Right: This is our console, where you can directly enter commands at the cursor and get results. Alternatively, you can select a line or several consecutive lines in the top left code area and press cmd + enter, allowing you to quickly obtain results here. Bottom Left: This is our working environment area, where the most important tab is ‘Environment’. Here, you can easily see all the objects in the current environment at a glance. Bottom Right: There are two important tabs. First, after running data visualization commands, you will see results in tab Plots (4). Secondly, in the second tab (5), you can browse your path and search for the files you need.\n\n\n\nThe outline of this tutorial:\n\nThe first sight of R: We quickly start this journey with understanding the main actor in R, function.\nData type and structure: Different types and structure of data are introduced.\nFlow control: Continue learning the syntax of three main flow controls.\nProbability: Study the functions related to probability theory.\nOther important things: We close the tutorial by remarking on some useful things."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#love-at-first-sight",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#love-at-first-sight",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "2.3 Love at First Sight?",
    "text": "2.3 Love at First Sight?\nIn R, we can enter commands in the console to have the computer perform the corresponding tasks. For example, we want to print ‘welcome to R’\n\nprint(\"welcome to R\")\n\n[1] \"welcome to R\"\n\n\n\n2.3.1 The main actor: function\nAs a computing language specifically designed for statisticians, the most essential elements in R are functions, since almost all tasks are accomplished through various functions. Similar to most programming languages, the general form of a function in R is\nFunctionName([input 1], [input 2], ..., [argument 1], [argument 2], ...)\ni.e. it consists of a function name, inputs and arguments enclosed in parentheses. For example, you can type the following math functions in the console,\n\nsin(pi) # 'pi' is built-in constant representing the mathematical value of Pi\n\n[1] 1.224647e-16\n\nexp(1) # It returns the natural logarithm\n\n[1] 2.718282\n\nlog(10, base = 10) # The logarithm of 10 to the base 10\n\n[1] 1\n\n\nRemark: As you can see from the code above, we use # sign to comment the code, in other words, characters after # are not considered as part of the command.\nR has extensive and well-organized help documentation. You can access help for specific functions using the ? or help() function. For example:\n\n?chisq.test\n\nYou will see the help document of this function in Rstudio.\n\n\n2.3.2 Functions for generating a sequence of values\nThe first function is c function, which can create a sequence of numbers based on your inputs and store in in memory. For example, we want to create a sequence of the integers from 1 to 10 and store it in a variable x.\nx = c(1,2,3,4,5,6,7,8,9,10)\nYou also can include characters in x by function c, for example\nx = c(letters)\n(x = c(\"I\", \"like\", \"R\", \"How about you?\"))\nRemark: In R, if you use parentheses to enclose a command, then the outputs will be printed automatically.\nTyping the integers from 1 to 10 can be done by another function seq\nseq(1,10)\nor simply by colon operator :\n1:10\nWell, the simple colon operator is neat but limited to integer sequence and increments of 1 (or -1, try 10:1 in your console). Use seq when you need more control over the sequence, for example, try the following code in your console\nseq(1, 10, 2) \nseq(1, 10, length.out = 5)\nseq(0, 1, 0.1)\nThe last function for creating sequence is rep, which can create copies of your inputs, for example\n# guess what will we get by the following commands?\nrep(1, 10)\nrep(1:4, 4)\nrep(rep(1:4,4), 2)\nrep(\"I like R\", 3)\n\n\n2.3.3 Custom function\nIn R, you are allowed to encapsulate specific tasks or calculations that you can reuse throughout your code. The syntax for defining a function is\n# Syntax of custom function\nfunction_name = function(inputs, arguments) {\n  # Function body: code to execute\n  # Optionally return a value\n  return(value)\n}\nFor example, we want a function to calculate the sum of two input values\nmySum = function(x = 1, y = 1){\n    res = x+y\n    return(res)\n}\nmySum(2,3)\nmySum()\n\nIn the parentheses after the syntax key word function, the values assigned to x and y are default values, namely R will take two ones as input automatically if we don’t enter any inputs into function mySum, for example, the result of the last line above should be 2.\nIf we don’t use return to indicate the results should be returned, then the results of the last line in the function will be returned as output."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#data-type-and-structure-in-r",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#data-type-and-structure-in-r",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "2.4 Data type and structure in R",
    "text": "2.4 Data type and structure in R\nIn R, data types (structure) define the nature of data that can be stored and manipulated. The main data types include numeric, character, logical, factor, array, matrix, data frame, list, function, each serving different purposes in data analysis and programming. Function class can check the type of a variable in the memory. Next, we list the simple types first and then illustrate more complex structures in details.\n\n2.4.1 Basic Data Types\n\nNumeric: Represents real numbers (both integers and decimals).\n\nx = 3.14; y = 2\nclass(x)\nclass(y)\nThis is a sample sentence with an annotated text that shows additional information on hover.\n\nInteger ( NE ): Represents whole numbers explicitly (defined with L).\n\nx = 5L; y = c(1L, 2L)\nclass(x)\nclass(y)\n\nComplex ( NE ): Represents complex numbers with real and imaginary parts.\n\nx = 1 + 2i\nclass(x)\n\nCharacter: Represents strings of text enclosed in quotes.\n\nx = \"Machine Learning\"\nclass(x)\n\nLogical: Represents Boolean values: TRUE or FALSE.\n\nx = TRUE; y = FALSE\nclass(x)\nclass(y)\nx & y # '&' is the AND operator\nx | y # '|' is the OR operator\nx + y # Can we calculate it?\nx/y # And this?\nRemark: if you apply basic arithmetic operators to logical values, then they will be transformed to numeric first, and then execute the calculations.\n\n\n2.4.2 Array\nArray is a data structure that can hold multiple values in a grid-like format, organized into dimensions (like rows, columns, and layers). Arrays can be one-dimensional (like a vector), two-dimensional (like a matrix), or multi-dimensional (with three or more dimensions).\nVector:\nIn R, a vector is one-dimensional array that contains data with same type. The outputs of the functions c, seq, and rep are essentially vectors, with the only difference being that they are viewed as a sequence with a length, but do not have dimensional attributes. For example,\nx = seq(1,10, by = 0.1)\nclass(x) # output is numeric\nlength(x) # output is 91\ndim(x) # output is NULL\nIn some programming language, such kind of objects without dimensional attributes are called tuple. This means that we can slice this type of sequence object to extract the desired elements. We will introduce slicing in the discussion about vectors below.\nOne can use the array function to embed the dimension attribute into such an object and then obtain a real vector, for example,\nx = seq(0,1,0.2)\nx = array(x, dim = length(x)) \n# length(x) is the default value for 'dim'\nclass(x)\nlength(x)\ndim(x)\nOne can use [index] to slice an array, for examples\n# Guess the outputs first, verify them in R\nx = seq(0,1,by=0.2)\nx[1] # the first element in an array\nx[length(x)] \nx[1:3]  \nx[c(1,2,5)]\nx[rep(2:4, 3)]\nx[-1]\nx[-c(1,2,5)]\nSome operations, examples\nx = seq(0,1,by=0.2)\n2*x + 1 # elementwise operations\nt(x) # transpose\ndim(t(x)) # column vector\ndim(t(t(x))) # row vector\nMatrix:\nMatrices in R are the same as matrices in linear algebra, that is matrix is a rectangle array. So, we can apply function array to create matrix as well, but need to specify two numbers for the argument dim. For example,\nX = array(1:10, dim = c(2,5))\nX\nFrom the output of the example above we can see that we can only fill in the elements by columns. If we want to fill in the elements by rows, then we have to use another function matrix. For example,\nX = matrix(1:10, nrow = 2, ncol = 5, byrow = FALSE) \nX\nX = matrix(1:10, nrow = 2, ncol = 5, byrow = TRUE)\nX\nFor slicing of matrix, see examples\n# Guess the outputs first, verify them in R\nX = matrix(1:9,3,3,byrow=T)\nX[1,2]\nX[1,]\nX[,3]\nX[1:2,2:3]\nX[ array(c(1,2), dim = c(1,2)) ]\nX[ array(c(1,2,3,2), dim = c(2,2)) ]\nMore operations for matrix, see examples\nX = matrix(1:9,3,3,byrow=T)\nX*X # elementwise multiplication\nX%*%X # matrix multiplication (in linear algebra)\nt(X) # transpose \ndet(X) # calculate determinant\n\nX = matrix(c(1,2,2,1),2,2)\neigen(X) # eigen decomposition of a square matrix\nTwo useful functions, cbind and rbind, which can combine objects by column and row, respectively. See examples,\nx1 = 1:3\nx2 = 3:5\n(X = cbind(x1,x2))\n(X = rbind(x1,x2))\nclass(X) # check the type of outputs\n\n\n2.4.3 Data Frame\nAs a programming language originally designed for statisticians, importing data and setting a specific structure for it is essential. It is so called data frame. In R, we can use various functions to read in different types of data, such as txt, csv, xlsx, and more. For example, you can apply read.table function to import data saved in a txt file. You can download Boston data here.\n# Prepare a txt file, 'Boston.txt', in a director\nsetwd(\"dir containing your data\") \n# we set the dir containg your data as the working director.\ndat = read.table(\"Boston.txt\", sep=\",\", header = T)\nRemark: Setting working director (WD) is always useful since it can simplify many things, for example, if we don’t set the WD as the folder containing ‘Boston.txt’, then you have to specify the dir in the first argument of the read.table function. Setting a WD can be done by function setwd, and for checking the current WD, you can use function getwd. In Rstudio, this action also can be done using mouse actions, see figure below.\n\n\n\nFirst, go to the right folder. Second, in tab ‘Files’, click the gear icon, then you will find it.\n\n\nData frame is a fundamental data structure used for storing tabular data, where each column can hold different types of data (e.g., numeric, character, or factor). Data frame can be created by function data.frame. For example:\n(X = cbind(x1,x2))\n(dat = data.frame(x1,x2)) \nclass(X)\nclass(dat) # it seems there is no difference between a matrix and a dataframe\nX%*%t(X) # try this \ndat%*%t(dat) # try this -&gt; matrix multiplication is not allowed.\nSo, usually, the operations and functions for a matrix are not allowed to apply to a data frame. Including different types of data is the main difference between data frame and matrix. For example:\n# with the same demo data above\nx3 = letters[1:3] # define another variable \nX = cbind(x1, x2, x3)\ndat = data.frame(x1, x2, x3)\nX\ndat # compare `X` and `dat`, draw yoru conclusions.\nFor a data frame, we still can use the same method as for matrix to slice. Another more practical way is using $ to slice. For examples:\n# with the same example above\ndat[,3] \ndat$x3\nSome useful functions for data frames\n\nhead and tail functions: they can help us to check the first and last few lines respectively. For examples:\n\ndat = iris # iris is a pre-stored data set in R which includes 150 iris flowers\nhead(dat)\ntail(dat)\nhead(dat, 10)\n\nnames function: it can help us quickly check the names of all variables.\nattach and detach functions: people feel very inconvenient to use $ to slice a data frame, but want to use the variable names directly. In this case, ´attach´ function can help us go into such kind of mode, and apparently detach function can cease this mode. For examples:\n\ndat = iris\nnames(iris) # [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"  \nSpecies # you can't find it\ndat$Species # works\n\nattach(dat)\nSpecies # also works\n\ndetach(dat)\nSpecies\n\n\n2.4.4 List\nThe last type of data structure I want to introduce to you is the list. Compared to matrices and data frames, its frequency of use in programming is not very high, but it is still very useful and reflects a higher level of understanding of R data structures. It is a versatile and powerful data structure that can hold an ordered collection of elements, which can be of different types. Unlike vectors, matrices, and data frames, lists can contain mixed data types, including other lists, vectors, data frames, and even functions. conversely, a data frame is essentially a type of list. Let’s see some examples\n# Example 1 \ny = list()\ny[[1]] = 1:10 # In a list, we use double square brackets to slice. \ny[[2]] = letters[2:7]\ny[[3]] = function(x){2*x}\ny\ny[[3]](2) # what is this?\n# Example 2\ny = list()\ny$x = 1:10 # In a list, we also can use `$` to slice.\ny$letters = letters[2:7]\ny$double = function(x){2*x}\ny\ny$double(2) # Now, you understand why data frame is also a list.\nRemark: Function is alo a kind of data type or structure in R. It has been investigated before, so we don’t discuss it here again."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#flow-control",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#flow-control",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "2.5 Flow Control",
    "text": "2.5 Flow Control\nFlow control refers to the mechanisms and structures used in programming to dictate the order in which instructions are executed in a program. It allows for making decisions, repeating actions, and controlling the flow of execution based on certain conditions or logic. In R and most programming languages, flow control is essential for creating dynamic and responsive programs. Here, we will mainly introduce loops and conditional statement.\n\n2.5.1 If/Else\nThe if/else statement executes a block of code based on a specified condition. The syntax is:\n# Syntax of If/Else statement\nif (CONDITION) {\n    # Code block to be executed if CONDITION is TRUE\n} else {\n    # Code block to be executed if CONDITION is FALSE\n}\nIn this syntax, the code block will be executed based on whether the CONDITION is TRUE or FALSE. For example:\n# Example 1\nx = 6\nif(x &gt; 5){\n  print(\"x is greater than 5\")\n}else{\n  print(\"x is 5 or less\")\n}\nIn R, else is not mandatory, especially for simple conditional checks. R allows you to use if to evaluate a condition and execute code if the condition is true, without requiring an else block to handle other cases. For example,\n# Example 2\nx = 6\nif(x &gt; 5){\n  print(\"x is greater than 5\")\n}\nThe ifelse function in R is used to execute one of two values based on a specified condition, element-wise across a vector. The syntax is:\n# Syntax of function 'ifelse'\nifelse(CONDITION, VALUE_IF_TRUE, VALUE_IF_FALSE)\nIn this syntax, ifelse evaluates each element of CONDITION. If the element meets the CONDITION (is TRUE), VALUE_IF_TRUE is returned; otherwise, VALUE_IF_FALSE is returned. For example:\n# Example 3\nx = c(3, 9, 1, 6, 5)\nresult = ifelse(x &gt; 5, \"grater than 5\", \"not grater than 5\")\nprint(result)\nIn this example, the code will check each element in x to see if it is greater than 5. If true, it will return “greater than 5”; otherwise, it will return “not greater than 5”.\n\n\n2.5.2 For Loop\nFor loop repeats a block of code a specified number of times. The syntax is\n# Syntax of for loop\nfor (i in SEQUENCE) {\n    # Code block to be executed\n}\nIn loop syntax, the code block will be executed with respect to each elements in SEQUENCE. For example, we want to print all numbers within 50 that are divisible by 3.\n# Example 4\nfor(i in 1:50){\n  if (i %% 3 == 0){ # %% is modulo operator, i.e. returns the remiainder of i/3\n    print(i)\n  }\n}\nRemark: The form of SEQUENCE can vary in many ways and doesn’t even need to be a numeric array. See the examples below.\n# Example 5: print all the even number within 50\nfor(i in seq(2, 50, by = 2)){\n  print(i)\n}\n# Example 6: say hello to famous mathematicians \nname_list = c(\"Gauss\", \"Euler\", \"Fourier\", \"Cantor\")\nfor(i in name_list){\n  print(paste0(\"Hello, \", i, \"!\"))\n}\n\n\n2.5.3 While Loop\nDifferent from a for loop, a while loop continues to execute a block of code as long as a specified condition remains true. This means that the number of iterations is not predetermined; instead, it depends on the state of the condition being evaluated. The syntax is\n# Syntax of while loop\nwhile (CONDITION) {\n    # Code block to be executed\n}\nThe CONDITION could be a logical value, or a command resulting logical value. The same example of for loop above, example 4, also can be implemented through a while loop.\n# Example 7\n# Initialize the starting number\nnumber = 3\n# While loop to print numbers divisible by 3 up to 50\nwhile(number &lt;= 50){\n  print(number)      # Print the current number\n  number = number + 3  # Move to the next multiple of 3\n}"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#about-probability",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#about-probability",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "2.6 About Probability",
    "text": "2.6 About Probability\nNote: You may temporarily ignore this part and come back to read after the lecture 3.\nAs a programming language originally designed for statisticians, functions related to probability distribution are essential. For example, how to generate random numbers from a distribution, how to calculate the probability of a random event, how to find the corresponding quantile values based on a given probability, and how to calculate the density function value of a given distribution are all well implemented in R. Below, we will introduce 4 functions using the normal (Gaussian) distribution as an example.\n\n2.6.1 Generate Random Numbers\nGenerating random numbers is essential for simulations, statistical modeling, and resampling techniques like bootstrapping, where random data or sampling is needed to test models, explore scenarios, or understand variability.\nIn R, the rnorm function is applied to generate random numbers from a normal (Gaussian) distribution. The syntax is\n# Syntax of fucntion 'rnorm'\nrnorm(SampleSize, mean = 0, sd = 1)\nFor example,\nx = rnorm(100)\nhist(x) \nRemark: We can’t simulate real random numbers on computer, but only generate pseudo-random numbers through algorithms designed to produce sequences that mimic the properties of randomness. These algorithms start with an initial value known as the random seed, which serves as the starting point for generating the sequence. By changing the seed, we can create different sequences of pseudo-random numbers, allowing for reproducibility in simulations and analyses. While these numbers may appear random, they are ultimately determined by the algorithm and the initial seed value. In R, the random seed can be controled by function ‘set.seed’, for example\n# Next, if you get the same value for 'a' and 'b', I will pay you 1000kr\n(a = rnorm(1))\n(b = rnorm(1)) \n\n# Next, if you get different values for 'a' and 'b', I will pay you 1000kr \nset.seed(2024)\n(a = rnorm(1))\nset.seed(2024)\n(b = rnorm(1))\nLife is seems like a tapestry woven with random numbers, however, to some extent it is not really random, but pseudo-random. On the one hand, it is a box of chocolates - every moment is a surprise, sweet or bitter, unfolding in unpredictable ways. You never know what the next piece will bring. However, on the other hand, we seem to be unable to escape the arrangement of fate. When you were born, God, like a careful gardener, had selected a unique seed for you. This seed contains the potential of your existence and shapes your journey. No matter what your current situation is, we should always cherish our unique seed.\n\n\n2.6.2 Find the Density Values\nThe density value is useful not only in probability but also statistics, because it presents an important quantity, likelihood value. It will be discussed in the next lecture in details.\nIn R, the dnorm function calculates the density (or height) of the normal distribution at a specific value. The syntax is\n# Syntax of function 'dnorm'\ndnorm(x, mean = 0, sd = 1)\nFor example\n# The outputs of the following two lines should be equal. Why?\ndnorm(0)\n1/sqrt(2*pi)\n\n\n2.6.3 Calculate the Probability\nIn R, the pnorm function calculates the probability of an event of a normal distribution. The syntax is\n# Syntax of function 'pnorm'\npnorm(a, mean = 0, sd = 1)\nThis function calculate the probability \\(\\Pr(X&lt;a)\\) which is the area under the normal density curve within the interval \\([-\\infty, a]\\), for example,\n\npnorm(1)\n\n[1] 0.8413447\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.4 Determine the quantile value\nQuantile value is very essential in statistics, for example, you all need quantile value whether you are conducting hypothesis testing or calculating confidence intervals. Actually, it is just an inverse operation of calculating probability, e.g. the quantile value of \\(0.84\\) for a standard normal distribution is approximately \\(1\\) The syntax is similar, for example, you should be familiar with the following quantile value\nqnorm(0.975, 0, 1) # what is it?\n\n\n2.6.5 Summary and Remark\n# Functions related to normal distribution.\nrnorm() # generate random numbers from normal distribution\ndnorm() # find the density value of normal distribution\npnorm() # calculate the probability of an event associated to normal distribution\nqnorm() # determine the quantile value of normal distribution\nRemark: As you can see, there is a pattern in the function names, i.e. a letter + norm, for example in ‘rnorm’,\n\nr: This prefix indicates that the function generates random numbers (random variates) from a specific distribution.\nnorm: This part of the name refers to the normal distribution, also known as the Gaussian distribution.\n\nThis naming convention is applied to other distributions as well. For example:\n\nrbinom: generates random numbers from a binomial distribution\ndexp: finds density value of an exponential distribution\nppois: calculates probability of a Poisson distribution\nqunif: determines the quantile value of a uniform distribution"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#other-useful-things",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#other-useful-things",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "2.7 Other Useful Things",
    "text": "2.7 Other Useful Things\nAlright, we now have a basic understanding of the most fundamental operations in R programming, most of which are things we will frequently use in this course. Next, we’ll introduce a few more useful concepts and some commonly used functions.\n\n2.7.1 Workspace\nIn R, the workspace refers to the environment where all objects (such as variables, functions, and data) are stored during an R session. It acts as a storage area that retains the data and objects you create, allowing you to work with them without needing to re-import or redefine them every time you start R. The most common scenario is when you’ve worked hard all day and want to take a break, but if you close R, all the objects in your working environment (memory) will disappear. In this case, you can save your current working environment as a workspace file, which has a .RData extension.\nThere are two ways to save your working environment as a workspace file. First, by mouse actions, you can click Session -&gt; Save Workspace As.... Or you can do it by command\nsave.image(\"FileName.RData\")\nThe next day, after enjoying the morning sunshine (if conditions permit) and your coffee, you can load this file and continue your hard work!\n\n\n2.7.2 Packages\nIf R could only be used for scientific computing, it would undoubtedly be overshadowed by numerous other scientific computing programs. The true strength of R lies in its extensibility, which is achieved through R packages. Initially, R packages were primarily written by statisticians to implement new methods, such as lme4 for fitting generalized linear mixed-effects models; survival for conducting survival analysis; psych for psychological research, and so on. However, writing packages is not exclusive to statisticians; an increasing number of non-statistical application packages have also been developed. Today, R has become incredibly versatile through the extension of various packages, for example this website is written by quarto package. Below, we will briefly illustrate how to install and load packages using examples.\ninstall.packages(\"kernelab\") \n# to install a new package. Note: the quotation marks are essential.\n\nlibrary(kernelab) \n# you can import a package by function `library`\n\n\n2.7.3 Useful Functions\nNext, some useful functions are introduced. These functions were extremely useful back when I was a student. However, in the era of RStudio, their usefulness has been greatly reduced. Nonetheless, they are still quite necessary for those who prefer keyboard operations or need to work on a server. In addition, these functions can, to some extent, enhance R users’ understanding of R programming.\n\nls function: it can list all the objects in the workspace or current environment.\nrm function: it can help us to remove objects from the workspace or current environment.\n\n# Example 1\nx = 1\nrm(x) \n# Example 2\nrm(list = ls()) # Danger Warning: This command will remove all objects listed by `ls`\n\nstr function: it displays the structure of an object.\n\n# Example 1\nx = list()\nx[[1]] = 1:10\nx[[2]] = letters[4:10]\nstr(x)\n# Example 2\nres = t.test(rnorm(30)) # do one sample t-test and save results in `res`\nstr(res) \n# You can see that the testing results are saved in a list of 10.\n# if you want to extract elements from it, the information coveryed by ´str´ is ideal.\n\nsummary function: it helps us to summarize useful information from an R objects. The information extracted depends on the type of the object. For examples\n\n# Example 1\ndat = iris[,-5] # we use the first 4 variable from iris data\nsummary(dat) # the type of ´dat´ is dataframe, then the summarized informations are...\n# Example 2\nres = t.test(rnorm(30))\nsummary(res) \n# the type of ´res´ is results of t test. The designer of this function decided \n# to show the names of all the elements in ´res´, similiar to the output of ´str´\n\nunique and table functions: they are useful when you want to check all possible values in a variable and the frequency of different possible values.\n\n# First, we create a small demo dataset\ntreatment = c(1,1,0,0,1,1,0,0)\nblock = c(1,1,1,1,2,2,2,2)\nsex = c(\"F\",\"M\",\"M\",\"M\",\"F\",\"F\",\"M\",\"F\")\nage = c(19,20,28,22,21,19,23,20)\noutcome = c(20,19,33,12,54,87,98,84)\nDat = data.frame(treatment, block, sex, age, outcome)\nhead(Data, 8)\n\n# Example 1:\nunique(Dat$sex)\ntable(Dat$sex)\nunique(Dat$age)\ntable(Dat$age)\n\n# Example 2:\ntable(Data$sex, Data$treatment) # do you know the name of the outputs?\n\nwhich function: it finds the index of elements that satisfy some conditions in a vector, or matrix, or data frame.\n\n# Example: Use the same demo data above\nwhich(Dat$sex == \"M\")\nwhich(Dat$age &lt; 21)\n\napply function: it is used to perform operations on rows or columns of matrices, data frames, or higher-dimensional arrays. It allows you to apply a function across the rows or columns without needing to use loops, making code more concise and often more efficient.\n\n# Syntax: \napply(X, margin, fun)\n# `margin` is an integer specifying whether to apply the `fun` across rows (1) or columns (2) \n# Examples: Use the demo data but ignore the variable `sex`\nDat = Dat[, -3]\napply(Dat, 2, mean)\nNext, show some useful functions for graphics. The ggplot2 package is definitely the top choice for plotting, but sometimes the following functions are more practical and convenient for data visualization. I will only list them below, and you are already strong enough to investigate them by yourself :)\n\nhist function: it can help use check the distribution of a variable.\nplot function: it is usually used to show the scatter plot of two variables.\npairs function: it shows the pairwise scatter plot of many variables."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2.html#final-words",
    "href": "Courses/c_mlwr1_2024/l2/l2.html#final-words",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "Final Words:",
    "text": "Final Words:\nAlright, our short journey with R programming has come to an end. I hope you now have a basic understanding of this software, its basic operations, and various syntax. As I mentioned earlier, the goal of this lecture, or even this course, is not solely to train you in R but rather to give you a quick introduction to this simple programming language so that we can use it to study machine learning concepts later on. The R programming language is a powerful tool in the field of data science. You can continue exploring various advanced techniques afterward.\nLastly, I want to say that the best way to learn any language is to engage in conversation; the same applies to programming languages. Only by repeatedly typing commands and scripts into the computer will you truly master and become familiar with this programming language.\n\nLecture 2 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_2.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_2.html",
    "title": "2.2 A powerful weapon, Rstudio",
    "section": "",
    "text": "When I was a student, we could only program using R’s rudimentary built-in interface, and many tasks required our direct involvement. After I graduated with my Ph.D., RStudio became increasingly popular due to its powerful and powerful features. We also inevitably fell into it. Nowadays，while enjoying the convenience it brings, I can’t help but feel nostalgic when I look back on those youthful days.\nRStudio is a powerful integrated development environment (IDE) specifically designed for R programming. It provides a user-friendly interface that enhances the R programming experience, making it easier for users to write code, visualize data, and manage projects. Whether you are a beginner or an experienced programmer, RStudio offers a range of features that streamline the data analysis process.\nThe main features of Rstudio are introduced in the following Figure. More advanced operations await your exploration.\n\n\n\nThe entire workspace is divided into four main areas. Top Left：This is where we write code. After writing the code, you can save it for future use. Top Right: This is our console, where you can directly enter commands at the cursor and get results. Alternatively, you can select a line or several consecutive lines in the top left code area and press cmd + enter, allowing you to quickly obtain results here. Bottom Left: This is our working environment area, where the most important tab is ‘Environment’. Here, you can easily see all the objects in the current environment at a glance. Bottom Right: There are two important tabs. First, after running data visualization commands, you will see results in tab Plots (4). Secondly, in the second tab (5), you can browse your path and search for the files you need.\n\n\nPrevious page: | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_0.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_0.html",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "",
    "text": "Machine learning is a discipline based on data and algorithms, so naturally, we need a programming language to implement algorithms and conduct experiments. For many reasons, the R language is a good choice, with its open-source nature and simple syntax being the primary ones. It’s important to note that this course does not focus on advanced applications of R, so we aim to minimize the learning curve, allowing students to quickly understand and master the language for convenient experimentation. In short, our ultimate goal is to understand models and algorithms through experimentation.\nOutline：\n\n2.1 An Overview of R Language\n2.2 A Prowerful Weapon, Rstudio\n2.3 Love at First Sight?\n2.4 Data type and structure in R\n2.5 Flow Control\n2.6 About Probability\n2.7 Other Useful Things\n\n\nLecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_0.html#r-programming",
    "href": "Courses/c_mlwr1_2024/l2/l2_0.html#r-programming",
    "title": "Lecture 2: A Short Introduction to R Programming",
    "section": "R programming",
    "text": "R programming\n\nLecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_6.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_6.html",
    "title": "2.6 About Probability",
    "section": "",
    "text": "Note: You may temporarily ignore this part and come back to read after the lecture 3.\nAs a programming language originally designed for statisticians, functions related to probability distribution are essential. For example, how to generate random numbers from a distribution, how to calculate the probability of a random event, how to find the corresponding quantile values based on a given probability, and how to calculate the density function value of a given distribution are all well implemented in R. Below, we will introduce 4 functions using the normal (Gaussian) distribution as an example.\n\n2.6.1 Generate Random Numbers\nGenerating random numbers is essential for simulations, statistical modeling, and resampling techniques like bootstrapping, where random data or sampling is needed to test models, explore scenarios, or understand variability.\nIn R, the rnorm function is applied to generate random numbers from a normal (Gaussian) distribution. The syntax is\n# Syntax of fucntion 'rnorm'\nrnorm(SampleSize, mean = 0, sd = 1)\nFor example,\nx = rnorm(100)\nhist(x) \nRemark: We can’t simulate real random numbers on computer, but only generate pseudo-random numbers through algorithms designed to produce sequences that mimic the properties of randomness. These algorithms start with an initial value known as the random seed, which serves as the starting point for generating the sequence. By changing the seed, we can create different sequences of pseudo-random numbers, allowing for reproducibility in simulations and analyses. While these numbers may appear random, they are ultimately determined by the algorithm and the initial seed value. In R, the random seed can be controled by function ‘set.seed’, for example\n# Next, if you get the same value for 'a' and 'b', I will pay you 1000kr\n(a = rnorm(1))\n(b = rnorm(1)) \n\n# Next, if you get different values for 'a' and 'b', I will pay you 1000kr \nset.seed(2024)\n(a = rnorm(1))\nset.seed(2024)\n(b = rnorm(1))\nLife is seems like a tapestry woven with random numbers, however, to some extent it is not really random, but pseudo-random. On the one hand, it is a box of chocolates - every moment is a surprise, sweet or bitter, unfolding in unpredictable ways. You never know what the next piece will bring. However, on the other hand, we seem to be unable to escape the arrangement of fate. When you were born, God, like a careful gardener, had selected a unique seed for you. This seed contains the potential of your existence and shapes your journey. No matter what your current situation is, we should always cherish our unique seed.\n\n\n2.6.2 Find the Density Values\nThe density value is useful not only in probability but also statistics, because it presents an important quantity, likelihood value. It will be discussed in the next lecture in details.\nIn R, the dnorm function calculates the density (or height) of the normal distribution at a specific value. The syntax is\n# Syntax of function 'dnorm'\ndnorm(x, mean = 0, sd = 1)\nFor example\n# The outputs of the following two lines should be equal. Why?\ndnorm(0)\n1/sqrt(2*pi)\n\n\n2.6.3 Calculate the Probability\nIn R, the pnorm function calculates the probability of an event of a normal distribution. The syntax is\n# Syntax of function 'pnorm'\npnorm(a, mean = 0, sd = 1)\nThis function calculate the probability \\(\\Pr(X&lt;a)\\) which is the area under the normal density curve within the interval \\([-\\infty, a]\\), for example,\n\npnorm(1)\n\n[1] 0.8413447\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.4 Determine the quantile value\nQuantile value is very essential in statistics, for example, you all need quantile value whether you are conducting hypothesis testing or calculating confidence intervals. Actually, it is just an inverse operation of calculating probability, e.g. the quantile value of \\(0.84\\) for a standard normal distribution is approximately \\(1\\) The syntax is similar, for example, you should be familiar with the following quantile value\nqnorm(0.975, 0, 1) # what is it?\n\n\n2.6.5 Summary and Remark\n# Functions related to normal distribution.\nrnorm() # generate random numbers from normal distribution\ndnorm() # find the density value of normal distribution\npnorm() # calculate the probability of an event associated to normal distribution\nqnorm() # determine the quantile value of normal distribution\nRemark: As you can see, there is a pattern in the function names, i.e. a letter + norm, for example in ‘rnorm’,\n\nr: This prefix indicates that the function generates random numbers (random variates) from a specific distribution.\nnorm: This part of the name refers to the normal distribution, also known as the Gaussian distribution.\n\nThis naming convention is applied to other distributions as well. For example:\n\nrbinom: generates random numbers from a binomial distribution\ndexp: finds density value of an exponential distribution\nppois: calculates probability of a Poisson distribution\nqunif: determines the quantile value of a uniform distribution\n\n\nPrevious page | Lecture 2 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s_2.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_s_2.html",
    "title": "Geometry of Linear Classifiers",
    "section": "",
    "text": "Note: From now, we will temporarily ignore the bias term, \\(w_0\\), or assume it as \\(0\\). It will not influence our final conclusion. No worries. So, the basic classifier is represented as \\(y = \\text{Sign}(\\textbf{w}^{\\top}\\textbf{x})\\)\nPreviously, we explored the geometric understanding of linear classifiers, which is that the classifier determines a linear decision boundary. Next, let’s understand a linear classifier from another view of geometry. Suppose we have a classifier with two feature variables, \\(x_1\\) and \\(x_2\\), and the “reasonable” weight vector is \\(\\textbf{w} = (0.6, 0.8)^{\\top}\\). Look at the conceptual plot below.\n\n\n\n\n\n\n\n\n\nIt is easy to see that all the vectors (points) in blue form a sharp angle with the weights vector (black). By the property of inner product, (read about inner product) for any point \\(\\textcolor{blue}{\\textbf{x}} = (\\textcolor{blue}{x_1},\\textcolor{blue}{x_2})^{\\top}\\) standing on the direction pointed by the blue arrow, \\(\\textbf{w}^{\\top}\\textcolor{blue}{\\textbf{x}} \\propto \\cos(\\alpha) &gt; 0\\), i.e. all the cases on this direction will be classify as positive. On the contrary, all the vectors (points) in blue form a obtuse angle with the weights vector, and then \\(\\textbf{w}^{\\top}\\textcolor{red}{\\textbf{x}} \\propto \\cos(\\beta) &lt; 0\\), i.e. all the points standing on the direction pointed by a red vector will be classified as negative. With this observation, we can easily understand how does a “reasonable” linear classifier work.\nBased on this principle, let’s have look at a concrete example in the figure below.\n\n\n\nIn a binary classification problem, we have two feature variables, each with 10 cases, blue for positive and red for negative. We have a “reasonable” weight vector \\(\\textbf{w}\\) (orange arrow), which determines a linear decision boundary (purple line). \\(\\textbf{w}\\) is “reasonable” because it has an angle less than 90 degrees with all positive vectors, but an angle greater than 90 degrees with all negative vectors. Of course, a more direct understanding is that this linear classification boundary divides the entire feature space into two parts, with all positive cases at the bottom and all negative cases at the top. However, the first explation is more useful for understanding the proceptron algorithm.\n\n\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_s_1.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_s_1.html",
    "title": "History",
    "section": "",
    "text": "Proceptron classifier is viewed as the foundation and building block of artificial neural network. For a historical introduction, see the figures below.\n\n\n\nThe Perceptron classifier was developed by Frank Rosenblatt in 1957 (LHS). Rosenblatt’s goal was to create a machine that could classify visual patterns, such as distinguishing between different shapes. At the time, he envisioned using large computers to simulate neural networks, inspired by how the brain processes information. His early experiments involved using a huge computer called the Mark I Perceptron (RHS), which attempted to recognize different shapes by adjusting weights based on input data. This work laid the foundation for modern neural networks and machine learning, despite initial limitations in its capacity to handle complex, non-linear problems.\n\n\nSuppose we are solving a binary classification problem with \\(p\\) feature variables. As discussed before, it can be represented as\n\\[\n  \\hat{y} = \\text{Sign}( \\textbf{w}^{\\top} \\textbf{x} + w_0  )\n\\]\nwhere \\(\\textbf{w} = (w_1, w_2, \\dots, w_p)^{\\top}\\), and \\(\\textbf{x} = (x_1, x_2, \\dots, x_p)^{\\top}\\). Different from before, here we represent the weighted sum of \\(p\\) feature variables, \\(w_1x_1+ \\dots + w_px_p\\), as the inner (dot) product of two vectors, i.e. \\(\\textbf{w} \\cdot \\textbf{x} = \\textbf{w}^{\\top} \\textbf{x}\\).\n\nNote: In order to understand proceptron algorithm, we need some basic knowledge about vector and its operations. If you are not familiar with it or need to refresh it, read about vector, operators, inner product before start reading the next.\n\nThe perceptron algorithm is about finding a set of reasonable weights. The key term here, “reasonable,” is easy to understand—it refers to a set of weights that can deliver good predictive performance. The core issue is how to find them.\n\nBrute-force idea: try all possible weight values and record the corresponding model performance, such as accuracy, and then choose the weights that yield the highest accuracy as the final model parameters.\n\nHowever, this idea is clearly not ideal. Even if we only have two feature variables, this would still not be a simple task. A smarter approach is to do it this way: we start with an initial guess for the weight values, and then gradually approach the most reasonable weights through some iterative mechanism. This mechanism is called the perceptron algorithm. Next, let’s dive into learning this magical mechanism—the perceptron algorithm.\nNext, the logic goes like this:\n\nFurther explore the geometric properties of linear classifiers\nUse geometry to understand how this algorithm works.\n\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1.html",
    "href": "Courses/c_mlwr1_2024/l1/l1.html",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "",
    "text": "In this lecture, we introduce machine learning to you. You will learn the basic elements in this field and an old, basic, but very interesting algorithm in machine learning.\nNext, I’ll begin with a review of key milestones in artificial intelligence. Then, we’ll delve into some metaphysical concepts, exploring the underlying logic of machine learning. By understanding the human learning process, we’ll gain insight into the entire machine learning process. Finally, after covering the ABCs of machine learning, we’ll focus on the fundamental forms of machine learning models."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1.html#mailstone-of-ai-alphago-2016",
    "href": "Courses/c_mlwr1_2024/l1/l1.html#mailstone-of-ai-alphago-2016",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "1.1 Mailstone of AI, AlphaGo, 2016",
    "text": "1.1 Mailstone of AI, AlphaGo, 2016\nMachine learning is not a new concept, but the latest version of the machine learning legend has indeed just happened recently. Have you heard about AlphaGo? AlphaGo is an artificial intelligence program developed by DeepMind that made history in 2016 by defeating a world champion Go player.\n\n\n\n\nFigure 1: LHS: Go is an ancient board game originating from China over 2,500 years ago, in which two players compete to capture territory using black and white stones on a grid. The game’s simple rules allow for deep strategic complexity, making it one of the most intellectually challenging games in the world. RHS: AlphaGo is an artificial intelligence program developed by DeepMind that became the first to defeat a professional human player, and eventually world champions, in the complex board game Go, showcasing a major milestone in AI’s capabilities in strategic thinking and decision-making. Source: Google search.\n\n\n\nBack in the year 2000, the computer program Deep Blue played to a draw against the world chess champion Garry Kasparov. Under this milestone, human couldn’t even fathom how to defeat top Go players on a Go board, and some believed it to be an impossible task forever. Indeed, the \\(19 \\times 19\\) Go board has 361 intersections, and the number of possible combinations is astronomical, making it a task that even the most powerful computers couldn’t handle. Furthermore, unlike chess, every Go piece (stone) has equal values and there is no difference between them, which makes it very difficult to evaluate the situation on the board and make decisions accordingly.\n\n\n\n\nFigure 2: Unlike in chess, each piece in Go starts with the same value, but its worth on the board isn’t fixed; it changes continuously with the evolving situation. This makes assessing the situation extraordinarily difficult, let alone writing a program to evaluate it. This resonates with life—though, unlike Go pieces, we have the power to determine our own path, at least to some extent.\n\n\n\nHowever, in just 16 years, this last bastion of human intelligence was breached by computer programs. In 2016, a computer program, AlphaGo, defeated top Go players from South Korea for the first time in a Go competition. In one year, the new version AlphaGo, Go Master, defeated the current world No. 1 ranking player from China. After that, the strongest version of AlphaZero could give three handicaps to top professional Go players. I consider this event to be a significant milestone in the history of AI. Among the many technologies behind AlphaGo, machine learning played a significant role. Following that, various applications of machine learning blossomed, and people started applying this computer technology to a wide range of fields.\n\n\n\n\nFigure 3: Machine learning applications: 1) AlphaGo, 2) Fraud detection, 3) Application in finance, 4) Pseudo CT image, 5) Face detection, 6) Spam filter, 7) Breast cancer automatical detection based on medical image.\n\n\n\nFor example, people use machine learning to generate CT scan images with potential side effects from harmless MRI scans; Machine learning trains programs to assist doctors in extracting essential information from medical images; More powerful spam filter is also trained by machine learning algorithm; When you take photos with your cellphone, automatic portrait recognition and focus are also achievements of machine learning; Up until now, you can communicate with ChatGPT seamlessly and obtain reliable information, all thanks to the dividends from machine learning."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1.html#philosophy-of-machine-learning",
    "href": "Courses/c_mlwr1_2024/l1/l1.html#philosophy-of-machine-learning",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "1.2 Philosophy of Machine Learning",
    "text": "1.2 Philosophy of Machine Learning\nLet’s first discuss some metaphysical matters. What is the philosophy behind machine learning? Let’s begin with the story about handwritten digit data. In the past, postal workers had to manually sort all mails according to handwritten post code. Aside from the high labor costs, we all would consider this an extremely tedious job with a high error rate. So, people wondered if they could scan handwritten post codes into computer and then let the computer recognize the digits. This is how handwritten digit data came into existence.\nAnother story is about medical imaging. In recent years, hospitals have introduced MRI technology for medical imaging. MRI excels in presenting soft tissue, and its use of magnetic fields results in minimal harm to the body. In contrast, traditional CT imaging relies on X-rays, which can have noticeable side effects on the human body. However, CT imaging is irreplaceable when it comes to displaying the solid tissues, like skeletal structure. Therefore, people have contemplated whether it’s possible to generate corresponding CT images from MRI image data, giving rise to the concept of generative CT images.\nThe two examples have a common feature, that is we aim to predict ‘expensive’ information using ‘cheap’ information. In the story of post office, the “cheap” information is the easily obtained image data, while the recognition of the postal code is considered “expensive” information. In the medical imaging story, the acquisition of MRI image data carries far less risk compared to the risks associated with CT images. From this perspective, MRI data is indeed much more cost-effective than CT data. Therefore, the basic idea of machine learning is to train a “machine” to transform “cheap” information into “expensive” information through data. In this way, expensive information is replaced by cheap information through machine learning models, thus avoiding high costs, unnecessary error costs, and additional risks.\n\n\n\n\nFigure 4: Philosophy of machine learning. Obviously, water cannot be turned into oil—this is simply a metaphor. A successful project requires researchers to maintain an honest attitude; unrealistic ideas, like a washing machine, will only produce waste water."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1.html#machine-learning-process",
    "href": "Courses/c_mlwr1_2024/l1/l1.html#machine-learning-process",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "1.3 Machine Learning Process",
    "text": "1.3 Machine Learning Process\nWhat is the process of machine learning like? First, let me tell you about some observations I have made about my sons. After they learned to speak, they began asking me all sorts of questions. For example, when we were at the supermarket, he would point at apples and ask me, ‘What’s this, Daddy?’ I just simply answered them, “It is an apple.” After a few times, they would change their questioning style from special to general, like, ’Daddy, is this an apple? In about half months, they turned into high-precision apple classifiers. They could even recognize that the logo on my laptop is also an apple! Amazing! I must emphasize that I never taught them how to recognize apples.\n\n\n\n\nFigure 5: I have two boys at home. On the LHS, the boy wearing his pants frontside back is my elder son, Siyi, when he was three years old. He was earnestly planting flowers in the artificial soccer field. On the RHS, the guy who resembles a sloth is my younger son, Siqi. It is quite evident that he is a happy fellow. Actually, he is very quiet and cool.\n\n\n\nWe can summarize the human learning process from the example of my sons learning to recognize apples. First, they would accumulate experience through observation and questioning. Once they had enough experience, they would begin their own learning and distill this into “knowledge.” Subsequently, they would use general questions to validate their knowledge. Finally, they would use their validated knowledge to identify the Apple logo. This human learning process is summarized in the following figure (up).\nIn fact, if we just change the names of the components, this is also the process of machine learning. For computer programs, “experience” is essentially “data”, “learning” involves “training” with algorithms, for example proceptron algorithm, and the “knowledge” distilled is a type of “model”. We call the “self-exam process” as “validation” and “applying” it to new problems as “generalization”. The entire machine learning process is presented in the following figure (down). In this course, we focus on “training” and “validation” steps. For “training” step, we introduce several basic and fundamental algorithms for linear models and discuss several validation methods for “validation” step. See Figure below.\n\n\n\n\nFigure 6: The human learning process (up) V.S. machine learning process (down)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1.html#machine-learning-abc",
    "href": "Courses/c_mlwr1_2024/l1/l1.html#machine-learning-abc",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "1.4 Machine Learning ABC",
    "text": "1.4 Machine Learning ABC\nIn machine learning, variables are often split into feature variables and target variables. Feature variables are the inputs to the model—information that helps the model make predictions—while target variables are the outcomes or labels the model is intended to predict. For instance, in the case of handwritten digit recognition, each pixel value in the image of a digit acts as a feature, providing the model with clues about the visual patterns, while the digit number itself (such as “5” or “9”) is the target variable. Similarly, in medical imaging applications like pseudo-CT imaging, the pixel values from an MRI image may serve as features, and the corresponding CT image’s pixel values become the target, as the model aims to predict CT values based on MRI inputs. There are many ways to categorize machine learning, with the most common being supervised and unsupervised learning. This categorization primarily depends on whether target variables are included in the research problem.\nIn supervised learning, a dataset includes known labels for each observations, which the model uses to learn relationships between features and targets. In a mathematical language, we try to find a map \\(f\\), or a function, that connect the features information and target information. For example, the Iris dataset is labeled with flower species (such as Setosa or Versicolor) based on measurements like petal length and width, which act as feature variables. The goal is for the model to learn these relationships so it can classify new, unseen examples accurately. This mapping, \\(f\\), also known as the model, has its functionality determined by model parameters, which are adjusted based on the data. The process of determining the “optimal” parameters is also called to ‘learning’. Once the optimal model parameters are set, the model is considered trained. In the case of the Iris example, for those who often confuse the three subspecies, the shape data of the flower can be used to predict the species. Many plant identification Apps work in this way. Supervised learning is often further divided into regression and classification problems, depending on the type of target variable. We will focus on this distinction in the following section.\nIn unsupervised learning, on the other hand, the dataset has no target variables, and the model’s task is to find underlying patterns or groupings in the data, such as clustering the Iris dataset’s measurements into groups without knowing the species in advance. In mathematical language, in unsupervised learning problem, we also want to learn a map \\(g\\) that connect feature variables and some “new” knowledge. In statistics, we often use the term “latent variable” or “latent information” to represent this “new” knowledge. The beautiful names Setosa, Versicolor, and Virginica did not exist before botanists classified and named them scientifically. This new knowledge emerged from analyzing data on the shapes of the flowers. In machine learning, we typically encounter two types of unsupervised learning problems: feature analysis and cluster analysis. In the first part of this course, we will not cover these topics.\n\nQuiz: In fact, we have encountered similar unsupervised learning problems in basic statistics. Do you know what they are?\n\n\n\n\n\nFigure 7: Feature variables V.S. Target Variable; Supervised Learning V.S. Unsupervised Learning"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1.html#regression-model-and-classification-model",
    "href": "Courses/c_mlwr1_2024/l1/l1.html#regression-model-and-classification-model",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "1.5 Regression Model and Classification Model",
    "text": "1.5 Regression Model and Classification Model\n\nRegression Model\nThe machine learning problem can be understood as regression problem when the target variable is a continuous variable. For example, predict the house price based on different feature variables; predict the pixel values of CT scans based on MRI scans; predict the stock price based on feature variables of market. A simple scenario displayed in the figure below, a basic regression model is a linear model, \\(y_i = w_0 + w_1x_i + \\epsilon_i\\). From a geometric perspective, a linear regression model can be seen as a straight line that passes through all sample observations. In the generalization stage, the target value can be predicted from feature variable through the regression model.\n\n\n\n\n\nFigure 8: Regression Problem\n\n\n\n\n\n\nClassification Model\nThe problem can be viewed as a classification problem when the target variable is categorical. We often refer to this type of target variable as labels. For example, in the classification with Iris data, the species variable is the label variable, and we aim for finding a good “function” taking 4 shaping variables as input to predict the labels based on data. This function is often refer to a classifier. So, what kind of function can perform this role? Let’s take a look at a real classifier first, a “coin sorter.” Its operation is quite simple, as it classifies coins based on their different diameters. Inside the machine, there are holes of varying sizes corresponding to different diameters, and through vibration, coins will fall into the holes that match their size. In essence, it’s classifying by comparing a variable to a threshold value. The idea is quite simple, but it is just the essential idea of machine learning classifier.\n\n\n\n\nFigure 9: LHS: Classification with iris data. RHS: A real classifier, coin sorter. The working principle: Variable (diameter) V.S. Threshold value.\n\n\n\nWell, usually we have multiple feature variables in a classification problem, then how do we apply this simple working principle to design a classifier? Let’s see another example. You might not know yet, in fact, teacher becomes a classifier after an exam. Well, to pass or not to pass is a classification problem. Suppose, in a secret exam, each student answers 5 questions and each question is worth 20 points. Student passes the exam if the total points are larger or equal to 60. I have corrected all the exams; the results are summarized in Table 1, and \\(1\\) indicating the question was correctly answered and \\(0\\) indicating not. Then, who can pass the exam?\n\n\n\n\n\nI believe it is a very simple problem, for example, Super girl correctly answered 4 questions and get 80 points that is above the threshold value 60, so she passed the exam! However, spiderman only got 20 points that is lower than 60, so he can’t pass. If we clearly write down the calculation process, we actually used the following formula to calculate the totol score, then compare the total score with the critical point, 60.\n\\[\n  20\\times Q_1 + 20\\times Q_2 + 20\\times Q_3 + 20\\times Q_4 + 20\\times Q_5 \\geq 60\n\\]\n\nNow, we know what a simple classifier looks like. Essentially, it is a two-step procedure. We create a single variable through the weighted sum of all feature variables first, then compare the resulting value with a threshold value. In formal, the classifier can be represented as\n\\[\n  y = \\text{Sign}(w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_p x_p)\n\\]\nwhere \\(\\text{Sign}(x)\\) is a sign function returning 1 if \\(x&gt;0\\) and 0 if \\(x&lt;0\\). We refer coefficients \\(w_1, \\dots, w_p\\) as weights, the weighted sum of feature variables \\(w_1 x_1 + w_2 x_2 + \\dots + w_p x_p\\) as scores and \\(w_0\\), the threshold value, as bias. If the score value is equal to 0, then this observation can’t be classified by this classifier, and the thing we can do best is flip a coin to make the decision. We call all the points that satisfy equation \\(w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_p x_p = 0\\) as the decision boundary. For example, in a 2D feature space, the decision boundary \\(w_0 + w_1x_1 + w_2x_2 =0\\) is just a straight line with a slope of \\(-w_1/w_2\\) and an intercept of \\(-w_0/w_2\\), see the figure below.\n\n\n\n\nFigure 10: In this example, the 2D feature space is cut into two parts by the decision boundary (red line). For any unlabeled observations (blue dots), if it is above the decision boundary, then it will be classified as yellow , otherwise, green.\n\n\n\nThis kind of classifier is called linear classifier, since the decision boundary is presented by a linear function. It is a straight line in 2D space, a plane in 3D space, and hyper-plane in a higher dimension space. You might have already realized that in fact, a classifier is solely determined by its weights and bias, and machine learning algorithms tell us how to find the optimal weights and bias through data. There are several classical methods (algorithms) for learning a linear classifier which are perceptron algorithm, linear discriminant analysis, logistic regression, and maximum margin classifier. In this course, we will introduce all of them except maximum margin classifier.\nRemark: Just as all the rules of arithmetic start with \\(1+1\\), don’t underestimate this linear classifier. You will see that all complex classifiers are built upon them. For example, maximum margin classifier is the foundation of SVM (Support vector machine) which dominate machine learning world for 20 years, the perceptron algorithm is the starting point of artificial neural net works, and no matter how complex a neural network architecture may be, as long as it is a classifier, its final layer will inevitably be a logistic regression model.\n\nLecture 1 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_2.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_2.html",
    "title": "2. Philosophy of Machine Learning",
    "section": "",
    "text": "Let’s first discuss some metaphysical matters. What is the philosophy behind machine learning? Let’s begin with the story about handwritten digit data. In the past, postal workers had to manually sort all mails according to handwritten post code. Aside from the high labor costs, we all would consider this an extremely tedious job with a high error rate. So, people wondered if they could scan handwritten post codes into computer and then let the computer recognize the digits. This is how handwritten digit data came into existence.\nAnother story is about medical imaging. In recent years, hospitals have introduced MRI technology for medical imaging. MRI excels in presenting soft tissue, and its use of magnetic fields results in minimal harm to the body. In contrast, traditional CT imaging relies on X-rays, which can have noticeable side effects on the human body. However, CT imaging is irreplaceable when it comes to displaying the solid tissues, like skeletal structure. Therefore, people have contemplated whether it’s possible to generate corresponding CT images from MRI image data, giving rise to the concept of generative CT images.\nThe two examples have a common feature, that is we aim to predict ‘expensive’ information using ‘cheap’ information. In the story of post office, the “cheap” information is the easily obtained image data, while the recognition of the postal code is considered “expensive” information. In the medical imaging story, the acquisition of MRI image data carries far less risk compared to the risks associated with CT images. From this perspective, MRI data is indeed much more cost-effective than CT data. Therefore, the basic idea of machine learning is to train a “machine” to transform “cheap” information into “expensive” information through data. In this way, expensive information is replaced by cheap information through machine learning models, thus avoiding high costs, unnecessary error costs, and additional risks.\n\n\n\n\nFigure 4: Philosophy of machine learning. Obviously, water cannot be turned into oil—this is simply a metaphor. A successful project requires researchers to maintain an honest attitude; unrealistic ideas, like a washing machine, will only produce waste water.\n\n\n\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_1.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_1.html",
    "title": "1. Mailstone of AI, AlphaGo, 2016",
    "section": "",
    "text": "Machine learning is not a new concept, but the latest version of the machine learning legend has indeed just happened recently. Have you heard about AlphaGo? AlphaGo is an artificial intelligence program developed by DeepMind that made history in 2016 by defeating a world champion Go player.\n\n\n\n\nFigure 1: LHS: Go is an ancient board game originating from China over 2,500 years ago, in which two players compete to capture territory using black and white stones on a grid. The game’s simple rules allow for deep strategic complexity, making it one of the most intellectually challenging games in the world. RHS: AlphaGo is an artificial intelligence program developed by DeepMind that became the first to defeat a professional human player, and eventually world champions, in the complex board game Go, showcasing a major milestone in AI’s capabilities in strategic thinking and decision-making. Source: Google search.\n\n\n\nBack in the year 2000, the computer program Deep Blue played to a draw against the world chess champion Garry Kasparov. Under this milestone, human couldn’t even fathom how to defeat top Go players on a Go board, and some believed it to be an impossible task forever. Indeed, the \\(19 \\times 19\\) Go board has 361 intersections, and the number of possible combinations is astronomical, making it a task that even the most powerful computers couldn’t handle. Furthermore, unlike chess, every Go piece (stone) has equal values and there is no difference between them, which makes it very difficult to evaluate the situation on the board and make decisions accordingly.\n\n\n\n\nFigure 2: Unlike in chess, each piece in Go starts with the same value, but its worth on the board isn’t fixed; it changes continuously with the evolving situation. This makes assessing the situation extraordinarily difficult, let alone writing a program to evaluate it. This resonates with life—though, unlike Go pieces, we have the power to determine our own path, at least to some extent.\n\n\n\nHowever, in just 16 years, this last bastion of human intelligence was breached by computer programs. In 2016, a computer program, AlphaGo, defeated top Go players from South Korea for the first time in a Go competition. In one year, the new version AlphaGo, Go Master, defeated the current world No. 1 ranking player from China. After that, the strongest version of AlphaZero could give three handicaps to top professional Go players. I consider this event to be a significant milestone in the history of AI. Among the many technologies behind AlphaGo, machine learning played a significant role. Following that, various applications of machine learning blossomed, and people started applying this computer technology to a wide range of fields.\n\n\n\n\nFigure 3: Machine learning applications: 1) AlphaGo, 2) Fraud detection, 3) Application in finance, 4) Pseudo CT image, 5) Face detection, 6) Spam filter, 7) Breast cancer automatical detection based on medical image.\n\n\n\nFor example, people use machine learning to generate CT scan images with potential side effects from harmless MRI scans; Machine learning trains programs to assist doctors in extracting essential information from medical images; More powerful spam filter is also trained by machine learning algorithm; When you take photos with your cellphone, automatic portrait recognition and focus are also achievements of machine learning; Up until now, you can communicate with ChatGPT seamlessly and obtain reliable information, all thanks to the dividends from machine learning.\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_4.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_4.html",
    "title": "4. Machine Learning ABC",
    "section": "",
    "text": "In machine learning, variables are often split into feature variables and target variables. Feature variables are the inputs to the model—information that helps the model make predictions—while target variables are the outcomes or labels the model is intended to predict. For instance, in the case of handwritten digit recognition, each pixel value in the image of a digit acts as a feature, providing the model with clues about the visual patterns, while the digit number itself (such as “5” or “9”) is the target variable. Similarly, in medical imaging applications like pseudo-CT imaging, the pixel values from an MRI image may serve as features, and the corresponding CT image’s pixel values become the target, as the model aims to predict CT values based on MRI inputs. There are many ways to categorize machine learning, with the most common being supervised and unsupervised learning. This categorization primarily depends on whether target variables are included in the research problem.\nIn supervised learning, a dataset includes known labels for each observations, which the model uses to learn relationships between features and targets. In a mathematical language, we try to find a map \\(f\\), or a function, that connect the features information and target information. For example, the Iris dataset is labeled with flower species (such as Setosa or Versicolor) based on measurements like petal length and width, which act as feature variables. The goal is for the model to learn these relationships so it can classify new, unseen examples accurately. This mapping, \\(f\\), also known as the model, has its functionality determined by model parameters, which are adjusted based on the data. The process of determining the “optimal” parameters is also called to ‘learning’. Once the optimal model parameters are set, the model is considered trained. In the case of the Iris example, for those who often confuse the three subspecies, the shape data of the flower can be used to predict the species. Many plant identification Apps work in this way. Supervised learning is often further divided into regression and classification problems, depending on the type of target variable. We will focus on this distinction in the following section.\nIn unsupervised learning, on the other hand, the dataset has no target variables, and the model’s task is to find underlying patterns or groupings in the data, such as clustering the Iris dataset’s measurements into groups without knowing the species in advance. In mathematical language, in unsupervised learning problem, we also want to learn a map \\(g\\) that connect feature variables and some “new” knowledge. In statistics, we often use the term “latent variable” or “latent information” to represent this “new” knowledge. The beautiful names Setosa, Versicolor, and Virginica did not exist before botanists classified and named them scientifically. This new knowledge emerged from analyzing data on the shapes of the flowers. In machine learning, we typically encounter two types of unsupervised learning problems: feature analysis and cluster analysis. In the first part of this course, we will not cover these topics.\n\nQuiz: In fact, we have encountered similar unsupervised learning problems in basic statistics. Do you know what they are?\n\n\n\n\n\nFigure 7: Feature variables V.S. Target Variable; Supervised Learning V.S. Unsupervised Learning\n\n\n\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_home.html",
    "href": "Courses/c_mlwr1_2024/l1/l1_home.html",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "",
    "text": "In this lecture, we introduce machine learning and related basic concepts. It covers the following things:"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_home.html#lecture-notes",
    "href": "Courses/c_mlwr1_2024/l1/l1_home.html#lecture-notes",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "Lecture notes:",
    "text": "Lecture notes:\n\nIntroduction to Machine Learning:\n\nRead the integrated notes: here;\nRead the paginated notes: here;\nDownload the PDF notes: here.\n\nProceptron and Its Algorithm (optional):\n\nRead the integrated notes: here;\nRead the paginated notes: here;\nDownload the PDF notes: here."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1_home.html#reading-guidelines-of-textbook",
    "href": "Courses/c_mlwr1_2024/l1/l1_home.html#reading-guidelines-of-textbook",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "Reading Guidelines of textbook:",
    "text": "Reading Guidelines of textbook:\nFor Lecture 1, it is recommended that you read the following sections in the textbook.\n\nChapter 1: Read pages 1 - 14. Extra attention to ‘Notation and Simple Matrix Algebra’ if necessary.\nChapter 2: Read sections 2.1, pages 15 - 28.\n\n\nCourse Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_home.html",
    "href": "Courses/c_mlwr1_2024/l6/l6_home.html",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "",
    "text": "In this lecture, we complete the discussion of overfitting probelm first, and then discuss the main issue model selection. In the end, we discuss a special topic in machine learning, feature selection. It covers the following things:"
  },
  {
    "objectID": "03_s_index.html",
    "href": "03_s_index.html",
    "title": "Skalds",
    "section": "",
    "text": "Skalds were poets in Norse and Viking Age societies, known for composing and reciting poetry that celebrated heroes, gods, and important events. They played a significant role in preserving history and cultural traditions through oral storytelling, often in the courts of kings and chieftains across Scandinavia and Iceland. I don’t have the ability to create immortal poetry, but I would like to share some of my own short writings here. Of course, if I am diligent enough. Source: google search.\n\n\n\n\nList of my short notes\n\nCracking Cipher, an example of problem solving by data\nMath, the thing in a rational world"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_home.html#lecture-notes",
    "href": "Courses/c_mlwr1_2024/l3/l3_home.html#lecture-notes",
    "title": "Lecture 3: Basic Knowledge in Probability Theory",
    "section": "Lecture notes:",
    "text": "Lecture notes:\n\nRead the integrated notes: here\nRead the pagination notes: here\nDownload the PDF notes here."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_home.html#reading-guidelines-of-textbook",
    "href": "Courses/c_mlwr1_2024/l3/l3_home.html#reading-guidelines-of-textbook",
    "title": "Lecture 3: Basic Knowledge in Probability Theory",
    "section": "Reading Guidelines of textbook:",
    "text": "Reading Guidelines of textbook:"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_home.html#prepare-before-the-qa-session",
    "href": "Courses/c_mlwr1_2024/l3/l3_home.html#prepare-before-the-qa-session",
    "title": "Lecture 3: Basic Knowledge in Probability Theory",
    "section": "Prepare before the Q&A session:",
    "text": "Prepare before the Q&A session:\n\nRead and study the materials of lecture 3 as much as you can\nYou may have some questions while reading. If so, feel free to ask me. If you prefer, you can also email me or send a message via Canvas to inquire."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_home.html#lesson",
    "href": "Courses/c_mlwr1_2024/l3/l3_home.html#lesson",
    "title": "Lecture 3: Basic Knowledge in Probability Theory",
    "section": "Lesson:",
    "text": "Lesson:\nLesson Entrance\nSolutions\n\nCourse Homepage"
  },
  {
    "objectID": "MathToolBox/la/la_03.html#linearly-independent",
    "href": "MathToolBox/la/la_03.html#linearly-independent",
    "title": "Linear Combination",
    "section": "",
    "text": "Next, I will show you one simple counter-example. Considering two vectors pointing in the same direction, we know that one vector can be represented as another vector scaled by some scalar. Therefore, the linear combination of the two vectors is equivalent to a scalar multiplication of arbitrary one vector. In this case, the resulting vector only stays in one direction or presents a line but not the 2D plane. Thus, we can’t create arbitrary vectors by the two vectors through linear combinations. It is also the case when the two vectors point in opposite directions.\n\n\n\n\n\nIn simple words, for two overlapping vectors, we can only create a new vector in the direction of two vectors. In a mathematical language, we can use a linear combination of two vectors with non-zero coefficients to get the zero vector. In this case, we say the two vectors are linearly dependent.\n\n\n\n\n\nOppositely, for two non-overlapping vectors, we can’t use a linear combination of them to create the zero vector unless the coefficients all are zeros, and we say the two vectors are linearly independent.\n\n\n\n\n\nThis definition can be extended to a more general scenario and leads to the general definition of linear dependence.\n\n\n\n\n\n\nFor 2D space, we can maximumly have two linearly independent vectors. For k-D space, we can maximumly have k linearly independent vectors. Is that true?\n\nGreat! It is a good time to introduce the next concept, basis, which is not only important in linear algebra but also in data analysis.\n\nPrevious page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_07.html#orthogonal",
    "href": "MathToolBox/la/la_07.html#orthogonal",
    "title": "Orthogonal and Projection",
    "section": "",
    "text": "In linear algebra, if the angle between two vectors is 90 degrees, then we say they are orthogonal.\nRecall your knowledge in high school, the cosine value of 90 degrees is 0. This fact provides a simple method to determine whether two vectors are orthogonal, i.e. two vectors are orthogonal if and only if their inner product is zero.\n\n\n\n\n\nOrthogonal is an important concept in many ways. In statistics, there is a simple and good example. From the previous section, we learned that the correlation between two variables is just the cosine value of the angle between two variables. So if two variables are orthogonal, then they are also uncorrelated. Another reason is that the concept of orthogonal leads to the next important concept, projection."
  },
  {
    "objectID": "MathToolBox/la/la_07.html#projection",
    "href": "MathToolBox/la/la_07.html#projection",
    "title": "Orthogonal and Projection",
    "section": "Projection",
    "text": "Projection\nA projection can be viewed as a vector created by two vectors through the following procedure. Suppose we have two non-overlapping vectors \\(\\textbf{x}\\) and \\(\\textbf{y}\\), a beam of light is incident from a direction perpendicular to \\(\\textbf{y}\\) cause a shadow of \\(\\textbf{x}\\) on \\(\\textbf{y}\\) and the shadow is called the projection of \\(\\textbf{x}\\) onto \\(\\textbf{y}\\).\n\n\n\n\n\nOne interesting problem is how to represent the shadow \\(\\textbf{x}_1\\) by the two existing vectors. You may have realized that \\(\\textbf{x}_1\\) overlaps with vector \\(\\textbf{y}\\), therefore \\(\\textbf{x}_1\\) should be scaled \\(\\textbf{y}\\), i.e. \\(\\textbf{x}_1 = k \\cdot \\textbf{y}\\). So the problem becomes finding the coefficient \\(k\\). Based on the orthogonal properties, we can find that\n\\[\n  k = \\frac{\\textbf{x}^{\\top} \\textbf{y}}{\\textbf{y}^{\\top} \\textbf{y}} \\text{, and }  \\textbf{x}_1 = \\frac{\\textbf{x}^{\\top} \\textbf{y}}{\\textbf{y}^{\\top} \\textbf{y}} \\textbf{y}.\n\\] through the following derivations.\n\n\n\n\n\nRemark: The scalar \\(k\\) is just the length of projection and it is called scalar projection. One can go further and find that the scalar projection is obtained by the inner product of \\(\\textbf{x}\\) and the unit vector in \\(\\textbf{y}\\) direction."
  },
  {
    "objectID": "MathToolBox/la/la_07.html#orthonormal-basis",
    "href": "MathToolBox/la/la_07.html#orthonormal-basis",
    "title": "Orthogonal and Projection",
    "section": "Orthonormal Basis",
    "text": "Orthonormal Basis\nIn 2D space, there is a pair of two special and nice unit vectors, \\((1,0)^{\\top}\\) and \\((0,1)^{\\top}\\). One can easily get the scalar projection of arbitrary vectors on them, isn’t it? Obviously, they also form a basis of 2D space. This basis is very nice since they are not only unit vectors but also orthogonal to each other. We call this kind of basis an orthonormal basis.\n\nPrevious page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_09.html#matrix-at-first-sight",
    "href": "MathToolBox/la/la_09.html#matrix-at-first-sight",
    "title": "Matrix, its true color",
    "section": "",
    "text": "The most straightforward definition of a matrix is the rectangle array. One needs to use two numbers to describe the size (shape) of a matrix, the numbers of rows and columns. In the following example, the matrix consists of 3 rows and 4 columns. We say it is a 3 by 4 matrix. One neat but informative notation is \\(\\textbf{A} = \\left\\{ a_{ij} \\right\\}_{3 \\times 4}\\). In this course, keep in mind, we always use \\(i\\) indicates for the row index and \\(j\\) for column index.\n\n\n\n\n\nYou may remember the idea in the previous section that a matrix can be viewed as a “row vector” of column vectors. In this example, suppose we can find 3 linearly independent vectors from the 4 column vectors, then they can be viewed as a basis and generate a space. Similarly, the matrix also can be viewed as a “column vector” of row vectors, and they also can generate a space if they are linearly independent. In statistics, especially MDA, we mainly work with a data matrix. Let’s talk about it later on."
  },
  {
    "objectID": "MathToolBox/la/la_09.html#matrix-true-colors",
    "href": "MathToolBox/la/la_09.html#matrix-true-colors",
    "title": "Matrix, its true color",
    "section": "Matrix, true colors",
    "text": "Matrix, true colors\nThe definition of the matrix above is very straightforward and very helpful when we understand a data matrix in MDA. However, the disadvantage of this definition is that it is too dry, and we cannot understand it from a functional point of view as well as a geometric point of view. Let’s take a look at what its true color. Let’s turn back to the concept of ‘linear combination’ and watch the following animation.\n\n\n\n\n\nFrom this animation, we can see that the \\(n \\times p\\) matrix \\(\\textbf{A}\\) determines a map (function) from the \\(\\mathbb{R}^p\\) space to the \\(\\mathbb{R}^n\\) space. We call this map or function a linear transformation. In some sense, the matrix can be viewed as a bridge between two spaces.\nRemark: we also want to emphasize that the meaning of an action that pre-multiply a vector by a matrix, \\(\\textbf{Ax}\\), even though we have not defined matrix multiplication yet. Keep in mind that this action means that one uses the elements of \\(\\textbf{x}\\) as coefficients to calculate the linear combination of column vectors of the matrix \\(\\textbf{A}\\). It is very helpful when we discuss the next important concept, the rank of a matrix.\n\nPrevious page | LA4DS Homepage | Next page"
  },
  {
    "objectID": "MathToolBox/la/la_09.html#at-first-sight",
    "href": "MathToolBox/la/la_09.html#at-first-sight",
    "title": "Matrix, its true color",
    "section": "",
    "text": "The most straightforward definition of a matrix is the rectangle array. One needs to use two numbers to describe the size (shape) of a matrix, the numbers of rows and columns. In the following example, the matrix consists of 3 rows and 4 columns. We say it is a 3 by 4 matrix. One neat but informative notation is \\(\\textbf{A} = \\left\\{ a_{ij} \\right\\}_{3 \\times 4}\\). In this course, keep in mind, we always use \\(i\\) indicates for the row index and \\(j\\) for column index.\n\n\n\n\n\nYou may remember the idea in the previous section that a matrix can be viewed as a “row vector” of column vectors. In this example, suppose we can find 3 linearly independent vectors from the 4 column vectors, then they can be viewed as a basis and generate a space. Similarly, the matrix also can be viewed as a “column vector” of row vectors, and they also can generate a space if they are linearly independent. In statistics, especially MDA, we mainly work with a data matrix. Let’s talk about it later on."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#preface",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#preface",
    "title": "Lecture 3: Probability Theory",
    "section": "3.1 Preface",
    "text": "3.1 Preface"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#from-frequence-to-probability",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#from-frequence-to-probability",
    "title": "Lecture 3: Basic Knowledge in Probability Theory",
    "section": "3.1 From Frequence to Probability",
    "text": "3.1 From Frequence to Probability\nEarly on people realized that there are many things in the real world where the outcome is uncertain, such as one doesn’t always get a head when flipping a coin, or doesn’t always get a certain number when throwing a dice, and so they refer to this kind of event as random event. At the same time, mathematicians also had found that the percentage of getting heads always seems to be close to \\(0.5\\) when they repeatedly flip an even coin. The following r codes simulate this process:\n\n# randomly generate 5000 numbers taking values 0 or 1 and save in x\nset.seed(8312)\nx = rbinom(1000,1,0.5)\n# calculate the cumulated ratio\nratio = cumsum(x)/(1:1000)\nplot(ratio, type = \"l\")\nabline(h = 0.5, col = \"red\")\n\n\n\n\nThe plot depicts a phenomenon that as the number of coin tosses increases, the ratio gets closer and closer to 0.5. The x-axis is the number of coin flipping, and the y-axis is the ratio of the number of heads and the number of coin flipping. BTW, this is a neat solution to one of esercises in Lab 1.\n\n\n\n\n0.5, that is 1/2, has different meanings for its numerator and denominator; the denominator is the number of all possible outcomes (Head or Tail), while the numerator is the number of outcomes included in this event. Thus, mathematicians defined the first probability model in which the possibility of a random event occurring can be quantified by the ratio of the number of outcomes associated with the event to the number of all possible ones. Meantime, we refer to this measure of possibility as the probability of an event.\n\\[\n\\Pr \\left( \\text{Event} \\right) = \\frac{\\text{number of outcomes associated with the event}}{\\text{total number of all possible outcomes}}\n\\] In the coin example, all possible outcomes are \\(H\\) and \\(T\\), and the outcome associated with the event, getting a head, is \\(H\\). So, the probability that get a head when flip a coin is \\[\n  \\Pr(\\text{Get a head when flip a coin}) = \\frac{1}{2}\n\\] Since the number of outcomes associated with the event is always less than the total number of all possible outcomes, the probability defined by this model is always a number between 0 and 1, which fits our intuitions.\nBased on this model, one can easily quantify the possibility of an event if one gets the number 3 when throwing a dice as \\(1/6\\). Before providing more examples, I have to emphasize that probability is a mathematical concept that only exists in our rational world. That is, you can only get a number which is 3 or not 3 after throwing a dice. Also, even if you throw a die over and over again for the rest of your life, the number of times you get a 3 as a percentage of your total throws will only be very close to \\(1/6\\).\n( NE ) Let’s explore some more complex examples. First, what is the probability that you get a number less than 5 by throwing a dice? Using the model above, the total number of outcomes of throwing a dice is 6 and there are 4 potential outcomes all satisfy this condition, therefore this probability is \\(2/3\\). Second, what is the probability that you get 2 heads after flipping a coin 6 times? Again, the total number of possible outcomes is \\(2^6\\) and the number of outcomes satisfying this condition is \\(15\\), therefore the probability of this event is \\(0.234375\\). Obviously, the second example is harder than all the examples before, and you probably need some knowledge of permutations and combinations to understand the meaning of numbers \\(2^6\\) and \\(15\\). However, this is not the main purpose here. Also, calculating such numbers is not essential to understanding the probability model. Of course, if you’re interested in these types of questions, consider solving some problems from a book on probability theory, which is a good mental exercise. For example, you might think carefully about why a full house can beat a flush in Poker? (ToDo4Sia: Write a note discussing how you teach your son about permutations and combinations.)\n\n\n\nA full house can beat a flush in Poker, why?"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#random-variable-and-distribution",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#random-variable-and-distribution",
    "title": "Lecture 3: Basic Knowledge in Probability Theory",
    "section": "3.2 Random Variable and Distribution",
    "text": "3.2 Random Variable and Distribution\nDiscussing the probability notion for events can often be cumbersome. For example, in the coin example, we have to write texts to represent it. To simplify the expression, we can use a capital letter \\(X\\) to represent the result of flipping a coin. It has two possible values, \\(0\\) and \\(1\\) which indicate getting a Tail and a Head respectively. Then the probability of that event can be represented as \\[\n  \\Pr(X = 1)\n\\] This letter \\(X\\) is so powerful that it encompasses not only the two possible outcomes of one flipping but also represents all potential results each time you flip this coin. We call this letter \\(X\\) as a random variable which can be understood as a description of the results of a certain experiment. Conventionally, it is usually presented by a uppercase letter and we use lowercase letter, e.g. \\(x\\), to represent its realization or observation.\nNote: the mathematical definition of a random variable goes beyond this and is much more profound; however, this understanding of random variables is sufficient for practical applications.\nIf the role of a random variable were merely to use a symbol to represent an experimental outcome, it wouldn’t be nearly as compelling. Its essence lies in its distribution. Through the distribution of a random variable, we can abstract the common features of a wide range of random events. For example, consider \\(X\\) in the context of a coin toss. If we specify that \\(X\\) takes the value 1 with a fixed probability \\(\\pi\\) and 0 with a probability of \\(1 - \\pi\\), we obtain a random variable with a specific distribution, commonly referred to as a binary distribution or Bernoulli distribution random variable. It can be denoted as \\(X \\sim \\text{Ber}(\\pi)\\), and its distribution can be simply described as the evaluation of the probability for each possible outcome, i.e.\n\n\n\n\n\nThis information of distribution also can be represented by a formula,\n\\[\n  \\Pr(X = k) = \\pi^k(1-\\pi)^{1-k}\n\\] where the possible value of \\(k\\) is 0 and 1, and \\(0&lt;\\pi&lt;1\\). It is called probability mass function (p.m.f).\nIn this way, the symbol \\(X\\) becomes much more powerful. The Random variable and its distribution can help us get away from the whole coin-flip thing. The symbol \\(X\\) is elevated, and it becomes a probabilistic model. It can not only represent the random experiment of flipping a coin but also depict the probability of randomly selecting a man in Umeå city center, or describe the incidence of a certain disease within a specific population over a certain time period in a given region.\nAnother example: One also can define a random variable \\(Y\\) denotes the result of throwing a dice, then it has 6 possible values, \\(1\\) to \\(6\\). Since the probabilities of getting all possible values are equal, \\(1/6\\), it is called the discrete uniform distribution. ( PA ) The  p.m.f  is \\[\n  \\Pr(Y = k) = 1/6\n\\] where \\(k = 1,2,\\dots,6\\). There aren’t necessarily only six possible outcomes; you can increase the number of possible values, allowing this probabilistic model to cover a broader range of random phenomena.\nMore possibilities, for example, one can use \\(Z\\) to denote a random variable that presents the number of accidents in a certain time period, for example, the number of traffic accidents in certain area in one month. In this case, the possible values should be \\(1,2,3,4,\\dots\\). From this phenomenon, we can abstract the Poisson distribution which models the number of times an event occurs in a fixed interval of time or space, under the conditions. The  p.m.f  is leave to you to explore.\nRandom variable with certain distribution also can help us simplify the calculation of the probability of an event. Let’s see the next example, Binomial distribution, denoted as \\(X \\sim \\text{Bin}(N,p)\\). The random variable \\(X\\) presents the number of positive results among \\(N\\) independent binary results experiment. The probability that getting a positive result in one experiment is \\(p\\). Obviously, the possible values of \\(X\\) are integers from \\(0\\) to \\(N\\), and the distribution can be represented as \\[\n  \\Pr(X = k) = \\frac{N!}{k!(N-k)!} p^{k}(1-p)^{N-k}\n\\] where the exclamation sign denotes factorial, i.e. \\(N!=N(N-1)(N-2)\\dots1\\). It is easy to see the probability of the relatively complicated random event discussed before, you get \\(2\\) heads after flipping a coin \\(6\\) times, which can be calculated by this distribution. We can define a random variables the number of getting heads after flipping a coin \\(6\\) times, so \\(X \\sim \\text{Bin}(6,0.5)\\). Replacing \\(N=6\\), \\(p=0.5\\), and \\(k=2\\) in the formula above, one can easily verify \\(\\Pr(X=2) = 0.234375\\)."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#expected-value-and-variance",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#expected-value-and-variance",
    "title": "Lecture 3: Probability Theory and Statistics",
    "section": "3.3 Expected Value and Variance",
    "text": "3.3 Expected Value and Variance\n\n3.3.1 Expected Value\nWe have introduced the most basic elements of probability theory, namely random variables and their distributions. Now, I have a question to you. Suppose we have two binomial distributed random variables,  \\(X_1 \\sim \\text{Bin}(10, 0.1)\\)  and  \\(X_2 \\sim \\text{Bin}(3, 0.7)\\) , can we compare them? (You can hover your cursor on \\(X_1\\) and \\(X_2\\) to get the meaning of them.) Well, the two random variables have different potential outcome, for \\(X_1\\), you might get a integer from \\(0\\) to \\(10\\), but only three possible values, \\(0, 1, 2, 3\\) for \\(X_2\\). Apparently, it is not comparable. Then can we compare their observed values? It doesn’t seem that simple neither. One is an experiment with a low success rate conducted 10 times, while the other has a high success rate but we only conduct it 3 times, so it’s hard to say which will have a higher number of successes. Let’s explore the answer in a straightforward way by actually generating two random numbers from their respective distributions and comparing them.\nX1 = rbinom(1, prob = 0.1, size = 10)\nX2 = rbinom(1, prob = 0.7, size = 3)\nX2&gt;X1\nIf you run the code above multiple times, you’ll get both TRUE and FALSE, but you may feel that the realization of \\(X_2\\) is often higher than the realization of \\(X_1\\). To verify our intuition, we can repeatedly run the code above and record the result each time. By doing this 1000 times, we can see the percentage of cases where the value of \\(X_2\\) is greater than the value of \\(X_1\\).\n\nres = numeric(1000)\nfor(i in 1:1000){\n  X1 = rbinom(1, prob = 0.1, size = 10)\n  X2 = rbinom(1, prob = 0.7, size = 3)\n  res[i] = ifelse(X2&gt;X1,1,0)\n}\nprint(paste0(\"The proportion of times X2&gt;X1 is \", sum(res)/10,\"%\"))\n\n[1] \"The proportion of times X2&gt;X1 is 71.1%\"\n\n\nIndeed, we often observe that the value of \\(X_2\\) is greater than \\(X_1\\). However, is there a way to reach a conclusion directly without relying on experiments? Or perhaps, can we explain this phenomenon using mathematical language? We then need to introduce another important concept, the expected value of a random variable. The expected value of a random variable is very similar to the concept that you are very familiar with, that is average value. If I ask you how often do you go to IKSU per week? Then most likely, you will answer me that you go to IKSU, on average, 3 times per week, since the number of visiting IKSU (the largest sports center in Umeå) per week is not a certain number (you often go there 3 times, but not always, for example, you are sick or have an important exam to prepare). Let me show you the number of my IKSU visits in the last 10 weeks. \\[\n  3,5,3,2,2,4,5,3,3,4\n\\]\nWe all know that the average value can be \\[\n  \\frac{3+5+3+2+2+4+5+3+3+4}{10} = 3.4\n\\] Of course, it is a super easy calculation, but let’s have a close look at it. This calculation can be represented as \\[\n  \\frac{2 \\times 2 + 4\\times3 + 2\\times4 +2\\times 5 }{10} = 0.2 \\times 2 + 0.4 \\times 3 + 0.2 \\times 4 + 0.2 \\times 5 = 3.4\n\\]\nNotice that the decimal in front of each integer, the possible value, is the percentage of the corresponding value that happened in the last ten weeks. In the rational world, if you still remember it, the percentage is replaced by the probability. Therefore, the definition of expected value is defined as the weighted sum of all possible values, and the weights are the corresponding probabilities. In a mathematical notation, the expected value of a random variable is presented as \\[\n  \\text{E}(X) = \\sum_{k} k \\Pr (X = k)\n\\] We can see that the expected value of a binary distributed random variable and a binomial distributed random variable is \\(p\\) and \\(Np\\) respectively. It is a good exercise to verify it. Now, we can turn back to the question at the beginning. By simple calculation, we can see that \\[\n  \\text{E}(X_2) = 3\\times0.7 = 2.1 &gt; \\text{E}(X_1) = 10\\times0.1 = 1\n\\] The expected value satisfies linearity. Suppose \\(a\\) and \\(b\\) are constant numbers and \\(X\\) is a random variable, then \\(\\text{E}(aX+b) = a \\text{E}(X) + b\\). In other words, linearity means the expectation operator and the linear operator (scalar multiplication and addition) are exchangeable, i.e. \\[\n  \\text{E} \\left(\\sum_{i=1}^n a_iX_i\\right)  = \\sum_{i=1}^n  a_i\\text{E}(X_i)\n\\]\n\n\n3.3.2 Variance\nThe expected value can help us determine the size of the “common” value of a random variable so that we can compare two random variables. One can also compare two random variables from another dimension, which is “value stability”. For example, we have two coins, one is even and the other is so uneven that there is a very high probability, 90%, of getting Heads. Then imagine that if we flip two coins repeatedly, we will get many heads for the uneven coin and occasionally get Tails; but for the even coin, we will get the same number of heads and tails with high probability. From the perspective of taking values, the values of uneven coins are very stable, while those of uniform coins are not. The stability of a random also refers to variation. High variation means low stability. The two things can be quantified by the variance.\nThe variance of a random variable \\(X\\) is defined as \\[\n  \\text{Var}(X) = \\text{E}(X - \\text{E}(X))^2\n\\]\nThis formula is very intuitive. First, we calculate the most frequently occurring value of this random variable, \\(\\text{E}(X)\\), and then compare it with any \\(X\\) by taking the difference. Finally, we calculate the average of the squares of this difference. If this random variable has stable values, then the value of \\(X\\) should often stay around the expected value, therefore \\(X - \\text{E}(X)\\) will generally be very small; conversely, if it varies significantly, it will be generally large, which aligns perfectly with our initial intention.\nBased on this definition, one can easily verify the variance of a binary distributed random variable and a binomial distributed random variable is \\(p(1-p)\\) and \\(np(1-p)\\) respectively. In the example above, the variance of the even coin is \\(0.25\\), but the uneven is \\(0.09\\).\nDifferent from the expected value, variance doesn’t satisfy the linearity, i.e. the variance operator and the linear operator are not exchangeable. However, it satisfies the following rules, \\[\n  \\text{Var}(aX+b) = a^2\\text{Var}(X)  \n\\]\nBased on the results above, we can easily find that a special linear combination, \\(\\left( X-\\text{E}(X) \\right)/\\sqrt{\\text{Var}(X)}\\), produces a standardized random variable, i.e. mean zero and variance 1."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#two-dependent-variables",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#two-dependent-variables",
    "title": "Lecture 3: Basic Probability Knowledge",
    "section": "3.4 Two Dependent Variables",
    "text": "3.4 Two Dependent Variables\n\n3.4.1 Sum rule and product rule\nLet’s think about a question like this. The basic conditions are presented in the following picture. Suppose we want to randomly pick a box and then randomly take out a fruit from it, then what is the probability that the fruit taken out is an apple? We can randomly pick the box by throwing a dice. If we get a number less than 5 then we choose the red box, or we choose the blue box.\n\n\n\nBox and Furits Problem: there are two boxes, red and blue, and the red box contains two apples and six oranges, while the blue box contains three apples and one orange.\n\n\nThis example is a bit more complicated than the previous because there are two randomized actions involved here. The result of this action involves a combination of the color of the box and the type of fruits. Let’s start by considering the probability that the red box is drawn while an apple is picked up. Again, we can use the previous formula to calculate this probability. There are six numbers corresponding to the dice, and the number of fruits inside the red box is 8, so all the possibilities are \\(6 \\times 8\\). But there are only numbers 1 through 4 and two apples, so there are only \\(4 \\times 2\\) possibilities that qualify. So the probability is \\[\n  \\frac{4\\times2}{6\\times8} = \\frac{4}{6} \\times \\frac{2}{8} = \\frac{1}{6}\n\\] It is easy to see that \\(4/6\\) is the probability of getting a number less than \\(5\\), i.e. the red box is selected. But what is the meaning of the second part, \\(2/8\\)? Since \\(8\\) is the number of fruits and \\(2\\) is the number of apples in the red box, it can be understood as the probability of getting an apple when the red box was selected. We refer to this probability as conditional probability and present it as \\(\\Pr(\\text{Apple} | \\text{Red}) = 2/8\\). This discussion can be summarized as \\[\n  \\Pr ( \\text{Red AND Apple} ) = \\Pr( \\text{Apple} | \\text{Red} ) \\Pr ( \\text{Red} )\n\\] We can also easily get the probability of getting an apple under the other possibility, i.e. \\[\n  \\Pr(\\text{Blue AND Apple}) = \\Pr(\\text{Apple} | \\text{Blue} ) \\Pr( \\text{Blue} ) = \\frac{3}{4} \\times \\frac{2}{6} = \\frac{1}{4}\n\\] “AND” corresponds to the “product rule”. \\[\n  \\Pr(E_1 \\text{ AND } E_2) = \\Pr(E_1 | E_2)\\Pr(E_2) = \\Pr(E_2 | E_1)\\Pr(E_1)\n\\] Going back to our original question, what is the probability of getting an apple? This random event can be labeled as “Red AND Apple OR Blue AND Apple”. Here, we have the second rule, i.e. “sum rule”, when considering the “OR” operator between two events that don’t happen simultaneously. \\[\n  \\Pr(E_1 \\text{ OR } E_2) = \\Pr(E_1) + \\Pr(E_2)\n\\]\nBased on the sum rule, the probability of getting an apple is calculated as \\[\n  \\Pr( \\text{Apple} ) = \\Pr( \\text{Red AND Apple} ) + \\Pr( \\text{Blue AND Apple} ) = \\frac{5}{12}\n\\]\nRemark: we can compare it with the sum-product rule in permutation and combinatorics. When there are different types of solutions for one thing, the total number of possible solutions is the sum of the number of possible solutions for each type. When there are different steps in doing one thing, the total number of solutions is the product of the number of possible solutions in each step.\n\n\n3.4.2 Joint distribution and marginal distribution\nSimilarly, you can verify the probabilities when orange is considered. If we use random variables to present the events, for example, the random variable \\(X\\) presents the box selected, \\(1\\) indicates red, and \\(0\\) indicates blue; the random variable \\(Y\\) presents the fruit drew, then the following table is called the joint distribution of two random variables.\n\n\n\n\n\nThe last column is called the marginal distribution of random variable \\(Y\\), and the last row is the marginal distribution of random variable \\(X\\).\n\n\n3.4.3 Posterior and Prior Probabilities\nWith the example above, the last interesting question is “What is the probability we chose the blue box if we get an orange?” First of all, it is a conditional probability, \\(\\Pr(X = 0 | Y = 0)\\). According to the product rule, we know that this conditional probability is the ratio between \\(\\Pr(X=0 \\text{ and } Y=0)\\) and \\(\\Pr(Y=0)\\). The first second has been calculated before and that is \\(7/12\\). Well, the first can be calculated by product rule again, i.e. \\(\\Pr( Y = 0 | X = 0 ) \\Pr(X=0)\\). Summarize, \\[\n  \\Pr(X = 0 | Y = 0) = \\frac{\\Pr( Y = 0 | X = 0 ) \\Pr(X=0)}{\\Pr(Y=0)}\n\\]\n\\(\\Pr(X=0|Y=0)\\) is different from the conditional probability in the first question \\(\\Pr(Y=0|X=0)\\). This probability is referred to as the posterior probability in the sense that we use the observation in the second step to update the probability of the first step. Correspondingly, the probability of drawing a blue box at the first step is called prior probability. The formula above is well known as Bayes formula.\n\n\n3.4.4 Statistically independent\nFrom the joint distribution, the probability of drawing an apple \\(\\Pr(Y = 1)\\) is \\(5/12\\). It is different from the probability of drawing an apple under the condition that the red box was selected, \\(\\Pr(Y=1 | X = 1) = 2/8\\). This fact implies that the value of random variable \\(Y\\) depends on the value of \\(X\\), or random variable \\(X\\) and \\(Y\\) are dependent. If we add 1 apple and 11 oranges to the blue box, then \\(\\Pr(Y=1) = \\Pr(Y=1 | X = 1)\\). In this case, the value of random variable \\(Y\\) doesn’t depend on the value of \\(X\\), i.e. they are independent.\n\n\n3.4.5 Covariance\nFor two random variables \\(X\\) and \\(Y\\), we can use covariance to quantify the degress of association between two random variables. The covariance is defined as \\[\n  \\text{Cov}(X, Y) = \\text{E}(X-\\text{E}(X))(Y-\\text{E}(Y))\n\\] The mean and variance of a weighted sum (linear combination) of two random variables are \\[\n  \\text{E}(aX+bY) = a\\text{E}(X) + b\\text{E}(Y)\n\\] and \\[\n  \\text{Var}(aX+bY) = a^2\\text{Var}(X) + 2ab\\text{Cov}(X,Y) + b^2\\text{Var}(Y)\n\\] respectively."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#continuous-variables",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#continuous-variables",
    "title": "Lecture 3: Probability Theory and Statistics",
    "section": "3.5 Continuous Variables",
    "text": "3.5 Continuous Variables\nNext, we will consider a more challenging concept, continuous random variable. As you may notice, all the random events that we are considering can be represented by categorical outcomes. For example, flipping a coin has two outcomes, throwing a dice has 6 outcomes, and so on. For this kind of random event, it is sufficient to consider random variables only taking integer values. Thus we refer to the random variables discussed as discrete random variables. In practice, however, there are many other random events whose outcomes are not categorical but continuous values, for example, the temperature, the height of adult males, and so on. Therefore we need another type of random variable, a continuous variable.\n\n3.5.1 Continuous distribution\nContinuous random variables are not difficult to understand, they are nothing more than random variables that take real numbers, but the problem is how to describe their distribution. Again, let us abstract mathematical concepts from reality. Let’s consider such a background problem, assuming that I have height data for all boys in middle school in Sweden. Height is obviously not a categorical variable, but we can still use grouping to describe the distribution of height from a discrete perspective. Specifically, we can evenly divide the possible range of height into several groups, and then calculate the percentage of the number of people in each group to the total number of people. Yes, if you are familiar with basic statistics, you can tell that this is a histogram at a glance.\n Such an approach has obvious flaws. For example, on the left-top of the plot, it is difficult for us to distinguish the probability of height being less than 175 and greater than 170 because these two values are combined into one group. How to do it? Very simple, we can split each large group into two small groups, of course, you also need to include more boys into the data set and then calculate the frequency of each group to represent the distribution of height, for example, on the right-top of the plot. If we still cannot distinguish the above probability, we can continue to split each group into two groups. Doing this we can see that the histogram is more detailed. If we have a sufficient large data set, we can continue to subdivide the height group and know the probability that we can distinguish the above two events. Suppose we put “all” the boys in middle school into this histogram, and each group can be subdivided infinitely. We can imagine that the upper edge of the histogram will be a smooth curve. We call this smooth curve the probability density function (p.d.f) and denote it as \\(f(x)\\). A valid  p.d.f  has to inherit two conditions from the  p.m.f  of a discrete random. First, the density value must be positive, \\(f(x) &gt; 0\\), and the integral on the whole domain should be 1, \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\). With this function, we can calculate the probability of many events, as well as the expectation and variance. \\[\n  \\Pr(X&lt;b) = \\int_{-\\infty}^bf(x)dx\n\\] \\[\n  \\text{E}(X) =  \\int_{-\\infty}^{\\infty} xf(x) dx\n\\]\nOne can compare the formula above with the expected value of a discrete random variable, \\(\\text{E}(X) = \\sum_k k\\Pr(x=k)\\). You can see similar patterns, they all are the “sum” of all possible values times the corresponding probability or density values. Keep in mind that the integral symbol is an elongated S which indicates “sum”.\nSo far, we have only specified the basic conditions for a function to be a p.d.f , but the exact form of this function depends on the continuous distribution it represents. Next, we will learn about one of the most common continuous distributions: the Normal distribution.\n\n\n3.5.2 Normal (Gaussian) distribution\nA continuous random variable is Normally distributed, \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), if its density function is\n\\[\n  f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} } e^{- \\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma}\\right)^2 }\n\\] The normal distribution is determined by two parameters, location parameter \\(\\mu\\) and shape parameter \\(\\sigma^2\\). Density functions of normal distribution with different parameters are displayed in the following picture.\n\n\n\n\n\nLHS: Normal distribution with fixed shape parameter (sigma = 1) and different location parameters, orange: mu = -4, blue: mu = 0, red: mu = 2. RHS: Normal distribution with fixed location parameter (mu = 0) and different shape parameters, orange: sigma = 3, blue: sigma = 1, red: sigma = 0.5.\n\n\n\n\nNow, I have two questions to you.\n\nWe have studied both discrete random variable and continuous random variable, their distribution of possible values can be presented by p.m.f and p.d.f respectively. The value of a p.m.f is just the probability when the random variable taking this value. However, what is the meaning of the value of a p.d.f?\nWhat is the essence of the Normal distribution? In simpler terms, how would you introduce the concept of the Normal distribution to a middle school student?\n\nThink about them and we will come back to them later on."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#likelihood",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#likelihood",
    "title": "Lecture 3: Probability Theory and Statistics",
    "section": "3.6 Likelihood",
    "text": "3.6 Likelihood\nLikelihood analysis is an important concept in the modern statistics. A good understanding of likelihood can help you improve your knowledge in statistics. By understanding the relationship between likelihood and probability, you can build a bridge in statistical or data and probability modeling, allowing you to use models with greater confidence.\n\n3.6.1 Likelihood value V.S. Probability\nWe have build up all the basic concepts of probability. In simple terms, probability is a mathematical model used to quantify the possibility of an event. The discussion of the possibility of this event is limited to the realm of rationality. For example, we can use probability to discuss the following questions.\n“Imagine you are standing in the square in the city center of Umeå, close your eyes for 30 seconds, then open your eyes and catch the first man you see. How likely is his height above 180 cm?”\nLet’s have a look at another scenario,\n“You went to downtown last weekend. You stood in the square and closed your eyes, then after 30 seconds you opened your eyes and grabbed the man you saw first and measured his height. His height is 230 and you think it is unbelievable.”\nIn this case, we have a real observation of a random variable and want to evaluate the possibility of this observation. In this case, a proper word is likelihood value.\nI hope from the two examples above, you can see the difference between the two synonyms for “possibility.” In one word, likelihood value is the evaluation of the possibility to one observed valued, but probability evaluate the possibility of an event (somehow have not happened). Now, we can answer the first question above, what is the meaning of the value of a p.d.f? It present the likelihood of a real observation. For example, we may assume the height of adult men in Sweden is Normally distributed with mean 170 and sd 5.5 (I got these statistics from SCB, Statistiska centralbyrån), then the likelihood values of observing a man who is 179 and a men who is 230 are\n\ndnorm(179,179,5.5) \n\n[1] 0.07253496\n\ndnorm(230,179,5.5)\n\n[1] 1.546941e-20\n\n\nSo, it is almost impossible to see a men who is 230 in the center of Umeå city.\nRemark 1: Unlike probability, likelihood is not standardized and is not a decimal between 0 and 1. When the standard deviation is very small, the likelihood value near the mean can be quite large. For example,\n\ndnorm(0, mean = 0, sd = 0.001)\n\n[1] 398.9423\n\n\nRemark 2 (HBG2K) : When people use the likelihood value, they often take the logarithm of it. One reason is that, even when the likelihood value is very small, we can still have a meaningful number for calculations. Another reason is that the distributions we commonly use, like the normal distribution, belong to the exponential family, so taking the logarithm transforms a nonlinear function into a linear one. For example,\n\ndnorm(230, 179, 5.5)\n\n[1] 1.546941e-20\n\nlog(dnorm(230, 179, 5.5))\n\n[1] -45.61542\n\n\nNext, we summarize the facts about discrete random variable and continuous random variable in the following table.\n\n\n\n\n\n\n\n3.6.2 The secrete message delivered by Normal distribution\nYou surely remember the previous question: what does the p.d.f of the normal distribution represent?\n\\[\n  f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} } e^{- \\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma}\\right)^2 }\n\\]\nOr, to put it another way, just as the binary distribution aims to describe random events like coin tossing, what kind of random phenomenon is the normal distribution trying to describe?\nLet’s start with en elementary understanding.\n\nThe normal distribution describes a bell-shaped, symmetric distribution of random phenomena, such as human IQ. Most people’s IQs are close to the mean, a smaller portion have higher or lower IQs, and only a very few have extremely high or low IQs. This forms a bell-shaped, symmetric distribution.\nWe can use more precise statistical terminology, such as confidence intervals, to describe this bell-shaped symmetric distribution of random phenomena. The normal distribution describes a random phenomenon where 68% of the probability is covered by the confidence interval of one sd around the mean, while the confidence interval of two sds can cover 95% of the probability, and the confidence interval of three sds can cover nearly all possibilities, as shown in the figure below.”\n\n\n\n\n\n\nIf you answer the question in such ways, then congratulations. You have rather good learning outputs from the previous basic statistics course. Next, let me introduce to you a deeper understanding based on the concept of likelihood. Let’s review the p.d.f of the normal distribution\n\n\n\nNote that the negative sign determines the inversely proportional relationship. The letter ( e ) represents the exponential function, which determines the rate of change.\n\n\nNotice that \\(\\left(\\frac{x-u}{\\sigma}\\right)^2\\) is the normalized distance between an observation \\(x\\) and the center point \\(\\mu\\). As mentioned before, the value of density function is the likelihood value of one observation, so the p.d.f is presenting the relationship between a specific observation \\(x\\) and its likelihood value. Then Normal distribution is describing such kind of random phenomenon:\n\nFor this kind of random phenomenon, the likelihood of an observation \\(x_0\\) is inversely proportion to the normalized distance between observed value \\(x_0\\) and the mean value \\(\\mu\\).\nMore precisely, when the observed value is far from the center point, the likelihood of observing it will decrease rapidly, and this decrease is exponential, as displayed below.\n\n\n\n\n\n\nIn summary, the normal distribution essentially describes the relationship between the distance of observed values from the center point and likelihood within a class of random phenomena. It connects the geometric concept of distance with the statistical concept of likelihood.\nThere are more stories about Normal (Gaussian) distribution. I will write a note about it. Please keep an eye on my space. ToDo4Sia: write a note telling the story of Gaussian distribution.\n\nLecture 3 Homepage"
  },
  {
    "objectID": "Skalds/20241101_Mathematics_a_Rational_World.html",
    "href": "Skalds/20241101_Mathematics_a_Rational_World.html",
    "title": "Mathematical, a Rational World",
    "section": "",
    "text": "In this short note, I want to make one point: all mathematical models exist only in our rational world and are abstractions of the real world. No mathematical model can perfectly replicate reality, but their advantage lies in helping us focus on the core issues when solving specific problems in real life by discarding irrelevant details. As the statistician George Box famously said, “All models are wrong, but some are useful.” This understanding is very beneficial; if you grasp it, it will help you elevate your thinking and eliminate confusion.\nLet’s start our discussion with a familiar mathematical concept. Circle is a fundamental concept in geometry, but where did it come from? The process of delineating the circle likely unfolds as follows: within the tangible world, a multitude of objects exhibit analogous characteristics in their forms. Take, for instance, the full moon, ripples in a lake, a wheel, and so forth. All of these things have a “full” shape, or their shapes appear to be very consistent from all directions. Consequently, individuals distilled this observation into a succinct principle, thereby defining the circle as follows:\nCircle is a collection of points that are equal distances to a fixed point.\nThis definition aptly encompasses the majority of objects with a circular shape. Nevertheless, it’s worth noting that once mathematicians have established the circle’s definition, the notion of a perfect circle, I mean mathematical circle, no longer finds a direct counterpart in the tangible (real) world. This is due to the challenge of locating a precise circle-like form within the physical realm. Even if one were to stumble upon an exceptionally round iron ring, ensuring that the distances from the ring’s center remain consistently uniform presents a challenge. In essence, the concept of a perfect circle, as defined in mathematics, resides solely within the confines of our abstract realm of reason, rather than within the tangible world.\n\n\n\nCircles in the real world\n\n\nWhile this might evoke a sense of melancholy, it underscores a fundamental connection between mathematical models and the tangible world. Mathematical models are, in essence, abstractions that represent real-world phenomena. Conversely, the real world can be approximated and understood through the lens of mathematical models.\nProbability theory and real-world data illustrate this relationship well. A probability model is essentially a mathematical construct that exists solely in our rational world, while data represent observations of the real world, vibrant and alive. The bridge between the two is precisely statistics. Clearly, statistics cannot exist without data, but it also relies on probability models to “tame” data, making it useful to us. Of course, data will never be fully tamed, just as if we were trying to grasp the mind of a higher power. As the saying goes, all models are wrong, but some are truly useful."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#tips-for-this-site-cont.-add",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#tips-for-this-site-cont.-add",
    "title": "Machine Learning with R, Part 1",
    "section": "Tips for this site (cont. add)",
    "text": "Tips for this site (cont. add)\n\nFor each lecture, I offer multiple ways to read. You can choose the integrated notes, allowing you to scroll up and down with ease, and use the side menu to jump between sections. However, if you find lengthy notes overwhelming, you might prefer the paginated version. It’s like how I divide a 2000-meter swim into four sessions—breaking it up makes it feel more achievable. Finally, I also provide a PDF version of the notes, making it convenient to print and read at your own pace.\nIn academia, people often use abbreviation, especially in technical writing. While this can be convenient for the author, it’s not always beginner-friendly. In my notes, I’ll do my best to implement a hover-over feature for annotating abbreviation. When you hover over the cursor on the abbreviation for a second, its full term will appear on the screen. For example: move your cursor on  IAT .  BTW , if you see  NE , it means “Not essential and you may skip”."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#tips-for-your-readings-cont.-add",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#tips-for-your-readings-cont.-add",
    "title": "Machine Learning with R, Part 1",
    "section": "Tips for your readings (cont. add)",
    "text": "Tips for your readings (cont. add)\n\nFor each lecture, I offer multiple ways to read. You can choose the integrated notes, allowing you to scroll up and down with ease, and use the side menu to jump between sections. However, if you find lengthy notes overwhelming, you might prefer the paginated version. It’s like how I divide a 2000-meter swim into four sessions—breaking it up makes it feel more achievable. Finally, I also provide a PDF version of the notes, making it convenient to print and read at your own pace.\nIn academia, people often use abbreviation, especially in technical writing. While this can be convenient for the author, it’s not always beginner-friendly. In my notes, I’ll do my best to implement a hover-over feature for annotating abbreviation. When you hover over the cursor on the abbreviation for a second, its full term will appear on the screen. For example: move your cursor on  IAT .  BTW , if you see  NE , it means “Not essential and you may skip”."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/mlwr1_index.html#tips-for-your-readings-ca",
    "href": "Courses/c_mlwr1_2024/mlwr1_index.html#tips-for-your-readings-ca",
    "title": "Machine Learning with R, Part 1",
    "section": "Tips for your readings ( CA )",
    "text": "Tips for your readings ( CA )\n\nFor each lecture, I offer multiple ways to read. You can choose the integrated notes, allowing you to scroll up and down with ease, and use the side menu to jump between sections. However, if you find lengthy notes overwhelming, you might prefer the paginated version. It’s like how I divide a 2000-meter swim into four sessions—breaking it up makes it feel more achievable. Finally, I also provide a PDF version of the notes, making it convenient to print and read at your own pace.\nIn academia, people often use abbreviation, especially in technical writing. While this can be convenient for the author, it’s not always beginner-friendly. In my notes, I’ll do my best to implement a hover-over feature for annotating abbreviation. When you hover over the cursor on the abbreviation for a second, its full term will appear on the screen. For example: move your cursor on  IAT .  BTW , if you see  NE , it means “Not essential and you may skip”."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_0.html",
    "href": "Courses/c_mlwr1_2024/l3/l3_0.html",
    "title": "Lecture 3: Basic Knowledge in Probability Theory",
    "section": "",
    "text": "Machine learning is essentially about finding methods for making decisions, and the best way to make decisions is based on assessing the probability (or likelihood) of potential outcomes. Therefore, probability theory has undoubtedly become a fundamental tool in this field.\nIn this lecture, we will cover the fundamental concepts of probability theory. To make this note self-contained, I will start from scratch. The benefit of this approach is that we can gradually build on each concept in a coherent manner. However, there may be some topics that we won’t use in this course, so I will list the most important aspects separately at the end. If you have already studied probability theory, you can treat this as a quick review. I will do my best to provide you with an intuitive picture of probability theory. If you feel this isn’t necessary, you can also skip ahead to the exercises.\nBefore we begin, I want to emphasize something important: probability models exist solely within our rational world. They are summaries of patterns in observational data, rather than replicas of real-world phenomena. No probability model can perfectly replicate reality, but it can help us solve real-world problems. As the statistician Box famously said, “All models are wrong, but some are useful.” I highly recommend you take a moment to read a mini note, it may open a door to a whole new perspective for you.\n(WBPA) The tools provided in this lesson are a type of soft tool. They may not help you directly with how to use the software, but they can offer you a way of thinking. A good understanding of them will make things easier for your studies not only in this course but also for all your data science knowledge.\nOutline:\n\n3.1 From Frequence to Probability\n3.2 Random Variable and Distribution\n3.3 Expected Value and Variance\n3.4 Two Dependent Variables\n3.5 Continuous Variables\n3.6 Likelihood Analysis\n\n\nLecture 3 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#likelihood-analysis",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#likelihood-analysis",
    "title": "Lecture 3: Basic Knowledge in Probability Theory",
    "section": "3.6 Likelihood Analysis",
    "text": "3.6 Likelihood Analysis\nLikelihood analysis is an important concept in the modern statistics. A good understanding of likelihood can help you improve your knowledge in statistics. By understanding the relationship between likelihood and probability, you can build a bridge in statistical or data and probability modeling, allowing you to use models with greater confidence.\n\n3.6.1 Likelihood value V.S. Probability\nWe have build up all the basic concepts of probability. In simple terms, probability is a mathematical model used to quantify the possibility of an event. The discussion of the possibility of this event is limited to the realm of rationality. For example, we can use probability to discuss the following questions.\n“Imagine you are standing in the square in the city center of Umeå, close your eyes for 30 seconds, then open your eyes and catch the first man you see. How likely is his height above 180 cm?”\nLet’s have a look at another scenario,\n“You went to downtown last weekend. You stood in the square and closed your eyes, then after 30 seconds you opened your eyes and grabbed the man you saw first and measured his height. His height is 230 and you think it is unbelievable.”\nIn this case, we have a real observation of a random variable and want to evaluate the possibility of this observation. In this case, a proper word is likelihood value. I hope from the two examples above, you can see the difference between the two synonyms for “possibility.” In one word, likelihood value is the evaluation of the possibility to one observed valued, but probability evaluate the possibility of an event (somehow have not happened). Now, we can answer the first question above, what is the meaning of the value of a p.d.f? It present the likelihood of a real observation. For example, we may assume the height of adult men in Sweden is Normally distributed with mean 170 and SD 5.5 (I got these statistics from SCB, Statistiska centralbyrån), then the likelihood values of observing a man who is 179 and a men who is 230 are\n\ndnorm(179,179,5.5) \n\n[1] 0.07253496\n\ndnorm(230,179,5.5)\n\n[1] 1.546941e-20\n\n\nSo, it is almost impossible to see a men who is 230 in the center of Umeå city.\nRemark 1: Unlike probability, likelihood is not standardized and is not a decimal between 0 and 1. When the standard deviation is very small, the likelihood value near the mean can be quite large. For example,\n\ndnorm(0, mean = 0, sd = 0.001)\n\n[1] 398.9423\n\n\nRemark 2 (HBG2K) : When people use the likelihood value, they often take the logarithm of it. One reason is that, even when the likelihood value is very small, we can still have a meaningful number for calculations. Another reason is that the distributions we commonly use, like the normal distribution, belong to the exponential family, so taking the logarithm transforms a nonlinear function into a linear one. For example,\n\ndnorm(230, 179, 5.5)\n\n[1] 1.546941e-20\n\nlog(dnorm(230, 179, 5.5))\n\n[1] -45.61542\n\n\nNext, we summarize the facts about discrete random variable and continuous random variable in the following table.\n\n\n\n\n\n\n\n3.6.2 The secrete message delivered by Normal distribution\nYou surely remember the previous question: what does the p.d.f of the normal distribution represent?\n\\[\n  f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} } e^{- \\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma}\\right)^2 }\n\\]\nOr, to put it another way, just as the binary distribution aims to describe random events like coin tossing, what kind of random phenomenon is the normal distribution trying to describe?\nLet’s start with en elementary understanding.\n\nThe normal distribution describes a bell-shaped, symmetric distribution of random phenomena, such as human IQ. Most people’s IQs are close to the mean, a smaller portion have higher or lower IQs, and only a very few have extremely high or low IQs. This forms a bell-shaped, symmetric distribution.\nWe can use more precise statistical terminology, such as confidence intervals, to describe this bell-shaped symmetric distribution of random phenomena. The normal distribution describes a random phenomenon where 68% of the probability is covered by the confidence interval of one SD around the mean, while the confidence interval of two SDs can cover 95% of the probability, and the confidence interval of three SDs can cover nearly all possibilities, as shown in the figure below.”\n\n\n\n\n\n\nIf you answer the question in such ways, then congratulations. You have rather good learning outputs from the previous basic statistics course. Next, let me introduce to you a deeper understanding based on the concept of likelihood. Let’s review the p.d.f of the normal distribution. Let’s watch an animation first.\n\n\n\nNote that the negative sign determines the inversely proportional relationship. The letter \\(e\\) represents the exponential function, which determines the rate of change.\n\n\nNotice that \\(\\left(\\frac{x-u}{\\sigma}\\right)^2\\) is the normalized distance between an observation \\(x\\) and the center point \\(\\mu\\). As mentioned before, the value of density function is the likelihood value of one observation, so the p.d.f is presenting the relationship between a specific observation \\(x\\) and its likelihood value. Then Normal distribution is describing such kind of random phenomenon:\n\nThe likelihood of an observation \\(x_0\\) is inversely proportion to the normalized distance between observed value \\(x_0\\) and the mean value \\(\\mu\\).\nMore precisely, when the observed value is far from the center point, the likelihood of observing it will decrease rapidly, and this decrease is exponential, as displayed below.\n\n\n\n\n\n\nIn summary, the normal distribution essentially describes the relationship between the distance of observed values from the center point and likelihood within a class of random phenomena. It connects the geometric concept of distance with the statistical concept of likelihood.\n\n\n3.6.3 Multivariate Gaussian Distribution\nWith the likelihood idea, we can easily extend the normal distribution to a multidimensional case. In this note, I will just list several points help you to understand the main idea.\nWhat is multidimensional case? For example, we may assume the height of Swedish adult men is normally distributed and denote as \\(X_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)\\). We also know that another important physical indicator is weight, which can also be assumed to follow a normal distribution, denoted as \\(X_2 \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)\\). If, for each Swedish adult man, we simultaneously consider height and weight as descriptive features rather than just one or the other, then we enter a multidimensional scenario. So, what changes do we need to make in a multidimensional scenario? With the height-weight example,\n\nWe need two values to determine the location of the center point of the distribution, or we need a mean vector \\((\\mu_1, \\mu_2)^{\\top}\\).\nIn the one dim case, we use variance/SD to control the shape of distribution. Naturally, we also need to consider the two variance simultaneously. However, this isn’t sufficient. We also need to take the association between the two variables, i.e. covariance, into account, as it is also an important factor in determine the overall shape. So, we need a covariance matrix to contain all the shape information, for example, in 2D case, \\[\n  \\left(\n  \\begin{matrix}\n\\text{Var}(\\text{Height}) & \\text{Cov}(\\text{Height}, \\text{Weight}) \\\\\n\\text{Cov}(\\text{Height}, \\text{Weight}) & \\text{Var}(\\text{Weight})\n  \\end{matrix}\n  \\right)\n\\] Usually, a covariance matrix is denoted by \\(\\boldsymbol{\\Sigma}\\) which is a \\(p \\times p\\) symmetric matrix (also need to be positive definite), \\(p\\) is the number of variables. In 2D case, the covariance matrix is usually presented as \\[\n  \\boldsymbol{\\Sigma}=\n  \\left(\n  \\begin{matrix}\n\\sigma_1^2 &\\sigma_{12}\\\\\n\\sigma_{12} & \\sigma_2^2\n  \\end{matrix}\n  \\right)\n\\] Given the definition of correlation, it is also can be represented as \\[\n  \\boldsymbol{\\Sigma}=\n  \\left(\n  \\begin{matrix}\n\\sigma_1^2 & \\rho \\sigma_1\\sigma_2\\\\\n  \\rho \\sigma_1\\sigma_2 & \\sigma_2^2\n  \\end{matrix}\n  \\right)\n\\] where \\(\\rho\\) is the correlation between \\(X_1\\) and \\(X_2\\)\nWe explained what is the essence of Normal distributions, that is likelihood value of an observation is inversely proportional to the normalized distance between the observation and mean value. Multivariate normal distribution should inherent this idea for sure. But the question is what is the normalized distance in 2D? Well, it is not such straightforward, I will explain it in another notes. (ToDo4Sia) We can just understand it as some normalized Euclidean distance in 2D. Thus, normal distributions, regardless of their dimensional, share a common pattern in their  p.d.f , i.e. \\[\n  f(\\textbf{x}) \\propto \\exp ( - d_M(\\textbf{x}, \\boldsymbol{\\mu}) )\n\\] where \\(\\textbf{x} = (x_1, x_2)^{\\top}\\), \\(\\boldsymbol{\\mu} = (\\mu_1, \\mu_2)^{\\top}\\) and \\(d_M(\\cdot, \\cdot )\\) is the normalized distance between two input points. From the LHS of the formula, in in the 2D setting, the likelihood value of each observation (man) is determined by two inputs. From the RHS of the formula, the likelihood value is determined by the normalized distance \\(d_M\\), i.e. closer to the center point, higher possibility to observe.\n\nNext, I show you some examples to demonstrate the arguments above.\nCase 1: \\(\\sigma_1^2 = \\sigma_2^2 = 1\\) and \\(\\rho = 0\\), i.e. the contrivance matrix is \\[\n\\boldsymbol{\\Sigma} =\n  \\left(\n  \\begin{matrix}\n    1 & 0 \\\\\n    0 & 1\n  \\end{matrix}\n  \\right)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the LHS, the 2D Normal density function, a surface, is displayed. You can move your cursor on the surface and see the chaning of likelihood values. You can rotate it to visualize it from the top, then you will find similar picture as the RHS. We call it as contour graph of the density function. It is the way to visualize a surface in a 2D plot. Each ring or curve presents a likelihood level, it means all the points on the ring have the sample likelihood value. The red one has the highest, 99%, and purple one has the lowest, 10%.\n\n\n\n\n\n\nCase 2: \\(\\sigma_1^2 = 1\\), \\(\\sigma_2^2 = 2\\) and \\(\\rho = 0\\) i.e. the contrivance matrix is \\[\n\\boldsymbol{\\Sigma} =\n  \\left(\n  \\begin{matrix}\n    1 & 0 \\\\\n    0 & 2\n  \\end{matrix}\n  \\right)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this case, the contour graph is changed from a circle to an ellipse because of the different variances. However, it is still vertical to the x-axis or parallel to the y-axis, as there is no correlation between the two variables.\n\n\n\n\n\n\nCase 3: \\(\\sigma_1 = 1\\), \\(\\sigma_2 = 2\\) and \\(\\rho = 0.8\\), i.e. the contrivance matrix is \\[\n\\boldsymbol{\\Sigma} =\n  \\left(\n  \\begin{matrix}\n    1 & 1.13 \\\\\n    1.13 & 2\n  \\end{matrix}\n  \\right)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBecause of the positive correlation between two variables, the contour graph leans to the right.\n\n\n\n\n\n\nCase 4: \\(\\sigma_1 = 1\\), \\(\\sigma_2 = 2\\) and \\(\\rho = -0.8\\), i.e. the contrivance matrix is \\[\n\\boldsymbol{\\Sigma} =\n  \\left(\n  \\begin{matrix}\n    1 & -1.13 \\\\\n    -1.13 & 2\n  \\end{matrix}\n  \\right)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, the contour graph leans to the left, as the correlation is negative.\n\n\n\n\n\n\n\nThere are more stories about Normal (Gaussian) distribution. I will write a note about it. Please keep an eye on my space. ToDo4Sia: write a note telling the story of Gaussian distribution.\n\nLecture 3 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_lesson_home.html",
    "href": "Courses/c_mlwr1_2024/l3/l3_lesson_home.html",
    "title": "Exercises on Probability Knowledge",
    "section": "",
    "text": "In this lesson, we do some exercises on probability knowledge. There are 6 main tasks, each covering various knowledge points from the lecture notes. Some require you to solve problems discussed in the lecture, while others introduce new questions or extend previous one.\nIf you are skilled in probability theory, you might treat it as a quick and easy test. For those who are not familiar with probability theory, these exercises can assistant you for learning or reviewing the basics. They are not mathematical puzzles; rather, they are basic exercises designed to help you understand those concepts. Again, the more you solve, the more you practice, the more you master.\n\nTask 1: Probabilities of events\nTask 1.1 A bag contains 5 red balls, 3 blue balls, and 2 green balls. If you randomly draw one marble from the bag:\n\nWhat is the probability of drawing a red ball?\nWhat is the probability of drawing a blue or green ball?\n\nTask 1.2 A fair six-sided dice is rolled once.\n\nWhat is the probability of getting an even number?\nWhat is the conditional probability of getting a number larger than 3 given the number is an even number?\nWhat is the probability that you first time get a number less than 2 after throwing the dice 5 times?\n\n\n\nTask 2: Discrete Random Variable and Distribution\nTask 2.1: In R, write a function returning the values of  p.m.f  of an arbitrary Binomial distribution.\nTask 2.2: Apply your function to print out the distribution of Binomial distribution \\(\\text{Bin}(10,0.7)\\).\n\n\nTask 3: Characteristic Values\nTask 3.1: Explain why the expected value of a Bernoulli distributed random variable, \\(X \\sim \\text{Ber}(p)\\), is \\(p\\).\nTask 3.2: Explain why the expected value of a Binomial distributed random variable, \\(X \\sim \\text{Bin}(N, p)\\), is \\(Np\\).\nTask 3.3: Explain why the variance of a Bernoulli distributed random variable is \\(p(1-p)\\).\nTask 3.4: Explain why the variance of a Binomial distributed random variable is \\(Np(1-p)\\).\nTask 3.5 (  HS  ): In Section 3.4.5, we discussed the covariance between two random variables and got formulas of calculating the characteristic values for the weighted sum (linear combination) of random variables. Actually, these formula can be represented in a matrix form, and the matrix form can help us to easily generalize it to multiple setting (more than two random variables). For example, the mean and variance of a linear combination of two random variables are \\[\n  \\text{E}(a_1X_1+a_2X_2) = a_1\\text{E}(X_1) + a_2\\text{E}(X_2)\n\\] It can be represented as \\[\n  \\text{E}(\\textbf{a}^{\\top}\\textbf{X}) = \\textbf{a}^{\\top}\\text{E}(\\textbf{X})\n\\] where \\(\\textbf{a} = (a_1, a_2)^{\\top}\\) and \\(\\textbf{X} = (X_1, X_2)^{\\top}\\). In a multivariate setting, we usually call \\(\\textbf{X}\\) as a random vector, it is a 2 dimensional random vector in this case, however, it can be arbitrary dimension in general.\nNow, it is your turn. Can you represent the following result of variance in a matrix form? \\[\n  \\text{Var}(a_1X_1+a_2X_2) = a_1^2\\text{Var}(X_1) + 2a_1a_2\\text{Cov}(X_1,X_2) + a_2^2\\text{Var}(X_2)\n\\]\n\n\nTask 4: Joint distribution\nTask 4.1: In Section 3.4.2, I only show you how to calculate the value in the first cell, i.e. \\(\\Pr( X=1, Y=1 )\\). Please calculate the values of the rest 3 cells, i.e. \\(\\Pr( X=1, Y=0 )\\), \\(\\Pr( X=0, Y=1 )\\), and \\(\\Pr( X=0, Y=0 )\\)\nTask 4.2: With the same background problem, calculate the probability that you eventually get an orange.\nTask 4.3: Apply Bayes Formula to calculate the posterior probability that we chose the red box if we get an apple, i.e. \\(\\Pr( X=1 | Y=1 )\\).\nTask 4.4: Discuss with your group mates, propose a new example to explain what is the difference between prior and posterior probabilities.\n\n\nTask 5: Continuous Random Variable and Distribution\nTask 5.1: \\(X \\sim \\mathcal{N}(1.2, 1)\\), use R to calculate \\(\\Pr(-1.5 &lt; X &lt; 2.2)\\).\nTask 5.2: Suppose \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then what is the distribution of \\(\\frac{X - \\mu}{\\sigma}\\)?\nTask 5.3: Explain why 95% of the probability is covered within two SDs around the mean in a Normal distribution.\n\n\nTask 6: Likelihood Analysis\nTask 6.1: With the “Box-Fruits” the background problem, let’s make a small adjustment: we use the flip of a fair coin to decide which box to choose.\nSuppose you got an apple—then which box do you think you are more likely to have chosen?\nTask 6.2 Now, we change back the original setting that we throw a dice to decide the box, but we get red box when getting a number less than 6. Suppose you got an apple—then which box do you think you are more likely to have chosen?\n\nLecture 3 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_lesson_home.html#task",
    "href": "Courses/c_mlwr1_2024/l3/l3_lesson_home.html#task",
    "title": "Exercises on Probability Knowledge",
    "section": "Task:",
    "text": "Task:\nIn R, write a function returning the values of  p.m.f  of an arbitrary Binomial distribution. Use R to print out the distribution of Binomial distribution \\(\\text{Bin}(10,0.7)\\)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_lesson_home.html#task-about-discrete-random-variable-and-distribution",
    "href": "Courses/c_mlwr1_2024/l3/l3_lesson_home.html#task-about-discrete-random-variable-and-distribution",
    "title": "Exercises on Probability Knowledge",
    "section": "Task: About Discrete Random Variable and Distribution",
    "text": "Task: About Discrete Random Variable and Distribution\n\nTask\nIn R, write a function returning the values of  p.m.f  of an arbitrary Binomial distribution.\n\n\nTask\nUse R to print out the distribution of Binomial distribution \\(\\text{Bin}(10,0.7)\\)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#characteristic-values",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#characteristic-values",
    "title": "Lecture 3: Basic Knowledge in Probability Theory",
    "section": "3.3 Characteristic Values",
    "text": "3.3 Characteristic Values\n\n3.3.1 Expected Value\nWe have introduced the most basic elements of probability theory, namely random variables and their distributions. Now, I have a question to you. Suppose we have two binomial distributed random variables,  \\(X_1 \\sim \\text{Bin}(10, 0.1)\\)  and  \\(X_2 \\sim \\text{Bin}(3, 0.7)\\) , can we compare them? (You can hover your cursor on \\(X_1\\) and \\(X_2\\) to get the meaning of them.) Well, the two random variables have different potential outcome, for \\(X_1\\), you might get a integer from \\(0\\) to \\(10\\), but only three possible values, \\(0, 1, 2, 3\\) for \\(X_2\\). Apparently, it is not comparable. Then can we compare their observed values? It doesn’t seem that simple neither. One is an experiment with a low success rate conducted 10 times, while the other has a high success rate but we only conduct it 3 times, so it’s hard to say which will have a higher number of successes. Let’s explore the answer in a straightforward way by actually generating two random numbers from their respective distributions and comparing them.\nX1 = rbinom(1, prob = 0.1, size = 10)\nX2 = rbinom(1, prob = 0.7, size = 3)\nX2&gt;X1\nIf you run the code above multiple times, you’ll get both TRUE and FALSE, but you may feel that the realization of \\(X_2\\) is often higher than the realization of \\(X_1\\). To verify our intuition, we can repeatedly run the code above and record the result each time. By doing this 1000 times, we can see the percentage of cases where the value of \\(X_2\\) is greater than the value of \\(X_1\\).\n\nres = numeric(1000)\nfor(i in 1:1000){\n  X1 = rbinom(1, prob = 0.1, size = 10)\n  X2 = rbinom(1, prob = 0.7, size = 3)\n  res[i] = ifelse(X2&gt;X1,1,0)\n}\nprint(paste0(\"The proportion of times X2&gt;X1 is \", sum(res)/10,\"%\"))\n\n[1] \"The proportion of times X2&gt;X1 is 69.5%\"\n\n\nIndeed, we often observe that the value of \\(X_2\\) is greater than \\(X_1\\). However, is there a way to reach a conclusion directly without relying on experiments? Or perhaps, can we explain this phenomenon using mathematical language? We then need to introduce another important concept, the expected value of a random variable. The expected value of a random variable is very similar to the concept that you are very familiar with, that is average value. If I ask you how often do you go to IKSU per week? Then most likely, you will answer me that you go to IKSU, on average, 3 times per week, since the number of visiting IKSU (the largest sports center in Umeå) per week is not a certain number (you often go there 3 times, but not always, for example, you are sick or have an important exam to prepare). Let me show you the number of my IKSU visits in the last 10 weeks. \\[\n  3,5,3,2,2,4,5,3,3,4\n\\]\nWe all know that the average value can be \\[\n  \\frac{3+5+3+2+2+4+5+3+3+4}{10} = 3.4\n\\] Of course, it is a super easy calculation, but let’s have a close look at it. This calculation can be represented as \\[\n  \\frac{2 \\times 2 + 4\\times3 + 2\\times4 +2\\times 5 }{10} = 0.2 \\times 2 + 0.4 \\times 3 + 0.2 \\times 4 + 0.2 \\times 5 = 3.4\n\\]\nNotice that the decimal in front of each integer, the possible value, is the percentage of the corresponding value that happened in the last ten weeks. In the rational world, if you still remember it, the percentage is replaced by the probability. Therefore, the definition of expected value is defined as the weighted sum of all possible values, and the weights are the corresponding probabilities. In a mathematical notation, the expected value of a random variable is presented as \\[\n  \\text{E}(X) = \\sum_{k} k \\Pr (X = k)\n\\] We can see that the expected value of a binary distributed random variable and a binomial distributed random variable is \\(p\\) and \\(Np\\) respectively. It is a good exercise to verify it. Now, we can turn back to the question at the beginning. By simple calculation, we can see that \\[\n  \\text{E}(X_2) = 3\\times0.7 = 2.1 &gt; \\text{E}(X_1) = 10\\times0.1 = 1\n\\] The expected value satisfies linearity. Suppose \\(a\\) and \\(b\\) are constant numbers and \\(X\\) is a random variable, then \\(\\text{E}(aX+b) = a \\text{E}(X) + b\\). In other words, linearity means the expectation operator and the linear operator (scalar multiplication and addition) are exchangeable, i.e. \\[\n  \\text{E} \\left(\\sum_{i=1}^n a_iX_i\\right)  = \\sum_{i=1}^n  a_i\\text{E}(X_i)\n\\]\n\n\n3.3.2 Variance\nThe expected value can help us determine the size of the “common” value of a random variable so that we can compare two random variables. One can also compare two random variables from another dimension, which is “value stability”. For example, we have two coins, one is even and the other is so uneven that there is a very high probability, 90%, of getting Heads. Then imagine that if we flip two coins repeatedly, we will get many heads for the uneven coin and occasionally get Tails; but for the even coin, we will get the same number of heads and tails with high probability. From the perspective of taking values, the values of uneven coins are very stable, while those of uniform coins are not. The stability of a random also refers to variation. High variation means low stability. The two things can be quantified by the variance.\nThe variance of a random variable \\(X\\) is defined as \\[\n  \\text{Var}(X) = \\text{E}(X - \\text{E}(X))^2\n\\]\nThis formula is very intuitive. First, we calculate the most frequently occurring value of this random variable, \\(\\text{E}(X)\\), and then compare it with any \\(X\\) by taking the difference. Finally, we calculate the average of the squares of this difference. If this random variable has stable values, then the value of \\(X\\) should often stay around the expected value, therefore \\(X - \\text{E}(X)\\) will generally be very small; conversely, if it varies significantly, it will be generally large, which aligns perfectly with our initial intention.\nBased on this definition, one can easily verify the variance of a binary distributed random variable and a binomial distributed random variable is \\(p(1-p)\\) and \\(np(1-p)\\) respectively. In the example above, the variance of the even coin is \\(0.25\\), but the uneven is \\(0.09\\).\nDifferent from the expected value, variance doesn’t satisfy the linearity, i.e. the variance operator and the linear operator are not exchangeable. However, it satisfies the following rules, \\[\n  \\text{Var}(aX+b) = a^2\\text{Var}(X)  \n\\]\nBased on the results above, we can easily find that a special linear combination, \\(\\left( X-\\text{E}(X) \\right)/\\sqrt{\\text{Var}(X)}\\), produces a standardized random variable, i.e. mean zero and variance 1."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#continuous-random-variables",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#continuous-random-variables",
    "title": "Lecture 3: Basic Knowledge in Probability Theory",
    "section": "3.5 Continuous Random Variables",
    "text": "3.5 Continuous Random Variables\nNext, we will consider a more challenging concept, continuous random variable. As you may notice, all the random events that we are considering can be represented by categorical outcomes. For example, flipping a coin has two outcomes, throwing a dice has 6 outcomes, and so on. For this kind of random event, it is sufficient to consider random variables only taking integer values. Thus we refer to the random variables discussed as discrete random variables. In practice, however, there are many other random events whose outcomes are not categorical but continuous values, for example, the temperature, the height of adult males, and so on. Therefore we need another type of random variable, a continuous variable.\n\n3.5.1 Continuous distribution\nContinuous random variables are not difficult to understand, they are nothing more than random variables that take real numbers, but the problem is how to describe their distribution. Again, let us abstract mathematical concepts from reality. Let’s consider such a background problem, assuming that I have height data for all boys in middle school in Sweden. Height is obviously not a categorical variable, but we can still use grouping to describe the distribution of height from a discrete perspective. Specifically, we can evenly divide the possible range of height into several groups, and then calculate the percentage of the number of people in each group to the total number of people. Yes, if you are familiar with basic statistics, you can tell that this is a histogram at a glance.\n\n\n\n\n\nSuch an approach has obvious flaws. For example, on the left-top of the plot, it is difficult for us to distinguish the probability of height being less than 175 and greater than 170 because these two values are combined into one group. How to do it? Very simple, we can split each large group into two small groups, of course, you also need to include more boys into the data set and then calculate the frequency of each group to represent the distribution of height, for example, on the right-top of the plot. If we still cannot distinguish the above probability, we can continue to split each group into two groups. Doing this we can see that the histogram is more detailed. If we have a sufficient large data set, we can continue to subdivide the height group and know the probability that we can distinguish the above two events. Suppose we put “all” the boys in middle school into this histogram, and each group can be subdivided infinitely. We can imagine that the upper edge of the histogram will be a smooth curve. We call this smooth curve the probability density function (p.d.f) and denote it as \\(f(x)\\). A valid  p.d.f  has to inherit two conditions from the  p.m.f  of a discrete random. First, the density value must be positive, \\(f(x) &gt; 0\\), and the integral on the whole domain should be 1, \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\). With this function, we can calculate the probability of many events, as well as the expectation and variance. \\[\n  \\Pr(X&lt;b) = \\int_{-\\infty}^bf(x)dx\n\\] \\[\n  \\text{E}(X) =  \\int_{-\\infty}^{\\infty} xf(x) dx\n\\]\nOne can compare the formula above with the expected value of a discrete random variable, \\(\\text{E}(X) = \\sum_k k\\Pr(x=k)\\). You can see similar patterns, they all are the “sum” of all possible values times the corresponding probability or density values. Keep in mind that the integral symbol is an elongated S which indicates “sum”.\nSo far, we have only specified the basic conditions for a function to be a p.d.f , but the exact form of this function depends on the continuous distribution it represents. Next, we will learn about one of the most common continuous distributions: the Normal distribution.\n\n\n3.5.2 Normal (Gaussian) distribution\nA continuous random variable is Normally distributed, \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), if its density function is\n\\[\n  f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} } e^{- \\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma}\\right)^2 }\n\\] The normal distribution is determined by two parameters, location parameter \\(\\mu\\) and shape parameter \\(\\sigma^2\\). Density functions of normal distribution with different parameters are displayed in the following picture.\n\n\n\n\n\nLHS: Normal distribution with fixed shape parameter (sigma = 1) and different location parameters, orange: mu = -4, blue: mu = 0, red: mu = 2. RHS: Normal distribution with fixed location parameter (mu = 0) and different shape parameters, orange: sigma = 3, blue: sigma = 1, red: sigma = 0.5.\n\n\n\n\nNow, I have two questions to you.\n\nWe have studied both discrete random variable and continuous random variable, their distribution of possible values can be presented by p.m.f and p.d.f respectively. The value of a p.m.f is just the probability when the random variable taking this value. However, what is the meaning of the value of a p.d.f?\nWhat is the essence of the Normal distribution? In simpler terms, how would you introduce the concept of the Normal distribution to a middle school student?\n\nThink about them and we will come back to them later on."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_4.html",
    "href": "Courses/c_mlwr1_2024/l3/l3_4.html",
    "title": "3.4 Two Dependent Random Variables",
    "section": "",
    "text": "3.4.1 Sum rule and product rule\nLet’s think about a question like this. The basic conditions are presented in the following picture. Suppose we want to randomly pick a box and then randomly take out a fruit from it, then what is the probability that the fruit taken out is an apple? We can randomly pick the box by throwing a dice. If we get a number less than 5 then we choose the red box, or we choose the blue box.\n\n\n\nBox and Furits Problem: there are two boxes, red and blue, and the red box contains two apples and six oranges, while the blue box contains three apples and one orange.\n\n\nThis example is a bit more complicated than the previous because there are two randomized actions involved here. The result of this action involves a combination of the color of the box and the type of fruits. Let’s start by considering the probability that the red box is drawn while an apple is picked up. Again, we can use the previous formula to calculate this probability. There are six numbers corresponding to the dice, and the number of fruits inside the red box is 8, so all the possibilities are \\(6 \\times 8\\). But there are only numbers 1 through 4 and two apples, so there are only \\(4 \\times 2\\) possibilities that qualify. So the probability is \\[\n  \\frac{4\\times2}{6\\times8} = \\frac{4}{6} \\times \\frac{2}{8} = \\frac{1}{6}\n\\] It is easy to see that \\(4/6\\) is the probability of getting a number less than \\(5\\), i.e. the red box is selected. But what is the meaning of the second part, \\(2/8\\)? Since \\(8\\) is the number of fruits and \\(2\\) is the number of apples in the red box, it can be understood as the probability of getting an apple when the red box was selected. We refer to this probability as conditional probability and present it as \\(\\Pr(\\text{Apple} | \\text{Red}) = 2/8\\). This discussion can be summarized as \\[\n  \\Pr ( \\text{Red AND Apple} ) = \\Pr( \\text{Apple} | \\text{Red} ) \\Pr ( \\text{Red} )\n\\] We can also easily get the probability of getting an apple under the other possibility, i.e. \\[\n  \\Pr(\\text{Blue AND Apple}) = \\Pr(\\text{Apple} | \\text{Blue} ) \\Pr( \\text{Blue} ) = \\frac{3}{4} \\times \\frac{2}{6} = \\frac{1}{4}\n\\] “AND” corresponds to the “product rule”. \\[\n  \\Pr(E_1 \\text{ AND } E_2) = \\Pr(E_1 | E_2)\\Pr(E_2) = \\Pr(E_2 | E_1)\\Pr(E_1)\n\\] Going back to our original question, what is the probability of getting an apple? This random event can be labeled as “Red AND Apple OR Blue AND Apple”. Here, we have the second rule, i.e. “sum rule”, when considering the “OR” operator between two events that don’t happen simultaneously. \\[\n  \\Pr(E_1 \\text{ OR } E_2) = \\Pr(E_1) + \\Pr(E_2)\n\\]\nBased on the sum rule, the probability of getting an apple is calculated as \\[\n  \\Pr( \\text{Apple} ) = \\Pr( \\text{Red AND Apple} ) + \\Pr( \\text{Blue AND Apple} ) = \\frac{5}{12}\n\\]\nRemark: we can compare it with the sum-product rule in permutation and combinations. When there are different types of solutions for one thing, the total number of possible solutions is the sum of the number of possible solutions for each type. When there are different steps in doing one thing, the total number of solutions is the product of the number of possible solutions in each step.\n\n\n3.4.2 Joint distribution and marginal distribution\nSimilarly, you can verify the probabilities when orange is considered. If we use random variables to present the events, for example, the random variable \\(X\\) presents the box selected, \\(1\\) indicates red, and \\(0\\) indicates blue; the random variable \\(Y\\) presents the fruit drew, then the following table is called the joint distribution of two random variables.\n\n\n\n\n\nThe last column is called the marginal distribution of random variable \\(Y\\), and the last row is the marginal distribution of random variable \\(X\\).\n\n\n3.4.3 Posterior and Prior Probabilities\nWith the example above, the last interesting question is “What is the probability we chose the blue box if we get an orange?” First of all, it is a conditional probability, \\(\\Pr(X = 0 | Y = 0)\\). According to the product rule, we know that this conditional probability is the ratio between \\(\\Pr(X=0 \\text{ and } Y=0)\\) and \\(\\Pr(Y=0)\\). The first second has been calculated before and that is \\(7/12\\). Well, the first can be calculated by product rule again, i.e. \\(\\Pr( Y = 0 | X = 0 ) \\Pr(X=0)\\). Summarize, \\[\n  \\Pr(X = 0 | Y = 0) = \\frac{\\Pr( Y = 0 | X = 0 ) \\Pr(X=0)}{\\Pr(Y=0)}\n\\]\n\\(\\Pr(X=0|Y=0)\\) is different from the conditional probability in the first question \\(\\Pr(Y=0|X=0)\\). This probability is referred to as the posterior probability in the sense that we use the observation in the second step to update the probability of the first step. Correspondingly, the probability of drawing a blue box at the first step is called prior probability. The formula above is well known as Bayes formula.\n\n\n3.4.4 Statistically independent\nFrom the joint distribution, the probability of drawing an apple \\(\\Pr(Y = 1)\\) is \\(5/12\\). It is different from the probability of drawing an apple under the condition that the red box was selected, \\(\\Pr(Y=1 | X = 1) = 2/8\\). This fact implies that the value of random variable \\(Y\\) depends on the value of \\(X\\), or random variable \\(X\\) and \\(Y\\) are dependent. If we add 1 apple and 11 oranges to the blue box, then \\(\\Pr(Y=1) = \\Pr(Y=1 | X = 1)\\). In this case, the value of random variable \\(Y\\) doesn’t depend on the value of \\(X\\), i.e. they are independent.\n\n\n3.4.5 Covariance and Correlation\nFor two random variables \\(X\\) and \\(Y\\), we can use covariance to quantify the degree of association between two random variables. The covariance is defined as \\[\n  \\text{Cov}(X, Y) = \\text{E}(X-\\text{E}(X))(Y-\\text{E}(Y))\n\\] The mean and variance of a weighted sum (linear combination) of two random variables are \\[\n  \\text{E}(aX+bY) = a\\text{E}(X) + b\\text{E}(Y)\n\\] and \\[\n  \\text{Var}(aX+bY) = a^2\\text{Var}(X) + 2ab\\text{Cov}(X,Y) + b^2\\text{Var}(Y)\n\\] respectively.\nAnother characteristic value that quantifies the correlation between two variables is correlation. It is simply normalized covariance, i.e.\n\\[\n  \\rho_{x,y} = \\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X)} \\cdot \\sqrt{\\text{Var}(Y)}}\n\\] When the two variable uncorrelated, \\(\\text{Cov}(X, Y) = 0\\), therefore \\(\\rho_{x,y} = 0\\). Another two extreme cases are \\(X = Y\\), i.e. two random variables are the same, and \\(X = -Y\\). In these cases, \\(\\text{Cov}(X, \\pm Y) = \\pm \\text{Cov}(X, X) = \\pm \\text{Var}(X)\\), therefore, \\(\\rho_{x,y} = \\pm 1\\). Thus, different from covariance, the correlation is a number between \\([-1, 1]\\) due to the normalization. So, it is more comparable than covariance and people often use it to quantify the association between variables.\n\nPrevious page | Lecture 3 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_4.html#sum-rule-and-product-rule",
    "href": "Courses/c_mlwr1_2024/l3/l3_4.html#sum-rule-and-product-rule",
    "title": "3.4 Two Dependent Random Variables",
    "section": "",
    "text": "Let’s think about a question like this. The basic conditions are presented in the following picture. Suppose we want to randomly pick a box and then randomly take out a fruit from it, then what is the probability that the fruit taken out is an apple? We can randomly pick the box by throwing a dice. If we get a number less than 5 then we choose the red box, or we choose the blue box.\n\n\n\nBox and Furits Problem: there are two boxes, red and blue, and the red box contains two apples and six oranges, while the blue box contains three apples and one orange.\n\n\nThis example is a bit more complicated than the previous because there are two randomized actions involved here. The result of this action involves a combination of the color of the box and the type of fruits. Let’s start by considering the probability that the red box is drawn while an apple is picked up. Again, we can use the previous formula to calculate this probability. There are six numbers corresponding to the dice, and the number of fruits inside the red box is 8, so all the possibilities are \\(6 \\times 8\\). But there are only numbers 1 through 4 and two apples, so there are only \\(4 \\times 2\\) possibilities that qualify. So the probability is \\[\n  \\frac{4\\times2}{6\\times8} = \\frac{4}{6} \\times \\frac{2}{8} = \\frac{1}{6}\n\\] It is easy to see that \\(4/6\\) is the probability of getting a number less than \\(5\\), i.e. the red box is selected. But what is the meaning of the second part, \\(2/8\\)? Since \\(8\\) is the number of fruits and \\(2\\) is the number of apples in the red box, it can be understood as the probability of getting an apple when the red box was selected. We refer to this probability as conditional probability and present it as \\(\\Pr(\\text{Apple} | \\text{Red}) = 2/8\\). This discussion can be summarized as \\[\n  \\Pr ( \\text{Red AND Apple} ) = \\Pr( \\text{Apple} | \\text{Red} ) \\Pr ( \\text{Red} )\n\\] We can also easily get the probability of getting an apple under the other possibility, i.e. \\[\n  \\Pr(\\text{Blue AND Apple}) = \\Pr(\\text{Apple} | \\text{Blue} ) \\Pr( \\text{Blue} ) = \\frac{3}{4} \\times \\frac{2}{6} = \\frac{1}{4}\n\\] “AND” corresponds to the “product rule”. \\[\n  \\Pr(E_1 \\text{ AND } E_2) = \\Pr(E_1 | E_2)\\Pr(E_2) = \\Pr(E_2 | E_1)\\Pr(E_1)\n\\] Going back to our original question, what is the probability of getting an apple? This random event can be labeled as “Red AND Apple OR Blue AND Apple”. Here, we have the second rule, i.e. “sum rule”, when considering the “OR” operator between two events that don’t happen simultaneously. \\[\n  \\Pr(E_1 \\text{ OR } E_2) = \\Pr(E_1) + \\Pr(E_2)\n\\]\nBased on the sum rule, the probability of getting an apple is calculated as \\[\n  \\Pr( \\text{Apple} ) = \\Pr( \\text{Red AND Apple} ) + \\Pr( \\text{Blue AND Apple} ) = \\frac{5}{12}\n\\]\nRemark: we can compare it with the sum-product rule in permutation and combinatorics. When there are different types of solutions for one thing, the total number of possible solutions is the sum of the number of possible solutions for each type. When there are different steps in doing one thing, the total number of solutions is the product of the number of possible solutions in each step."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_4.html#joint-distribution-and-marginal-distribution",
    "href": "Courses/c_mlwr1_2024/l3/l3_4.html#joint-distribution-and-marginal-distribution",
    "title": "3.4 Two Dependent Random Variables",
    "section": "3.4.2 Joint distribution and marginal distribution",
    "text": "3.4.2 Joint distribution and marginal distribution\nSimilarly, you can verify the probabilities when orange is considered. If we use random variables to present the events, for example, the random variable \\(X\\) presents the box selected, \\(1\\) indicates red, and \\(0\\) indicates blue; the random variable \\(Y\\) presents the fruit drew, then the following table is called the joint distribution of two random variables.\n\n\n\n\n\nThe last column is called the marginal distribution of random variable \\(Y\\), and the last row is the marginal distribution of random variable \\(X\\)."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_4.html#posterior-and-prior-probabilities",
    "href": "Courses/c_mlwr1_2024/l3/l3_4.html#posterior-and-prior-probabilities",
    "title": "3.4 Two Dependent Random Variables",
    "section": "3.4.3 Posterior and Prior Probabilities",
    "text": "3.4.3 Posterior and Prior Probabilities\nWith the example above, the last interesting question is “What is the probability we chose the blue box if we get an orange?” First of all, it is a conditional probability, \\(\\Pr(X = 0 | Y = 0)\\). According to the product rule, we know that this conditional probability is the ratio between \\(\\Pr(X=0 \\text{ and } Y=0)\\) and \\(\\Pr(Y=0)\\). The first second has been calculated before and that is \\(7/12\\). Well, the first can be calculated by product rule again, i.e. \\(\\Pr( Y = 0 | X = 0 ) \\Pr(X=0)\\). Summarize, \\[\n  \\Pr(X = 0 | Y = 0) = \\frac{\\Pr( Y = 0 | X = 0 ) \\Pr(X=0)}{\\Pr(Y=0)}\n\\]\n\\(\\Pr(X=0|Y=0)\\) is different from the conditional probability in the first question \\(\\Pr(Y=0|X=0)\\). This probability is referred to as the posterior probability in the sense that we use the observation in the second step to update the probability of the first step. Correspondingly, the probability of drawing a blue box at the first step is called prior probability. The formula above is well known as Bayes formula."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_4.html#statistically-independent",
    "href": "Courses/c_mlwr1_2024/l3/l3_4.html#statistically-independent",
    "title": "3.4 Two Dependent Random Variables",
    "section": "3.4.4 Statistically independent",
    "text": "3.4.4 Statistically independent\nFrom the joint distribution, the probability of drawing an apple \\(\\Pr(Y = 1)\\) is \\(5/12\\). It is different from the probability of drawing an apple under the condition that the red box was selected, \\(\\Pr(Y=1 | X = 1) = 2/8\\). This fact implies that the value of random variable \\(Y\\) depends on the value of \\(X\\), or random variable \\(X\\) and \\(Y\\) are dependent. If we add 1 apple and 11 oranges to the blue box, then \\(\\Pr(Y=1) = \\Pr(Y=1 | X = 1)\\). In this case, the value of random variable \\(Y\\) doesn’t depend on the value of \\(X\\), i.e. they are independent."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_4.html#covariance",
    "href": "Courses/c_mlwr1_2024/l3/l3_4.html#covariance",
    "title": "3.4 Two Dependent Random Variables",
    "section": "3.4.5 Covariance",
    "text": "3.4.5 Covariance\nFor two random variables \\(X\\) and \\(Y\\), we can use covariance to quantify the degress of association between two random variables. The covariance is defined as \\[\n  \\text{Cov}(X, Y) = \\text{E}(X-\\text{E}(X))(Y-\\text{E}(Y))\n\\] The mean and variance of a weighted sum (linear combination) of two random variables are \\[\n  \\text{E}(aX+bY) = a\\text{E}(X) + b\\text{E}(Y)\n\\] and \\[\n  \\text{Var}(aX+bY) = a^2\\text{Var}(X) + 2ab\\text{Cov}(X,Y) + b^2\\text{Var}(Y)\n\\] respectively.\n\nPrevious page | Lecture 3 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_6.html",
    "href": "Courses/c_mlwr1_2024/l3/l3_6.html",
    "title": "3.6 Likelihood Analysis",
    "section": "",
    "text": "Likelihood analysis is an important concept in the modern statistics. A good understanding of likelihood can help you improve your knowledge in statistics. By understanding the relationship between likelihood and probability, you can build a bridge in statistical or data and probability modeling, allowing you to use models with greater confidence.\n\n3.6.1 Likelihood value V.S. Probability\nWe have build up all the basic concepts of probability. In simple terms, probability is a mathematical model used to quantify the possibility of an event. The discussion of the possibility of this event is limited to the realm of rationality. For example, we can use probability to discuss the following questions.\n“Imagine you are standing in the square in the city center of Umeå, close your eyes for 30 seconds, then open your eyes and catch the first man you see. How likely is his height above 180 cm?”\nLet’s have a look at another scenario,\n“You went to downtown last weekend. You stood in the square and closed your eyes, then after 30 seconds you opened your eyes and grabbed the man you saw first and measured his height. His height is 230 and you think it is unbelievable.”\nIn this case, we have a real observation of a random variable and want to evaluate the possibility of this observation. In this case, a proper word is likelihood value. I hope from the two examples above, you can see the difference between the two synonyms for “possibility.” In one word, likelihood value is the evaluation of the possibility to one observed valued, but probability evaluate the possibility of an event (somehow have not happened). Now, we can answer the first question above, what is the meaning of the value of a p.d.f? It present the likelihood of a real observation. For example, we may assume the height of adult men in Sweden is Normally distributed with mean 170 and SD 5.5 (I got these statistics from SCB, Statistiska centralbyrån), then the likelihood values of observing a man who is 179 and a men who is 230 are\n\ndnorm(179,179,5.5) \n\n[1] 0.07253496\n\ndnorm(230,179,5.5)\n\n[1] 1.546941e-20\n\n\nSo, it is almost impossible to see a men who is 230 in the center of Umeå city.\nRemark 1: Unlike probability, likelihood is not standardized and is not a decimal between 0 and 1. When the standard deviation is very small, the likelihood value near the mean can be quite large. For example,\n\ndnorm(0, mean = 0, sd = 0.001)\n\n[1] 398.9423\n\n\nRemark 2 (HBG2K) : When people use the likelihood value, they often take the logarithm of it. One reason is that, even when the likelihood value is very small, we can still have a meaningful number for calculations. Another reason is that the distributions we commonly use, like the normal distribution, belong to the exponential family, so taking the logarithm transforms a nonlinear function into a linear one. For example,\n\ndnorm(230, 179, 5.5)\n\n[1] 1.546941e-20\n\nlog(dnorm(230, 179, 5.5))\n\n[1] -45.61542\n\n\nNext, we summarize the facts about discrete random variable and continuous random variable in the following table.\n\n\n\n\n\n\n\n3.6.2 The secrete message delivered by Normal distribution\nYou surely remember the previous question: what does the p.d.f of the normal distribution represent?\n\\[\n  f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} } e^{- \\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma}\\right)^2 }\n\\]\nOr, to put it another way, just as the binary distribution aims to describe random events like coin tossing, what kind of random phenomenon is the normal distribution trying to describe?\nLet’s start with en elementary understanding.\n\nThe normal distribution describes a bell-shaped, symmetric distribution of random phenomena, such as human IQ. Most people’s IQs are close to the mean, a smaller portion have higher or lower IQs, and only a very few have extremely high or low IQs. This forms a bell-shaped, symmetric distribution.\nWe can use more precise statistical terminology, such as confidence intervals, to describe this bell-shaped symmetric distribution of random phenomena. The normal distribution describes a random phenomenon where 68% of the probability is covered by the confidence interval of one SD around the mean, while the confidence interval of two SDs can cover 95% of the probability, and the confidence interval of three SDs can cover nearly all possibilities, as shown in the figure below.”\n\n\n\n\n\n\nIf you answer the question in such ways, then congratulations. You have rather good learning outputs from the previous basic statistics course. Next, let me introduce to you a deeper understanding based on the concept of likelihood. Let’s review the p.d.f of the normal distribution. Let’s watch an animation first.\n\n\n\nNote that the negative sign determines the inversely proportional relationship. The letter \\(e\\) represents the exponential function, which determines the rate of change.\n\n\nNotice that \\(\\left(\\frac{x-u}{\\sigma}\\right)^2\\) is the normalized distance between an observation \\(x\\) and the center point \\(\\mu\\). As mentioned before, the value of density function is the likelihood value of one observation, so the p.d.f is presenting the relationship between a specific observation \\(x\\) and its likelihood value. Then Normal distribution is describing such kind of random phenomenon:\n\nThe likelihood of an observation \\(x_0\\) is inversely proportion to the normalized distance between observed value \\(x_0\\) and the mean value \\(\\mu\\).\nMore precisely, when the observed value is far from the center point, the likelihood of observing it will decrease rapidly, and this decrease is exponential, as displayed below.\n\n\n\n\n\n\nIn summary, the normal distribution essentially describes the relationship between the distance of observed values from the center point and likelihood within a class of random phenomena. It connects the geometric concept of distance with the statistical concept of likelihood.\n\n\n3.6.3 Multivariate Gaussian Distribution\nWith the likelihood idea, we can easily extend the normal distribution to a multidimensional case. In this note, I will just list several points help you to understand the main idea.\nWhat is multidimensional case? For example, we may assume the height of Swedish adult men is normally distributed and denote as \\(X_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)\\). We also know that another important physical indicator is weight, which can also be assumed to follow a normal distribution, denoted as \\(X_2 \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)\\). If, for each Swedish adult man, we simultaneously consider height and weight as descriptive features rather than just one or the other, then we enter a multidimensional scenario. So, what changes do we need to make in a multidimensional scenario? With the height-weight example,\n\nWe need two values to determine the location of the center point of the distribution, or we need a mean vector \\((\\mu_1, \\mu_2)^{\\top}\\).\nIn the one dim case, we use variance/SD to control the shape of distribution. Naturally, we also need to consider the two variance simultaneously. However, this isn’t sufficient. We also need to take the association between the two variables, i.e. covariance, into account, as it is also an important factor in determine the overall shape. So, we need a covariance matrix to contain all the shape information, for example, in 2D case, \\[\n  \\left(\n  \\begin{matrix}\n\\text{Var}(\\text{Height}) & \\text{Cov}(\\text{Height}, \\text{Weight}) \\\\\n\\text{Cov}(\\text{Height}, \\text{Weight}) & \\text{Var}(\\text{Weight})\n  \\end{matrix}\n  \\right)\n\\] Usually, a covariance matrix is denoted by \\(\\boldsymbol{\\Sigma}\\) which is a \\(p \\times p\\) symmetric matrix (also need to be positive definite), \\(p\\) is the number of variables. In 2D case, the covariance matrix is usually presented as \\[\n  \\boldsymbol{\\Sigma}=\n  \\left(\n  \\begin{matrix}\n\\sigma_1^2 &\\sigma_{12}\\\\\n\\sigma_{12} & \\sigma_2^2\n  \\end{matrix}\n  \\right)\n\\] Given the definition of correlation, it is also can be represented as \\[\n  \\boldsymbol{\\Sigma}=\n  \\left(\n  \\begin{matrix}\n\\sigma_1^2 & \\rho \\sigma_1\\sigma_2\\\\\n  \\rho \\sigma_1\\sigma_2 & \\sigma_2^2\n  \\end{matrix}\n  \\right)\n\\] where \\(\\rho\\) is the correlation between \\(X_1\\) and \\(X_2\\)\nWe explained what is the essence of Normal distributions, that is likelihood value of an observation is inversely proportional to the normalized distance between the observation and mean value. Multivariate normal distribution should inherent this idea for sure. But the question is what is the normalized distance in 2D? Well, it is not such straightforward, I will explain it in another notes. (ToDo4Sia) We can just understand it as some normalized Euclidean distance in 2D. Thus, normal distributions, regardless of their dimensional, share a common pattern in their  p.d.f , i.e. \\[\n  f(\\textbf{x}) \\propto \\exp ( - d_M(\\textbf{x}, \\boldsymbol{\\mu}) )\n\\] where \\(\\textbf{x} = (x_1, x_2)^{\\top}\\), \\(\\boldsymbol{\\mu} = (\\mu_1, \\mu_2)^{\\top}\\) and \\(d_M(\\cdot, \\cdot )\\) is the normalized distance between two input points. From the LHS of the formula, in in the 2D setting, the likelihood value of each observation (man) is determined by two inputs. From the RHS of the formula, the likelihood value is determined by the normalized distance \\(d_M\\), i.e. closer to the center point, higher possibility to observe.\n\nNext, I show you some examples to demonstrate the arguments above.\nCase 1: \\(\\sigma_1^2 = \\sigma_2^2 = 1\\) and \\(\\rho = 0\\), i.e. the contrivance matrix is \\[\n\\boldsymbol{\\Sigma} =\n  \\left(\n  \\begin{matrix}\n    1 & 0 \\\\\n    0 & 1\n  \\end{matrix}\n  \\right)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the LHS, the 2D Normal density function, a surface, is displayed. You can move your cursor on the surface and see the chaning of likelihood values. You can rotate it to visualize it from the top, then you will find similar picture as the RHS. We call it as contour graph of the density function. It is the way to visualize a surface in a 2D plot. Each ring or curve presents a likelihood level, it means all the points on the ring have the sample likelihood value. The red one has the highest, 99%, and purple one has the lowest, 10%.\n\n\n\n\n\n\nCase 2: \\(\\sigma_1^2 = 1\\), \\(\\sigma_2^2 = 2\\) and \\(\\rho = 0\\) i.e. the contrivance matrix is \\[\n\\boldsymbol{\\Sigma} =\n  \\left(\n  \\begin{matrix}\n    1 & 0 \\\\\n    0 & 2\n  \\end{matrix}\n  \\right)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this case, the contour graph is changed from a circle to an ellipse because of the different variances. However, it is still vertical to the x-axis or parallel to the y-axis, as there is no correlation between the two variables.\n\n\n\n\n\n\nCase 3: \\(\\sigma_1 = 1\\), \\(\\sigma_2 = 2\\) and \\(\\rho = 0.8\\), i.e. the contrivance matrix is \\[\n\\boldsymbol{\\Sigma} =\n  \\left(\n  \\begin{matrix}\n    1 & 1.13 \\\\\n    1.13 & 2\n  \\end{matrix}\n  \\right)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBecause of the positive correlation between two variables, the contour graph leans to the right.\n\n\n\n\n\n\nCase 4: \\(\\sigma_1 = 1\\), \\(\\sigma_2 = 2\\) and \\(\\rho = -0.8\\), i.e. the contrivance matrix is \\[\n\\boldsymbol{\\Sigma} =\n  \\left(\n  \\begin{matrix}\n    1 & -1.13 \\\\\n    -1.13 & 2\n  \\end{matrix}\n  \\right)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, the contour graph leans to the left, as the correlation is negative.\n\n\n\n\n\n\n\nThere are more stories about Normal (Gaussian) distribution. I will write a note about it. Please keep an eye on my space. ToDo4Sia: write a note telling the story of Gaussian distribution.\n\nPrevious page | Lecture 3 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_3.html",
    "href": "Courses/c_mlwr1_2024/l3/l3_3.html",
    "title": "3.3 Characteristic Values",
    "section": "",
    "text": "We have introduced the most basic elements of probability theory, namely random variables and their distributions. Now, I have a question to you. Suppose we have two binomial distributed random variables,  \\(X_1 \\sim \\text{Bin}(10, 0.1)\\)  and  \\(X_2 \\sim \\text{Bin}(3, 0.7)\\) , can we compare them? (You can hover your cursor on \\(X_1\\) and \\(X_2\\) to get the meaning of them.) Well, the two random variables have different potential outcome, for \\(X_1\\), you might get a integer from \\(0\\) to \\(10\\), but only three possible values, \\(0, 1, 2, 3\\) for \\(X_2\\). Apparently, it is not comparable. Then can we compare their observed values? It doesn’t seem that simple neither. One is an experiment with a low success rate conducted 10 times, while the other has a high success rate but we only conduct it 3 times, so it’s hard to say which will have a higher number of successes. Let’s explore the answer in a straightforward way by actually generating two random numbers from their respective distributions and comparing them.\nX1 = rbinom(1, prob = 0.1, size = 10)\nX2 = rbinom(1, prob = 0.7, size = 3)\nX2&gt;X1\nIf you run the code above multiple times, you’ll get both TRUE and FALSE, but you may feel that the realization of \\(X_2\\) is often higher than the realization of \\(X_1\\). To verify our intuition, we can repeatedly run the code above and record the result each time. By doing this 1000 times, we can see the percentage of cases where the value of \\(X_2\\) is greater than the value of \\(X_1\\).\n\nres = numeric(1000)\nfor(i in 1:1000){\n  X1 = rbinom(1, prob = 0.1, size = 10)\n  X2 = rbinom(1, prob = 0.7, size = 3)\n  res[i] = ifelse(X2&gt;X1,1,0)\n}\nprint(paste0(\"The proportion of times X2&gt;X1 is \", sum(res)/10,\"%\"))\n\n[1] \"The proportion of times X2&gt;X1 is 71.2%\"\n\n\nIndeed, we often observe that the value of \\(X_2\\) is greater than \\(X_1\\). However, is there a way to reach a conclusion directly without relying on experiments? Or perhaps, can we explain this phenomenon using mathematical language? We then need to introduce another important concept, the expected value of a random variable. The expected value of a random variable is very similar to the concept that you are very familiar with, that is average value. If I ask you how often do you go to IKSU per week? Then most likely, you will answer me that you go to IKSU, on average, 3 times per week, since the number of visiting IKSU (the largest sports center in Umeå) per week is not a certain number (you often go there 3 times, but not always, for example, you are sick or have an important exam to prepare). Let me show you the number of my IKSU visits in the last 10 weeks. \\[\n  3,5,3,2,2,4,5,3,3,4\n\\]\nWe all know that the average value can be \\[\n  \\frac{3+5+3+2+2+4+5+3+3+4}{10} = 3.4\n\\] Of course, it is a super easy calculation, but let’s have a close look at it. This calculation can be represented as \\[\n  \\frac{2 \\times 2 + 4\\times3 + 2\\times4 +2\\times 5 }{10} = 0.2 \\times 2 + 0.4 \\times 3 + 0.2 \\times 4 + 0.2 \\times 5 = 3.4\n\\]\nNotice that the decimal in front of each integer, the possible value, is the percentage of the corresponding value that happened in the last ten weeks. In the rational world, if you still remember it, the percentage is replaced by the probability. Therefore, the definition of expected value is defined as the weighted sum of all possible values, and the weights are the corresponding probabilities. In a mathematical notation, the expected value of a random variable is presented as \\[\n  \\text{E}(X) = \\sum_{k} k \\Pr (X = k)\n\\] We can see that the expected value of a binary distributed random variable and a binomial distributed random variable is \\(p\\) and \\(Np\\) respectively. It is a good exercise to verify it. Now, we can turn back to the question at the beginning. By simple calculation, we can see that \\[\n  \\text{E}(X_2) = 3\\times0.7 = 2.1 &gt; \\text{E}(X_1) = 10\\times0.1 = 1\n\\] The expected value satisfies linearity. Suppose \\(a\\) and \\(b\\) are constant numbers and \\(X\\) is a random variable, then \\(\\text{E}(aX+b) = a \\text{E}(X) + b\\). In other words, linearity means the expectation operator and the linear operator (scalar multiplication and addition) are exchangeable, i.e. \\[\n  \\text{E} \\left(\\sum_{i=1}^n a_iX_i\\right)  = \\sum_{i=1}^n  a_i\\text{E}(X_i)\n\\]"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_3.html#expected-value",
    "href": "Courses/c_mlwr1_2024/l3/l3_3.html#expected-value",
    "title": "3.3 Characteristic Values",
    "section": "",
    "text": "We have introduced the most basic elements of probability theory, namely random variables and their distributions. Now, I have a question to you. Suppose we have two binomial distributed random variables,  \\(X_1 \\sim \\text{Bin}(10, 0.1)\\)  and  \\(X_2 \\sim \\text{Bin}(3, 0.7)\\) , can we compare them? (You can hover your cursor on \\(X_1\\) and \\(X_2\\) to get the meaning of them.) Well, the two random variables have different potential outcome, for \\(X_1\\), you might get a integer from \\(0\\) to \\(10\\), but only three possible values, \\(0, 1, 2, 3\\) for \\(X_2\\). Apparently, it is not comparable. Then can we compare their observed values? It doesn’t seem that simple neither. One is an experiment with a low success rate conducted 10 times, while the other has a high success rate but we only conduct it 3 times, so it’s hard to say which will have a higher number of successes. Let’s explore the answer in a straightforward way by actually generating two random numbers from their respective distributions and comparing them.\nX1 = rbinom(1, prob = 0.1, size = 10)\nX2 = rbinom(1, prob = 0.7, size = 3)\nX2&gt;X1\nIf you run the code above multiple times, you’ll get both TRUE and FALSE, but you may feel that the realization of \\(X_2\\) is often higher than the realization of \\(X_1\\). To verify our intuition, we can repeatedly run the code above and record the result each time. By doing this 1000 times, we can see the percentage of cases where the value of \\(X_2\\) is greater than the value of \\(X_1\\).\n\nres = numeric(1000)\nfor(i in 1:1000){\n  X1 = rbinom(1, prob = 0.1, size = 10)\n  X2 = rbinom(1, prob = 0.7, size = 3)\n  res[i] = ifelse(X2&gt;X1,1,0)\n}\nprint(paste0(\"The proportion of times X2&gt;X1 is \", sum(res)/10,\"%\"))\n\n[1] \"The proportion of times X2&gt;X1 is 71.2%\"\n\n\nIndeed, we often observe that the value of \\(X_2\\) is greater than \\(X_1\\). However, is there a way to reach a conclusion directly without relying on experiments? Or perhaps, can we explain this phenomenon using mathematical language? We then need to introduce another important concept, the expected value of a random variable. The expected value of a random variable is very similar to the concept that you are very familiar with, that is average value. If I ask you how often do you go to IKSU per week? Then most likely, you will answer me that you go to IKSU, on average, 3 times per week, since the number of visiting IKSU (the largest sports center in Umeå) per week is not a certain number (you often go there 3 times, but not always, for example, you are sick or have an important exam to prepare). Let me show you the number of my IKSU visits in the last 10 weeks. \\[\n  3,5,3,2,2,4,5,3,3,4\n\\]\nWe all know that the average value can be \\[\n  \\frac{3+5+3+2+2+4+5+3+3+4}{10} = 3.4\n\\] Of course, it is a super easy calculation, but let’s have a close look at it. This calculation can be represented as \\[\n  \\frac{2 \\times 2 + 4\\times3 + 2\\times4 +2\\times 5 }{10} = 0.2 \\times 2 + 0.4 \\times 3 + 0.2 \\times 4 + 0.2 \\times 5 = 3.4\n\\]\nNotice that the decimal in front of each integer, the possible value, is the percentage of the corresponding value that happened in the last ten weeks. In the rational world, if you still remember it, the percentage is replaced by the probability. Therefore, the definition of expected value is defined as the weighted sum of all possible values, and the weights are the corresponding probabilities. In a mathematical notation, the expected value of a random variable is presented as \\[\n  \\text{E}(X) = \\sum_{k} k \\Pr (X = k)\n\\] We can see that the expected value of a binary distributed random variable and a binomial distributed random variable is \\(p\\) and \\(Np\\) respectively. It is a good exercise to verify it. Now, we can turn back to the question at the beginning. By simple calculation, we can see that \\[\n  \\text{E}(X_2) = 3\\times0.7 = 2.1 &gt; \\text{E}(X_1) = 10\\times0.1 = 1\n\\] The expected value satisfies linearity. Suppose \\(a\\) and \\(b\\) are constant numbers and \\(X\\) is a random variable, then \\(\\text{E}(aX+b) = a \\text{E}(X) + b\\). In other words, linearity means the expectation operator and the linear operator (scalar multiplication and addition) are exchangeable, i.e. \\[\n  \\text{E} \\left(\\sum_{i=1}^n a_iX_i\\right)  = \\sum_{i=1}^n  a_i\\text{E}(X_i)\n\\]"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_3.html#variance",
    "href": "Courses/c_mlwr1_2024/l3/l3_3.html#variance",
    "title": "3.3 Characteristic Values",
    "section": "3.3.2 Variance",
    "text": "3.3.2 Variance\nThe expected value can help us determine the size of the “common” value of a random variable so that we can compare two random variables. One can also compare two random variables from another dimension, which is “value stability”. For example, we have two coins, one is even and the other is so uneven that there is a very high probability, 90%, of getting Heads. Then imagine that if we flip two coins repeatedly, we will get many heads for the uneven coin and occasionally get Tails; but for the even coin, we will get the same number of heads and tails with high probability. From the perspective of taking values, the values of uneven coins are very stable, while those of uniform coins are not. The stability of a random also refers to variation. High variation means low stability. The two things can be quantified by the variance.\nThe variance of a random variable \\(X\\) is defined as \\[\n  \\text{Var}(X) = \\text{E}(X - \\text{E}(X))^2\n\\]\nThis formula is very intuitive. First, we calculate the most frequently occurring value of this random variable, \\(\\text{E}(X)\\), and then compare it with any \\(X\\) by taking the difference. Finally, we calculate the average of the squares of this difference. If this random variable has stable values, then the value of \\(X\\) should often stay around the expected value, therefore \\(X - \\text{E}(X)\\) will generally be very small; conversely, if it varies significantly, it will be generally large, which aligns perfectly with our initial intention.\nBased on this definition, one can easily verify the variance of a binary distributed random variable and a binomial distributed random variable is \\(p(1-p)\\) and \\(np(1-p)\\) respectively. In the example above, the variance of the even coin is \\(0.25\\), but the uneven is \\(0.09\\).\nDifferent from the expected value, variance doesn’t satisfy the linearity, i.e. the variance operator and the linear operator are not exchangeable. However, it satisfies the following rules, \\[\n  \\text{Var}(aX+b) = a^2\\text{Var}(X)  \n\\]\nBased on the results above, we can easily find that a special linear combination, \\(\\left( X-\\text{E}(X) \\right)/\\sqrt{\\text{Var}(X)}\\), produces a standardized random variable, i.e. mean zero and variance 1.\n\nPrevious page | Lecture 3 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_1.html",
    "href": "Courses/c_mlwr1_2024/l3/l3_1.html",
    "title": "3.1 From Frequence to Probability",
    "section": "",
    "text": "Early on people realized that there are many things in the real world where the outcome is uncertain, such as one doesn’t always get a head when flipping a coin, or doesn’t always get a certain number when throwing a dice, and so they refer to this kind of event as random event. At the same time, mathematicians also had found that the percentage of getting heads always seems to be close to \\(0.5\\) when they repeatedly flip an even coin. The following r codes simulate this process:\n\n# randomly generate 5000 numbers taking values 0 or 1 and save in x\nset.seed(8312)\nx = rbinom(1000,1,0.5)\n# calculate the cumulated ratio\nratio = cumsum(x)/(1:1000)\nplot(ratio, type = \"l\")\nabline(h = 0.5, col = \"red\")\n\n\n\n\nThe plot depicts a phenomenon that as the number of coin tosses increases, the ratio gets closer and closer to 0.5. The x-axis is the number of coin flipping, and the y-axis is the ratio of the number of heads and the number of coin flipping. BTW, this is a neat solution to one of esercises in Lab 1.\n\n\n\n\n0.5, that is 1/2, has different meanings for its numerator and denominator; the denominator is the number of all possible outcomes (Head or Tail), while the numerator is the number of outcomes included in this event. Thus, mathematicians defined the first probability model in which the possibility of a random event occurring can be quantified by the ratio of the number of outcomes associated with the event to the number of all possible ones. Meantime, we refer to this measure of possibility as the probability of an event.\n\\[\n\\Pr \\left( \\text{Event} \\right) = \\frac{\\text{number of outcomes associated with the event}}{\\text{total number of all possible outcomes}}\n\\] In the coin example, all possible outcomes are \\(H\\) and \\(T\\), and the outcome associated with the event, getting a head, is \\(H\\). So, the probability that get a head when flip a coin is \\[\n  \\Pr(\\text{Get a head when flip a coin}) = \\frac{1}{2}\n\\] Since the number of outcomes associated with the event is always less than the total number of all possible outcomes, the probability defined by this model is always a number between 0 and 1, which fits our intuitions.\nBased on this model, one can easily quantify the possibility of an event if one gets the number 3 when throwing a dice as \\(1/6\\). Before providing more examples, I have to emphasize that probability is a mathematical concept that only exists in our rational world. That is, you can only get a number which is 3 or not 3 after throwing a dice. Also, even if you throw a die over and over again for the rest of your life, the number of times you get a 3 as a percentage of your total throws will only be very close to \\(1/6\\).\n( NE ) Let’s explore some more complex examples. First, what is the probability that you get a number less than 5 by throwing a dice? Using the model above, the total number of outcomes of throwing a dice is 6 and there are 4 potential outcomes all satisfy this condition, therefore this probability is \\(2/3\\). Second, what is the probability that you get 2 heads after flipping a coin 6 times? Again, the total number of possible outcomes is \\(2^6\\) and the number of outcomes satisfying this condition is \\(15\\), therefore the probability of this event is \\(0.234375\\). Obviously, the second example is harder than all the examples before, and you probably need some knowledge of permutations and combinations to understand the meaning of numbers \\(2^6\\) and \\(15\\). However, this is not the main purpose here. Also, calculating such numbers is not essential to understanding the probability model. Of course, if you’re interested in these types of questions, consider solving some problems from a book on probability theory, which is a good mental exercise. For example, you might think carefully about why a full house can beat a flush in Poker? (ToDo4Sia: Write a note discussing how you teach your son about permutations and combinations.)\n\n\n\nA full house can beat a flush in Poker, why?\n\n\n\nPrevious page | Lecture 3 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_2.html",
    "href": "Courses/c_mlwr1_2024/l3/l3_2.html",
    "title": "3.2 Random Variable and Distribution",
    "section": "",
    "text": "Discussing the probability notion for events can often be cumbersome. For example, in the coin example, we have to write texts to represent it. To simplify the expression, we can use a capital letter \\(X\\) to represent the result of flipping a coin. It has two possible values, \\(0\\) and \\(1\\) which indicate getting a Tail and a Head respectively. Then the probability of that event can be represented as \\[\n  \\Pr(X = 1)\n\\] This letter \\(X\\) is so powerful that it encompasses not only the two possible outcomes of one flipping but also represents all potential results each time you flip this coin. We call this letter \\(X\\) as a random variable which can be understood as a description of the results of a certain experiment. Conventionally, it is usually presented by a uppercase letter and we use lowercase letter, e.g. \\(x\\), to represent its realization or observation.\nNote: the mathematical definition of a random variable goes beyond this and is much more profound; however, this understanding of random variables is sufficient for practical applications.\nIf the role of a random variable were merely to use a symbol to represent an experimental outcome, it wouldn’t be nearly as compelling. Its essence lies in its distribution. Through the distribution of a random variable, we can abstract the common features of a wide range of random events. For example, consider \\(X\\) in the context of a coin toss. If we specify that \\(X\\) takes the value 1 with a fixed probability \\(\\pi\\) and 0 with a probability of \\(1 - \\pi\\), we obtain a random variable with a specific distribution, commonly referred to as a binary distribution or Bernoulli distribution random variable. It can be denoted as \\(X \\sim \\text{Ber}(\\pi)\\), and its distribution can be simply described as the evaluation of the probability for each possible outcome, i.e.\n\n\n\n\n\nThis information of distribution also can be represented by a formula,\n\\[\n  \\Pr(X = k) = \\pi^k(1-\\pi)^{1-k}\n\\] where the possible value of \\(k\\) is 0 and 1, and \\(0&lt;\\pi&lt;1\\). It is called probability mass function (p.m.f).\nIn this way, the symbol \\(X\\) becomes much more powerful. The Random variable and its distribution can help us get away from the whole coin-flip thing. The symbol \\(X\\) is elevated, and it becomes a probabilistic model. It can not only represent the random experiment of flipping a coin but also depict the probability of randomly selecting a man in Umeå city center, or describe the incidence of a certain disease within a specific population over a certain time period in a given region.\nAnother example: One also can define a random variable \\(Y\\) denotes the result of throwing a dice, then it has 6 possible values, \\(1\\) to \\(6\\). Since the probabilities of getting all possible values are equal, \\(1/6\\), it is called the discrete uniform distribution. ( PA ) The  p.m.f  is \\[\n  \\Pr(Y = k) = 1/6\n\\] where \\(k = 1,2,\\dots,6\\). There aren’t necessarily only six possible outcomes; you can increase the number of possible values, allowing this probabilistic model to cover a broader range of random phenomena.\nMore possibilities, for example, one can use \\(Z\\) to denote a random variable that presents the number of accidents in a certain time period, for example, the number of traffic accidents in certain area in one month. In this case, the possible values should be \\(1,2,3,4,\\dots\\). From this phenomenon, we can abstract the Poisson distribution which models the number of times an event occurs in a fixed interval of time or space, under the conditions. The  p.m.f  is leave to you to explore.\nRandom variable with certain distribution also can help us simplify the calculation of the probability of an event. Let’s see the next example, Binomial distribution, denoted as \\(X \\sim \\text{Bin}(N,p)\\). The random variable \\(X\\) presents the number of positive results among \\(N\\) independent binary results experiment. The probability that getting a positive result in one experiment is \\(p\\). Obviously, the possible values of \\(X\\) are integers from \\(0\\) to \\(N\\), and the distribution can be represented as \\[\n  \\Pr(X = k) = \\frac{N!}{k!(N-k)!} p^{k}(1-p)^{N-k}\n\\] where the exclamation sign denotes factorial, i.e. \\(N!=N(N-1)(N-2)\\dots1\\). It is easy to see the probability of the relatively complicated random event discussed before, you get \\(2\\) heads after flipping a coin \\(6\\) times, which can be calculated by this distribution. We can define a random variables the number of getting heads after flipping a coin \\(6\\) times, so \\(X \\sim \\text{Bin}(6,0.5)\\). Replacing \\(N=6\\), \\(p=0.5\\), and \\(k=2\\) in the formula above, one can easily verify \\(\\Pr(X=2) = 0.234375\\).\n\nPrevious page: | Lecture 3 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_5.html",
    "href": "Courses/c_mlwr1_2024/l3/l3_5.html",
    "title": "3.5 Continuous Random Variables",
    "section": "",
    "text": "Next, we will consider a more challenging concept, continuous random variable. As you may notice, all the random events that we are considering can be represented by categorical outcomes. For example, flipping a coin has two outcomes, throwing a dice has 6 outcomes, and so on. For this kind of random event, it is sufficient to consider random variables only taking integer values. Thus we refer to the random variables discussed as discrete random variables. In practice, however, there are many other random events whose outcomes are not categorical but continuous values, for example, the temperature, the height of adult males, and so on. Therefore we need another type of random variable, a continuous variable."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_5.html#continuous-distribution",
    "href": "Courses/c_mlwr1_2024/l3/l3_5.html#continuous-distribution",
    "title": "3.5 Continuous Random Variables",
    "section": "3.5.1 Continuous distribution",
    "text": "3.5.1 Continuous distribution\nContinuous random variables are not difficult to understand, they are nothing more than random variables that take real numbers, but the problem is how to describe their distribution. Again, let us abstract mathematical concepts from reality. Let’s consider such a background problem, assuming that I have height data for all boys in middle school in Sweden. Height is obviously not a categorical variable, but we can still use grouping to describe the distribution of height from a discrete perspective. Specifically, we can evenly divide the possible range of height into several groups, and then calculate the percentage of the number of people in each group to the total number of people. Yes, if you are familiar with basic statistics, you can tell that this is a histogram at a glance.\n\n\n\n\n\nSuch an approach has obvious flaws. For example, on the left-top of the plot, it is difficult for us to distinguish the probability of height being less than 175 and greater than 170 because these two values are combined into one group. How to do it? Very simple, we can split each large group into two small groups, of course, you also need to include more boys into the data set and then calculate the frequency of each group to represent the distribution of height, for example, on the right-top of the plot. If we still cannot distinguish the above probability, we can continue to split each group into two groups. Doing this we can see that the histogram is more detailed. If we have a sufficient large data set, we can continue to subdivide the height group and know the probability that we can distinguish the above two events. Suppose we put “all” the boys in middle school into this histogram, and each group can be subdivided infinitely. We can imagine that the upper edge of the histogram will be a smooth curve. We call this smooth curve the probability density function (p.d.f) and denote it as \\(f(x)\\). A valid  p.d.f  has to inherit two conditions from the  p.m.f  of a discrete random. First, the density value must be positive, \\(f(x) &gt; 0\\), and the integral on the whole domain should be 1, \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\). With this function, we can calculate the probability of many events, as well as the expectation and variance. \\[\n  \\Pr(X&lt;b) = \\int_{-\\infty}^bf(x)dx\n\\] \\[\n  \\text{E}(X) =  \\int_{-\\infty}^{\\infty} xf(x) dx\n\\]\nOne can compare the formula above with the expected value of a discrete random variable, \\(\\text{E}(X) = \\sum_k k\\Pr(x=k)\\). You can see similar patterns, they all are the “sum” of all possible values times the corresponding probability or density values. Keep in mind that the integral symbol is an elongated S which indicates “sum”.\nSo far, we have only specified the basic conditions for a function to be a p.d.f , but the exact form of this function depends on the continuous distribution it represents. Next, we will learn about one of the most common continuous distributions: the Normal distribution."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_5.html#normal-gaussian-distribution",
    "href": "Courses/c_mlwr1_2024/l3/l3_5.html#normal-gaussian-distribution",
    "title": "3.5 Continuous Random Variables",
    "section": "3.5.2 Normal (Gaussian) distribution",
    "text": "3.5.2 Normal (Gaussian) distribution\nA continuous random variable is Normally distributed, \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), if its density function is\n\\[\n  f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} } e^{- \\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma}\\right)^2 }\n\\] The normal distribution is determined by two parameters, location parameter \\(\\mu\\) and shape parameter \\(\\sigma^2\\). Density functions of normal distribution with different parameters are displayed in the following picture.\n\n\n\n\n\nLHS: Normal distribution with fixed shape parameter (sigma = 1) and different location parameters, orange: mu = -4, blue: mu = 0, red: mu = 2. RHS: Normal distribution with fixed location parameter (mu = 0) and different shape parameters, orange: sigma = 3, blue: sigma = 1, red: sigma = 0.5.\n\n\n\n\nNow, I have two questions to you.\n\nWe have studied both discrete random variable and continuous random variable, their distribution of possible values can be presented by p.m.f and p.d.f respectively. The value of a p.m.f is just the probability when the random variable taking this value. However, what is the meaning of the value of a p.d.f?\nWhat is the essence of the Normal distribution? In simpler terms, how would you introduce the concept of the Normal distribution to a middle school student?\n\nThink about them and we will come back to them later on.\n\nPrevious page | Lecture 3 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3.html#two-dependent-random-variables",
    "href": "Courses/c_mlwr1_2024/l3/l3.html#two-dependent-random-variables",
    "title": "Lecture 3: Basic Knowledge in Probability Theory",
    "section": "3.4 Two Dependent Random Variables",
    "text": "3.4 Two Dependent Random Variables\n\n3.4.1 Sum rule and product rule\nLet’s think about a question like this. The basic conditions are presented in the following picture. Suppose we want to randomly pick a box and then randomly take out a fruit from it, then what is the probability that the fruit taken out is an apple? We can randomly pick the box by throwing a dice. If we get a number less than 5 then we choose the red box, or we choose the blue box.\n\n\n\nBox and Furits Problem: there are two boxes, red and blue, and the red box contains two apples and six oranges, while the blue box contains three apples and one orange.\n\n\nThis example is a bit more complicated than the previous because there are two randomized actions involved here. The result of this action involves a combination of the color of the box and the type of fruits. Let’s start by considering the probability that the red box is drawn while an apple is picked up. Again, we can use the previous formula to calculate this probability. There are six numbers corresponding to the dice, and the number of fruits inside the red box is 8, so all the possibilities are \\(6 \\times 8\\). But there are only numbers 1 through 4 and two apples, so there are only \\(4 \\times 2\\) possibilities that qualify. So the probability is \\[\n  \\frac{4\\times2}{6\\times8} = \\frac{4}{6} \\times \\frac{2}{8} = \\frac{1}{6}\n\\] It is easy to see that \\(4/6\\) is the probability of getting a number less than \\(5\\), i.e. the red box is selected. But what is the meaning of the second part, \\(2/8\\)? Since \\(8\\) is the number of fruits and \\(2\\) is the number of apples in the red box, it can be understood as the probability of getting an apple when the red box was selected. We refer to this probability as conditional probability and present it as \\(\\Pr(\\text{Apple} | \\text{Red}) = 2/8\\). This discussion can be summarized as \\[\n  \\Pr ( \\text{Red AND Apple} ) = \\Pr( \\text{Apple} | \\text{Red} ) \\Pr ( \\text{Red} )\n\\] We can also easily get the probability of getting an apple under the other possibility, i.e. \\[\n  \\Pr(\\text{Blue AND Apple}) = \\Pr(\\text{Apple} | \\text{Blue} ) \\Pr( \\text{Blue} ) = \\frac{3}{4} \\times \\frac{2}{6} = \\frac{1}{4}\n\\] “AND” corresponds to the “product rule”. \\[\n  \\Pr(E_1 \\text{ AND } E_2) = \\Pr(E_1 | E_2)\\Pr(E_2) = \\Pr(E_2 | E_1)\\Pr(E_1)\n\\] Going back to our original question, what is the probability of getting an apple? This random event can be labeled as “Red AND Apple OR Blue AND Apple”. Here, we have the second rule, i.e. “sum rule”, when considering the “OR” operator between two events that don’t happen simultaneously. \\[\n  \\Pr(E_1 \\text{ OR } E_2) = \\Pr(E_1) + \\Pr(E_2)\n\\]\nBased on the sum rule, the probability of getting an apple is calculated as \\[\n  \\Pr( \\text{Apple} ) = \\Pr( \\text{Red AND Apple} ) + \\Pr( \\text{Blue AND Apple} ) = \\frac{5}{12}\n\\]\nRemark: we can compare it with the sum-product rule in permutation and combinations. When there are different types of solutions for one thing, the total number of possible solutions is the sum of the number of possible solutions for each type. When there are different steps in doing one thing, the total number of solutions is the product of the number of possible solutions in each step.\n\n\n3.4.2 Joint distribution and marginal distribution\nSimilarly, you can verify the probabilities when orange is considered. If we use random variables to present the events, for example, the random variable \\(X\\) presents the box selected, \\(1\\) indicates red, and \\(0\\) indicates blue; the random variable \\(Y\\) presents the fruit drew, then the following table is called the joint distribution of two random variables.\n\n\n\n\n\nThe last column is called the marginal distribution of random variable \\(Y\\), and the last row is the marginal distribution of random variable \\(X\\).\n\n\n3.4.3 Posterior and Prior Probabilities\nWith the example above, the last interesting question is “What is the probability we chose the blue box if we get an orange?” First of all, it is a conditional probability, \\(\\Pr(X = 0 | Y = 0)\\). According to the product rule, we know that this conditional probability is the ratio between \\(\\Pr(X=0 \\text{ and } Y=0)\\) and \\(\\Pr(Y=0)\\). The first second has been calculated before and that is \\(7/12\\). Well, the first can be calculated by product rule again, i.e. \\(\\Pr( Y = 0 | X = 0 ) \\Pr(X=0)\\). Summarize, \\[\n  \\Pr(X = 0 | Y = 0) = \\frac{\\Pr( Y = 0 | X = 0 ) \\Pr(X=0)}{\\Pr(Y=0)}\n\\]\n\\(\\Pr(X=0|Y=0)\\) is different from the conditional probability in the first question \\(\\Pr(Y=0|X=0)\\). This probability is referred to as the posterior probability in the sense that we use the observation in the second step to update the probability of the first step. Correspondingly, the probability of drawing a blue box at the first step is called prior probability. The formula above is well known as Bayes formula.\n\n\n3.4.4 Statistically independent\nFrom the joint distribution, the probability of drawing an apple \\(\\Pr(Y = 1)\\) is \\(5/12\\). It is different from the probability of drawing an apple under the condition that the red box was selected, \\(\\Pr(Y=1 | X = 1) = 2/8\\). This fact implies that the value of random variable \\(Y\\) depends on the value of \\(X\\), or random variable \\(X\\) and \\(Y\\) are dependent. If we add 1 apple and 11 oranges to the blue box, then \\(\\Pr(Y=1) = \\Pr(Y=1 | X = 1)\\). In this case, the value of random variable \\(Y\\) doesn’t depend on the value of \\(X\\), i.e. they are independent.\n\n\n3.4.5 Covariance and Correlation\nFor two random variables \\(X\\) and \\(Y\\), we can use covariance to quantify the degree of association between two random variables. The covariance is defined as \\[\n  \\text{Cov}(X, Y) = \\text{E}(X-\\text{E}(X))(Y-\\text{E}(Y))\n\\] The mean and variance of a weighted sum (linear combination) of two random variables are \\[\n  \\text{E}(aX+bY) = a\\text{E}(X) + b\\text{E}(Y)\n\\] and \\[\n  \\text{Var}(aX+bY) = a^2\\text{Var}(X) + 2ab\\text{Cov}(X,Y) + b^2\\text{Var}(Y)\n\\] respectively.\nAnother characteristic value that quantifies the correlation between two variables is correlation. It is simply normalized covariance, i.e.\n\\[\n  \\rho_{x,y} = \\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X)} \\cdot \\sqrt{\\text{Var}(Y)}}\n\\] When the two variable uncorrelated, \\(\\text{Cov}(X, Y) = 0\\), therefore \\(\\rho_{x,y} = 0\\). Another two extreme cases are \\(X = Y\\), i.e. two random variables are the same, and \\(X = -Y\\). In these cases, \\(\\text{Cov}(X, \\pm Y) = \\pm \\text{Cov}(X, X) = \\pm \\text{Var}(X)\\), therefore, \\(\\rho_{x,y} = \\pm 1\\). Thus, different from covariance, the correlation is a number between \\([-1, 1]\\) due to the normalization. So, it is more comparable than covariance and people often use it to quantify the association between variables."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4.html#gaussian-discriminant-analysis",
    "href": "Courses/c_mlwr1_2024/l4/l4.html#gaussian-discriminant-analysis",
    "title": "Lecture 4: Gaussian Discriminant Analysis",
    "section": "4.2 Gaussian Discriminant Analysis",
    "text": "4.2 Gaussian Discriminant Analysis"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4.html#lda-function-in-mass-package",
    "href": "Courses/c_mlwr1_2024/l4/l4.html#lda-function-in-mass-package",
    "title": "Lecture 4: Gaussian Discriminant Analysis",
    "section": "4.3 ‘lda’ function in ‘MASS’ package",
    "text": "4.3 ‘lda’ function in ‘MASS’ package"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4.html#model-evaluations",
    "href": "Courses/c_mlwr1_2024/l4/l4.html#model-evaluations",
    "title": "Lecture 4: Gaussian Discriminant Analysis",
    "section": "4.4 Model Evaluations",
    "text": "4.4 Model Evaluations\nThe evaluation of a classification model is crucial to assess its performance and ensure its effectiveness in real-world applications. Accuracy is the first metric that comes to mind when evaluating a model, but it is not sufficient. For example, in a study with 100 observations, where 95 are healthy individuals and 5 are cancer patients, a model that simply classifies every observation as healthy would achieve 95% accuracy. However, this would fail to identify the cancer patients, making the model useless for the task at hand. Next, we will explore some model evaluation methods to better understand and measure the performance of classification models.\n\n4.4.1 Confusion Matrix and related statistics\nA confusion matrix is a powerful tool used to evaluate the performance of a classification model. It shows the counts of actual versus predicted classifications, providing insights into how well the model performs across different classes.\nA general confusion matrix for a binary classification problem has the following form:\n\n\n\n\n\n\n\n\n\nPredicted Positive (P)\nPredicted Negative (N)\n\n\n\n\nActual Positive (P)\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative (N)\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nIn the matrix, the rows represent the actual class of the observations in the data set, i.e. the true labels. The first row (Actual Positive) contains all cases that actually belong to the positive class (e.g., cancer patients), while, the second row contains all cases that actually belong to the negative class (e.g., healthy individuals).\nThe columns represent the predicted class according to the model. The first column (Predicted Positive) contains all cases that the model predicted to be positive (e.g., predicted cancer), and the second column (Predicted Negative) contains all cases that the model predicted to be negative (e.g., predicted healthy).\nWith this structure, each cell in the matrix contains different meaning:\n\nTP: The number of correct predictions where the actual class is positive and the model predicted positive.\nFP: The number of incorrect predictions where the actual class is negative but the model predicted positive.\nFN: The number of incorrect predictions where the actual class is positive but the model predicted negative.\nTN: The number of correct predictions where the actual class is negative and the model predicted negative.\n\nThese metrics provide a comprehensive way to assess the performance of a classification model, especially when dealing with imbalanced data sets. For example, the confusion matrix of the useless classifier mentioned above is displayed below.\n\n\n\n\nPredicted Cancer\nPredicted Healthy\n\n\n\n\nActual Cancer\n0\n5\n\n\nActual Healthy\n0\n95\n\n\n\nIn this example, in addition to accuracy, we can further calculate other statistics to comprehensively evaluate the performance of this classifier.\n\nSensitivity: (True positive rate) The proportion of true positive predictions out of all actual positive cases. This statistic indicates how effectively the classifier identifies the cases of interest, showing how sensitive it is to detecting positive instances.\n\\[\n\\text{Sensitivity} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{0}{0+5} = 0\n\\]\nSpecificity: (True negative rate) The proportion of true negative predictions out of all actual negative cases. \\[\n\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{95}{0+95} = 1\n\\] In the lazy classifier example, although this lazy classifier has very extremely high specificity, 100%, and high accuracy, 95%, we can’t say it is good at all as the extremely low sensitivity, 0. So, people usually simultaneously use the three statistics, i.e. accuracy, sensitivity, and specificity, to evaluate the performance of a classifier.\nPrecision: Sometime, people are also interested in the quality of positive predictions, then the proportion of true positive predictions out of all predicted positive cases, i.e. precision, is used. \\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\\] In the lazy classifier example, it is an extreme cases and precision is not defined as no case is predicted as positive. Mathematically, we also arrive at the same conclusion. Since there are no positive predictions, 0 appears in the denominator, and therefore this ratio is not defined.\n\n\n\n4.4.2 More Choices\nIt is often difficult and inconvenient to compare different things by considering several dimensions at once. The best approach is to find a statistic that can simultaneously evaluate a classifier from multiple perspectives.\nF-score: it is a statistic that combines precision and sensitivity into a single measure to evaluate the performance of a classifier, especially in situations where both false positives and false negatives are important. Essentially, it is the harmonic mean of precision and recall, giving a balance between the two metrics.\n\\[\n  \\text{F-score} = 2 \\times \\frac{\\text{Precision} \\times \\text{sensitivity}}{\\text{Precision} + \\text{sensitivity}}\n\\] F-score ranges from 0 to 1, and it indicates the perfect precision and sensitivity (best performance) for a classifier when it is \\(1\\), but worst performance when it is \\(0\\) With the same example above, suppose we have a classifier always predict a person as a cancer patient, then this classifier has perfect sensitivity but very low precision which is 0.05. The F-score is \\(2\\times\\frac{0.05 \\times 1}{0.05 + 1} = 0.095\\). If someone is willing to use this classifier, they must have ignored the negative effects of misclassifying a healthy person as a cancer patient.\nCohen Kappa Statistics: it is another option that can be used to comprehensively evaluate a classifier. Essentially, it is used to measure the agreement between two raters (classifiers). For example, suppose there are two classifiers both classify 100 cases. If the two classifiers agree with each other, then we can get the following matrix that is similar to the idea of confusion matrix.\n\n\n\n\n\n\n\n\n\nClassifier 2: Positive\nClassifier 2: Negative\n\n\n\n\nClassifier 1: Positive\n30\n0\n\n\nClassifier 1: Negative\n0\n70\n\n\n\nIgnoring whether they are good classifiers, we can say that the two classifiers have the exactly same predictions, in another word, the two classifiers agree with each other. Let’s see another example,\n\n\n\n\n\n\n\n\n\nClassifier 2: Positive\nClassifier 2: Negative\n\n\n\n\nClassifier 1: Positive\n30\n10\n\n\nClassifier 1: Negative\n5\n55\n\n\n\nIn this case, apparently the two classifiers don’t have exactly the same predictions, since there are 5 cases that are predicted as negative by classifier 1 but positive by classifier 2, also 10 disagreements are in an opposite way. However, they still show a certain level of agreement. So, the question is can we design a statistic to quantify the agreement. Of course, the answer is Cohen Kappa statistic. Before showing you the formula of Kappa statistic, let’s clarify one thing. If we set ‘Classifier 1’ as the classifier you want to evaluate, and ‘Classifier 2’ as the ground truth, then this statistic will measure the agreement between your model and the ground truth, and the matrix becomes the confusion matrix.\nNext, let’s have a look at the calculations of this statistic with the notations in a confusion matrix. \\[\n  \\kappa = \\frac{P_o - P_e}{1 - P_e}\n\\]\n\n\\(P_o\\) is the observed agreement: the proportion of times the two raters agree, i.e. the accuracy \\[\nP_o = \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\n\\(P_e\\) is the expected agreement: the proportion of times the two raters would be expected to agree by chance \\[\nP_e = \\left( \\frac{(TP + FP)(TP + FN)}{(TP + TN + FP + FN)^2} \\right) + \\left( \\frac{(TN + FP)(TN + FN)}{(TP + TN + FP + FN)^2} \\right)\n\\] In general, we can use the following table as reference to evaluate a classifier. Here is the information in table format:\n\n\n\n\nKappa (κ) Value\nInterpretation\n\n\n\n\nκ ≥ 0.81\nAlmost perfect agreement\n\n\n0.61 ≤ κ &lt; 0.80\nSubstantial agreement\n\n\n0.41 ≤ κ &lt; 0.60\nModerate agreement\n\n\n0.21 ≤ κ &lt; 0.40\nFair agreement\n\n\nκ ≤ 0.20\nSlight agreement\n\n\nNegative\nWorse than random chance\n\n\n\nLet’s go back to the previous example:\n\n\n\n\n\n\n\n\n\nClassifier 2: Positive\nClassifier 2: Negative\n\n\n\n\nClassifier 1: Positive\n30 (TP)\n10 (FP)\n\n\nClassifier 1: Negative\n5 (FN)\n55 (TN)\n\n\n\nIn this case, \\(\\kappa = 0.68\\), and it suggests a substantial agreement between the two classifiers. If the ‘classifier 2’ represents the ground truth, then \\(\\kappa\\) indicates that ‘classifier 1’ is a rather good classifier.\n\nLecture 4 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_home.html#lecture-notes",
    "href": "Courses/c_mlwr1_2024/l4/l4_home.html#lecture-notes",
    "title": "Lecture 4: Gaussian Discrimination Analysis",
    "section": "Lecture notes:",
    "text": "Lecture notes:\n\nRead the integrated notes: here\nRead the pagination notes: here\nDownload the PDF notes here."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_home.html#reading-guidelines-you-can-read-the-book",
    "href": "Courses/c_mlwr1_2024/l4/l4_home.html#reading-guidelines-you-can-read-the-book",
    "title": "Lecture 4: Gaussian Discrimination Analysis",
    "section": "Reading Guidelines: You can read the book…",
    "text": "Reading Guidelines: You can read the book…\nFor lecture 4, it is recommended that you read the following sections in the textbook.\n\nChapter 4: Read sections 4.4 and subsections from 4.7.3 - 4.7.5"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4.html#basic-ideas",
    "href": "Courses/c_mlwr1_2024/l4/l4.html#basic-ideas",
    "title": "Lecture 4: Gaussian Discriminant Analysis",
    "section": "4.1 Basic Ideas",
    "text": "4.1 Basic Ideas\nLet me start with a motivating example. I have data on a guy’s height and weight: He is 170 cm tall and weigh 68 kg. Given that I know he is either Swedish or Chinese, which country do you think they are from? Do you have any ideas?\nLet’s analyze this problem.\n\nFirst, it’s clearly a classification problem—we want to make a judgment based on height and weight data.\nNext, essentially, we now have an observation, so we can measure the likelihood value of this observation. However, if we want to measure the likelihood value, we first need to find an appropriate model.\nThis problem is simple; the bivariate normal distribution is definitely a fit, but we still need specific parameters to determine this normal model.\nReturning to the problem itself: our premise is to make a judgment between Sweden and China. Clearly, we have two candidate models in front of us—the Swedish normal model and the Chinese normal model.\n\nNow we have a plan. We can set up two candidate models and obtain their parameters from each country’s statistical department—that is, two means and a 2D covariance matrix. Then, we use these two models to evaluate the observed likelihood values for this person. If the likelihood value calculated from the Chinese model is higher than that from the Swedish model, we guess that they are Chinese; otherwise, we guess they are Swedish.\nTo do so, I got the statistics for both models, that is the average height and weight are \\((179, 75)\\), the SDs are \\((5,4)\\), and the correlation is \\(0.6\\) in Sweden; for China, the average values are \\((170, 65)\\), SDs are \\((5.5,3)\\), and correlation is also \\(0.6\\). Next, I plot the contour graphs of density function for both model and find the position of this guy in R\n\nlibrary(mvtnorm)\nlibrary(ellipse)\n\n# my function for generating the contour map\ncontour.g &lt;- function(mu = c(0,0), Sigma, lwd=2, text = \"\"){\n  plot(0,0, xlim=c(mu[1]-2*sqrt(Sigma[1,1]),mu[1]+2*sqrt(Sigma[1,1])), \n       ylim=c(mu[2]-1*sqrt(Sigma[2,2]),mu[2]+1*sqrt(Sigma[2,2])), asp = 1,\n       xlab = \"\", ylab = \"\", main = text)\n  cv &lt;- c(0.01,0.05,0.1,0.25,0.5,0.75,0.9)\n  co &lt;- c(\"purple\",\"blue\",\"green\",\"yellow\",\"light blue\",\"orange\", \"red\")\n  for(i in 1:7){\n    points(ellipse(x = Sigma, centre = mu, level=cv[i], npoints=250), col=co[i], lwd=lwd, type = \"l\")\n  }\n}\n\n# model Sweden\nmu1 = c(179, 75)\nsigma1 = matrix(c(5^2, 0.6*5*4, 0.6*5*4, 4^2), byrow = T, 2)\n# model China\nmu2 = c(170, 65)\nsigma2 = matrix(c(5.5^2, 0.6*5.5*3, 0.6*5.5*3, 3^2), byrow = T, 2)\n\npar(mfrow = c(1,2))\ncontour.g(mu1, sigma1, text = \"Swedish Men\")\ntext(x = 170, y = 68, \"Sia\" ) # find my position\ncontour.g(mu2, sigma2, text = \"Chinese Men\")\ntext(x = 170, y = 68, \"Sia\" ) # find my position\n\n\n\n\n\n\n\n\nOops, it was me. Obviously, under the Chinese model, I’m already a bit overweight but still within the light blue likelihood level, while under the Swedish model, I’m somewhat on the margins. So, the final judgment is that this guy is Chinese. As you know, I am originally from China, so our idea works. Great!\nDisclaimer: Except for my personal data, all other data is fabricated purely for educational purposes, with no malice or discrimination intended."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4.html#implementation-in-r",
    "href": "Courses/c_mlwr1_2024/l4/l4.html#implementation-in-r",
    "title": "Lecture 4: Gaussian Discriminant Analysis",
    "section": "4.3 Implementation in R",
    "text": "4.3 Implementation in R\n\n4.3.4 More carefully\n\n\n4.3.5 Multiple labels\n\n\n4.3.1 R functions for GDA\n\n\n4.3.2 Fisher’s idea"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_1.html",
    "href": "Courses/c_mlwr1_2024/l4/l4_1.html",
    "title": "4.1 Basic Ideas",
    "section": "",
    "text": "Let me start with a motivating example. I have data on a guy’s height and weight: He is 170 cm tall and weigh 68 kg. Given that I know he is either Swedish or Chinese, which country do you think they are from? Do you have any ideas?\nLet’s analyze this problem.\n\nFirst, it’s clearly a classification problem—we want to make a judgment based on height and weight data.\nNext, essentially, we now have an observation, so we can measure the likelihood value of this observation. However, if we want to measure the likelihood value, we first need to find an appropriate model.\nThis problem is simple; the bivariate normal distribution is definitely a fit, but we still need specific parameters to determine this normal model.\nReturning to the problem itself: our premise is to make a judgment between Sweden and China. Clearly, we have two candidate models in front of us—the Swedish normal model and the Chinese normal model.\n\nNow we have a plan. We can set up two candidate models and obtain their parameters from each country’s statistical department—that is, two means and a 2D covariance matrix. Then, we use these two models to evaluate the observed likelihood values for this person. If the likelihood value calculated from the Chinese model is higher than that from the Swedish model, we guess that they are Chinese; otherwise, we guess they are Swedish.\nTo do so, I got the statistics for both models, that is the average height and weight are \\((179, 75)\\), the SDs are \\((5,4)\\), and the correlation is \\(0.6\\) in Sweden; for China, the average values are \\((170, 65)\\), SDs are \\((5.5,3)\\), and correlation is also \\(0.6\\). Next, I plot the contour graphs of density function for both model and find the position of this guy in R\n\nlibrary(mvtnorm)\nlibrary(ellipse)\n\n# my function for generating the contour map\ncontour.g &lt;- function(mu = c(0,0), Sigma, lwd=2, text = \"\"){\n  plot(0,0, xlim=c(mu[1]-2*sqrt(Sigma[1,1]),mu[1]+2*sqrt(Sigma[1,1])), \n       ylim=c(mu[2]-1*sqrt(Sigma[2,2]),mu[2]+1*sqrt(Sigma[2,2])), asp = 1,\n       xlab = \"\", ylab = \"\", main = text)\n  cv &lt;- c(0.01,0.05,0.1,0.25,0.5,0.75,0.9)\n  co &lt;- c(\"purple\",\"blue\",\"green\",\"yellow\",\"light blue\",\"orange\", \"red\")\n  for(i in 1:7){\n    points(ellipse(x = Sigma, centre = mu, level=cv[i], npoints=250), col=co[i], lwd=lwd, type = \"l\")\n  }\n}\n\n# model Sweden\nmu1 = c(179, 75)\nsigma1 = matrix(c(5^2, 0.6*5*4, 0.6*5*4, 4^2), byrow = T, 2)\n# model China\nmu2 = c(170, 65)\nsigma2 = matrix(c(5.5^2, 0.6*5.5*3, 0.6*5.5*3, 3^2), byrow = T, 2)\n\npar(mfrow = c(1,2))\ncontour.g(mu1, sigma1, text = \"Swedish Men\")\ntext(x = 170, y = 68, \"Sia\" ) # find my position\ncontour.g(mu2, sigma2, text = \"Chinese Men\")\ntext(x = 170, y = 68, \"Sia\" ) # find my position\n\n\n\n\n\n\n\n\nOops, it was me. Obviously, under the Chinese model, I’m already a bit overweight but still within the light blue likelihood level, while under the Swedish model, I’m somewhat on the margins. So, the final judgment is that this guy is Chinese. As you know, I am originally from China, so our idea works. Great!\nDisclaimer: Except for my personal data, all other data is fabricated purely for educational purposes, with no malice or discrimination intended.\n\nPrevious page | Lecture 4 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4.html#classifier-constructed-based-on-gaussian-model",
    "href": "Courses/c_mlwr1_2024/l4/l4.html#classifier-constructed-based-on-gaussian-model",
    "title": "Lecture 4: Gaussian Discriminant Analysis",
    "section": "4.2 Classifier constructed based on Gaussian model",
    "text": "4.2 Classifier constructed based on Gaussian model\nLet’s summarize the basic idea of the motivating example. Essentially, we propose two candidate models based on the labels, use these two models to evaluate the object we want to predict, and then make a judgment based on the likelihood evaluation results. The candidate models here are normal distributions. Remember? A probabilistic model is essentially a probability distribution.\n\n4.2.1 Gaussian Discriminan Analysis\nUp to this point, we have actually designed a function where the input consists of the two feature variables, height and weight, and the output is the prediction of the label. A classifier constructed based on this idea is called Gaussian Discriminant Analysis (GDA). Next, let’s describe this method using precise mathematical language.\nSuppose we have a target variable, \\(Y\\) which has \\(\\{-1, 1\\}\\) as possible values. We denote the feature variables as \\(\\textbf{X} = (X_1, X_2, \\dots, X_p)^{\\top}\\). Just emphasize that \\(\\textbf{X}\\) is not a specific observation, and it is just a random variable now. Then we assume that \\[\n  \\begin{matrix}\n    \\textbf{x}|y=1 & \\sim & \\mathcal{N}(\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1)\\\\\n    \\textbf{x}|y=-1 & \\sim & \\mathcal{N}(\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}_2)\n  \\end{matrix}\n\\] With this model, we make the prediction by comparing the likelihood values of a new case with respect to each distribution, i.e. a new case \\(\\textbf{x}_{\\text{new}}\\) is predicted as category \\(1\\) if \\[\n  f(\\textbf{x}_{new}; \\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1) &gt; f(\\textbf{x}_{new}; \\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}_2)\n\\] where \\(f\\) is the density function of multivariate Gaussian distribution, otherwise, the new case will be classified to another group.\n\n\n4.2.2 Linear or Nonlinear?\nIn Lecture 1, we explained the general form of a linear classifier, i.e., the decision boundary of the classifier is a line, a plane, or a hyperplane in higher dimensions. So far, our GDA is just an R function, kind of black box, so the question is: Is it a linear classifier? We are going to investigate it in this subsection.\nFirst, to understand the nature of the classifier’s decision boundary, it is best to express it using mathematical language. Review the decision rule presented in the previous subsection, it is very easy to see the decision boundary can be represented as \\[\n  \\ell = \\left\\{ \\textbf{x}: f(\\textbf{x}; \\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1) = f(\\textbf{x}; \\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}_2) \\right\\}\n\\] where \\(f_1\\) and \\(f_2\\) are p.d.f of normal distribution. While the notation above is in set theory language, it is not difficult to understand it. It means that, in set \\(\\ell\\), we collect all the points \\(\\textbf{x}\\) that satisfy the condition, \\(f(\\textbf{x}; \\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1) = f(\\textbf{x}; \\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}_2)\\), i.e. equal likelihood in a certain dimensional space.\nNext, I will do some experiments in R to investigate the properties the decision boundary. Obviously, the decision boundary is determined by the two density functions together. So, my plan is to assign different parameter values to the two density functions, then, using the contour graphs of the two density functions, find the points where their values are the same, and connect them. This way, we can roughly understand the shape of the decision boundary.\nBefore we begin, there’s one thing we need to clarify. We only need to focus on the effect of covariance matrix, because it controls the shape of the distribution, and the shape of the distribution determines the shape of the decision boundary.\n\n\nFirst, let’s check the simplest situation, that is the two groups all have the simplest covariance matrix, i.e.  \\[\n  \\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 =   \\left(\n  \\begin{matrix}\n    1 & 0 \\\\\n    0 & 1\n  \\end{matrix}\n  \\right)\n\\]\n\n\n\n\n\n\n\n\nFrom the animation above, we can see that if we connect all the intersection points of contours with the same color, it will roughly form a straight line. In this very special case, our decision boundary is linear. Now, let’s relax the conditions a bit so that the Gaussian model can cover more possibilities, but not too far—just a small step.\n\n\nLet’s assume that the two distributions have the same, but arbitrary, covariance matrix， i.e. \\[\n  \\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 =   \\left(\n  \\begin{matrix}\n    1 & 0.3 \\\\\n    0.3 & 0.5\n  \\end{matrix}\n  \\right)\n\\]\n\n\n\n\n\n\n\n\nWow, it is still a linear classifier. Now, let’s further relax the conditions to the most general case, where the two distributions have completely different and arbitrary covariance matrices.\n\n\n\\[\n  \\boldsymbol{\\Sigma}_1 =   \\left(\n  \\begin{matrix}\n    0.5 & 0.25 \\\\\n    0.25 & 1\n  \\end{matrix}\n  \\right)\n\\] \\[\n  \\boldsymbol{\\Sigma}_2 =   \\left(\n  \\begin{matrix}\n    1 & -0.3 \\\\\n    -0.3 & 0.5\n  \\end{matrix}\n  \\right)\n\\]\n\n\n\n\n\n\n\n\nFrom the animation above, we can that the decision boundary is not linear anymore. Alright, we can draw a conclusion: the decision boundary in GDA is determined by the covariance matrices of the two distributions. If the two distributions share the same covariance matrix, then the classifier is linear, often referred to as Linear Discriminant Analysis (LDA). Otherwise, it is nonlinear, though not overly flexible—the decision boundary is a quadratic function, which is why it’s also known as Quadratic Discriminant Analysis (QDA).\nOf course, this is just an empirical conclusion and not strictly formal. If you’re interested, you can try deriving the decision boundary using the density function of the multivariate normal distribution. You can refer to a textbook for a deeper understanding—we won’t go into detail here. (Or leave it to Sia in future. ToDo4Sia, write a separate note to explain. )\n\n\n4.2.3 Training algorithm and Example in R\nOne relatively unique aspect is that the algorithm for training this classifier is very simple—it simply estimates the parameters of the two populations based on the data. More specifically, suppose we have a data set containing both target variable y and the data matrix of feature variables, X. We can use the target variable to find all rows from each class in X and estimate the means and covariance matrix respectively. Then we are just ready to construct the classifier with those parameters estimation.\nThere’s just one detail to pay attention to: in LDA, how do we estimate the shared covariance matrix? This shared covariance is called the pooled covariance matrix. You might recall that in a two-sample t-test in basic statistics, we also need to calculate the pooled variance. It’s the same principle here. To estimate the pooled covariance matrix, we can first mean-center each data set separately and then use all the mean-centered data to estimate the covariance matrix.\nLet’s illustrate this with the following example in R.\nR Examples\nFirst, we import a data set for this demo. Here, we use iris data, but only consider two two species, versicolor and virginica\n\ndata = iris[51:150,]\ndata$Species = as.factor(as.numeric(data$Species))\nlevels(data$Species) = c(\"versicolor\",\"virginica\")\npairs(data[,-5], col = data[,5])\n\n\n\n\n\n\n\n\nNext, we want to write a R function to classify each flower based on the 4 feature variables.\nSecond, we can use target variable y to find out the rows of X for each species and estimate the mean and contrivance matrix.\n\n# split data to X and Y\nX = as.matrix(data[,-5])\ny = data[,5]\n# estimate the mean vectors and covariance matrices \n(mu1 = colMeans(X[1:50,]))\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       5.936        2.770        4.260        1.326 \n\n(S1 = cov(X[1:50,]))\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   0.26643265  0.08518367   0.18289796  0.05577959\nSepal.Width    0.08518367  0.09846939   0.08265306  0.04120408\nPetal.Length   0.18289796  0.08265306   0.22081633  0.07310204\nPetal.Width    0.05577959  0.04120408   0.07310204  0.03910612\n\n(mu2 = colMeans(X[-(1:50),]))\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       6.588        2.974        5.552        2.026 \n\n(S2 = cov(X[-(1:50),]))\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   0.40434286  0.09376327   0.30328980  0.04909388\nSepal.Width    0.09376327  0.10400408   0.07137959  0.04762857\nPetal.Length   0.30328980  0.07137959   0.30458776  0.04882449\nPetal.Width    0.04909388  0.04762857   0.04882449  0.07543265\n\n\nNow, we are ready to build the function of our classifier.\n\nlibrary(mvtnorm)\n# function for making decision\nclassifier = function(x,mu1,S1,mu2,S2){\n  # Here, we use function 'dmvnorm' in package 'mvtnorm'\n  ell1 = dmvnorm(x,mu1,S1)\n  ell2 = dmvnorm(x,mu2,S2)\n  res = ifelse(ell1 &gt; ell2, \"versicolor\", \"virginica\")\n  return(res)\n}\n\nWe can apply our classifier on the 27th flower and check the accuracy of the classifier.\n\nid = 27\nclassifier(X[id,],mu1, S1,mu2,S2)\n\n[1] \"versicolor\"\n\ny[id]\n\n[1] versicolor\nLevels: versicolor virginica\n\nres = numeric(100)\nfor(i in 1:100){\n  res[i] = classifier(X[i,],mu1, S1,mu2,S2)\n}\n(acc = mean(res == y))\n\n[1] 0.97\n\n\nLooks good. Quiz: is it LDA or QDA? You need to estimate the pooled contrivance matrix if you want to go for QDA.\n\ndemeanX = X\ndemeanX[1:50,] = X[1:50,] - matrix(rep(mu1,50), ncol = 4, byrow = T)\ndemeanX[51:100,] = X[51:100,] - matrix(rep(mu2,50), ncol = 4, byrow = T)\nS = cov(demeanX)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4.html#further-discussion",
    "href": "Courses/c_mlwr1_2024/l4/l4.html#further-discussion",
    "title": "Lecture 4: Gaussian Discriminant Analysis",
    "section": "4.3 Further Discussion",
    "text": "4.3 Further Discussion\nSo far so good! We have developed the first model, or algorithm, for a binary classification problem. I hope you got a real feel for classifiers. Next, I will discuss this classifier from various perspectives.\n\n4.3.1 More carefully\nLet’s go back to the initial motivating example. In fact, we are still missing a crucial piece of information for this problem: the distribution of the target variable. For example, suppose I add a condition before you make a decision: this observation is from an athlete in the Swedish delegation at the Olympic Village. Then, you certainly wouldn’t classify this person as belonging to the Chinese group. This is an extreme example, but it truly highlights an issue: the distribution of the label variable can impact your prediction. Here, the distribution of label can be presented as \\(\\Pr(Y=1) = 1\\) and \\(\\Pr(Y=-1) = 0\\). Let’s modify this example to make it less extreme. Suppose I obtained this observation in Idivuoma. You can Google this place name, and you’ll find that it would be quite reasonable to say that the proportion of Swedish people here is 95%. Then, how do you make the prediction?\nLet’s see a toy example. As shown in the image below, let’s assume we have two categories, and their proportions are not 50/50. It’s clear that the blue category appears significantly more often than the orange one. Now, I have a green point, and its distance to the orange center is significantly shorter than to the blue center. This means that the likelihood of the green point under the orange model is higher than under the blue model. So, the question is: which category do you think it belongs to?\n\n\n\n\n\n500 blue points generated from the bivariate normal distribution with mean (0,0), equal variance 1, and correlation 0. 10 orange points are observations from bivariate normal with mean (3,0), and the same covariance matrix. The green point is (2,1).\n\n\n\n\nIn this example, the green point is closer to the orange group, however, a more reasonable label is blue since we have more chance to observe a blue point than an orange point in the neighborhood of green point. Next, we discuss how to use the prior probability to correct our prediction. First, let’s clarify our decision rule. Essentially, our previous decision-making method can be summarized by an expression.\n\\[\n  \\widehat{y}=\\arg \\max_{y}\\Pr(\\textbf{x}|y)\n\\] In this expression, \\(\\arg \\max_{y}\\) means return the value of \\(y\\) such that the conditional probability (likelihood) of observation of \\(\\textbf{x}\\), \\(\\Pr(\\textbf{x}|y)\\), is larger. To take the prior probability \\(\\Pr(y = 1)\\) and \\(\\Pr(y = -1)\\) into account, we can add this prior probability on the RHS of the expression, such that the majority gets more weight for the final decision. That is \\[\n  \\widehat{y}=\\arg \\max_{y} \\Pr(\\textbf{x}|y)\\Pr(y)\n\\] If we normalize this expression by \\(\\Pr(\\textbf{x})\\), and apply Bayes formula, then we can get the final expression of our classifier, that is \\[\n  \\widehat{y} = \\arg \\max_{y} \\Pr(y| \\textbf{x})\n\\] So, instead of evaluating the conditional likelihood, we predict the value of target variable \\(y\\) as the one with larger posterior likelihood. How does this change affect the decision boundary of LDA? We still need a bit math to make it clear, but the conclusion is simple: this change only affects the bias term \\(w_0\\) of the decision boundary. That is the bias term will be corrected as \\(w_0-\\log\\left(\\Pr(y=-1)/\\Pr(y=1)\\right)\\).\nRemark: Actually, \\(\\widehat{y} = \\arg \\max_{y} \\Pr(y| \\textbf{x})\\) is the general decision rule for classification. It is not only for GDA classifier, but also for other classifier, for example, we will see it again in the discussion of logistic regression.\n\n\n4.3.2 Multiple labels\nSo far, we discussed the binary-label case, however, it can be naturally extend to a multiple-label setting. In a binary-label case, we only need to evaluate the posterior likelihood when \\(y=1\\) or \\(-1\\). In a \\(k\\)-label setting, we need to estimate \\(k\\) different Normal model and evaluate \\(k\\) posterior likelihood of observation \\(\\textbf{x}\\), then predict the label as the one holding the largest posterior likelihood.\n\n\n4.3.3 R functions for GDA\nWe have implemented this idea in subsection 4.2.3. Is there any functions that can perform LDA and QDA directly? Yes, you can apply functions lda and qda in MASS package. We use the same demo data as before.\n\ndat = iris[51:150,]\ndat$Species = as.factor(as.numeric(dat$Species))\nlevels(dat$Species) = c(\"versicolor\",\"virginica\")\nclass(dat)\n\n[1] \"data.frame\"\n\n\nRemark: Notice that the type of data is data frame. It is very essential. Typically, functions in R packages require the input data to be of type ‘data frame’. This simplifies the function syntax and makes it easier to use the resulting model for predictions.\nNext, we apply lda to train the classifier.\n\nlibrary(MASS)\nmodel_lda = lda(Species~., data = dat)\n\n\npredict_res = predict(model_lda, dat)\nstr(predict_res)\n\nList of 3\n $ class    : Factor w/ 2 levels \"versicolor\",\"virginica\": 1 1 1 1 1 1 1 1 1 1 ...\n $ posterior: num [1:100, 1:2] 1 0.999 0.997 0.994 0.991 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:100] \"51\" \"52\" \"53\" \"54\" ...\n  .. ..$ : chr [1:2] \"versicolor\" \"virginica\"\n $ x        : num [1:100, 1] -2.47 -1.94 -1.53 -1.34 -1.26 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:100] \"51\" \"52\" \"53\" \"54\" ...\n  .. ..$ : chr \"LD1\"\n\n\nThe usage of qda is very similar to lda, so I leave it for you to explore. Currently, we have a classifier in the form of an R function, which is very convenient to use. But what exactly does it look like? In other words, as a linear classifier, how can we write out its decision boundary according to the R outputs?\n\nmodel_lda\n\nCall:\nlda(Species ~ ., data = dat)\n\nPrior probabilities of groups:\nversicolor  virginica \n       0.5        0.5 \n\nGroup means:\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026\n\nCoefficients of linear discriminants:\n                    LD1\nSepal.Length -0.9431178\nSepal.Width  -1.4794287\nPetal.Length  1.8484510\nPetal.Width   3.2847304\n\n\nWe’ll discuss this in the next subsection.\n\n\n4.3.4 Fisher’s idea\nHow can we write out the decision boundary for LDA based on R outputs? Well, let’s start by discussing a simpler problem.\nToDo4Sia: write about dimentional reduction view.\nQuestion 1: Suppose we have only one feature variable—how would we classify in this case?\nFirst, one thing is certain: we need to find a threshold value for comparison, just like in the coin sort example. So, the question is, from a classification perspective, how do we choose the optimal threshold value? Let’s take a look at the figure below.\n\n\n\n\n\n\n\n\n\nThe above figure shows the distribution of the feature variable \\(X\\) for two populations, both following a normal distribution with a shared variance. Now, here’s the question: among the three suggested threshold values—red, blue, and purple—which do you think is most suitable for classification? The red dashed line is definitely not suitable. Although the blue population would be correctly classified, most observations to the right of the red line would be misclassified. Similarly, the blue dashed line is slightly better but still has a 50% chance of misclassification. Given this, the purple dashed line appears to be the optimal classification boundary. The next question is, do you know how to calculate the position of the purple dashed line? From the figure, it’s clear that the purple threshold value is simply the average of the centers (means) of the two populations. Therefore, the decision boundary for a single-feature Linear Discriminant Analysis (LDA) is: \\[\n  X = \\frac{\\mu_1 + \\mu_2}{2}\n\\]\nwhere \\(\\mu_1\\) and \\(\\mu_2\\) are the means of the two populations, i.e. an observation with a value of \\(X\\) larger than \\(\\frac{\\mu_1 + \\mu_2}{2}\\) will be classified as blue.\nAll right, I can explain Fisher’s idea now. When we have multiple feature variables, we first assign a weight to each feature. Then, we calculate a weighted sum of all the feature variables to produce a score value. \\[\n  \\text{score} = w_1 x_1 + w_2x_2 + \\dots + w_px_p\n\\] In this way, the problem is reduced to the same form as in Problem 1 above, where we classify based on a threshold applied to this score. In other words, we need to find the mean score for each population separately and then calculate their average. This average serves as the threshold for classification, similar to the approach with a single feature. By comparing an observation’s score to this threshold, we can classify it accordingly.\nLet’s look at the outputs of lda function of the previous example again. Think about how to write down the decision boundary of LDA classifier?\n\nmodel_lda\n\nCall:\nlda(Species ~ ., data = dat)\n\nPrior probabilities of groups:\nversicolor  virginica \n       0.5        0.5 \n\nGroup means:\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026\n\nCoefficients of linear discriminants:\n                    LD1\nSepal.Length -0.9431178\nSepal.Width  -1.4794287\nPetal.Length  1.8484510\nPetal.Width   3.2847304\n\n\nLet me tell the quick answer. The last part of the outputs, ‘coefficients of linear discriminant’, tells us the weights for calculating the scores. Then, the mean scores of versicolor can be calculated as the weighted sum of means for versicolor in the ‘Group means’ \\[\n  \\textcolor{blue}{-0.94} \\times 5.94 + (\\textcolor{blue}{-1.48}) \\times 2.77 + \\textcolor{blue}{1.85} \\times 4.26 + \\textcolor{blue}{3.28} \\times 1.32 = 2.53\n\\] Similarly, the mean scores of virginica is \\[\n  \\textcolor{blue}{-0.94} \\times 6.59 + (\\textcolor{blue}{-1.48}) \\times 2.97 + \\textcolor{blue}{1.85} \\times 5.55 + \\textcolor{blue}{3.28} \\times 2.03 = 6.34\n\\] Quiz: Please write down the decision boundary of the LDA classifier.\nSo far, we only explained the first step of Fisher’s idea. However, how to find the optimal weights is still unclear. Let’s continue to investigate with the next question.\nQuestion 2 ( NE ): What are the conditions for the optimal weights for a classification problem?\nYou might think the answer is simple. Referring back to the single feature variable case, based on the analysis above, the further apart the centers of the two populations are, the smaller the overlapping portion of their distributions will be, and the more likely we are to get a better threshold value. However, in reality, the distance between the centers of the two distributions alone does not fully reflect the optimal situation. See the figure below.\n\n\n\n\n\n\n\n\n\nFirst, which situation do you think would lead to a better classifier? Clearly, in the case on the RHS, we can obtain a better classifier than in the case in LHS, because the two populations on the RHS have very little overlap. However, let’s take a look at their mean differences. The mean difference on the LHS is 15, which is greater than the 5 on the RHS.\nSummary: We need to find a set of weights through which we can calculate the score values for the two populations. The best set of weights is the one that minimizes the distance between the centers of the score values for the two classes, while also minimizing the variance of the score values within each class. This optimal weights is called Fisher’s projection.\nFor now, let’s leave the algorithm for calculating the optimal weights to your future self, or ToDo4Sia. Here, you only need to know that the decision boundary obtained through Fisher’s projection is the same as the decision boundary from LDA. This is why LDA is often referred to as Fisher’s Linear Discriminant Analysis."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/Discussion of Lectures 1 and 2.html#incremental-lists",
    "href": "Courses/c_mlwr1_2024/Discussion of Lectures 1 and 2.html#incremental-lists",
    "title": "Discussion of Lectures 1 and 2",
    "section": "Incremental Lists",
    "text": "Incremental Lists"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/Discussion of Lectures 1 and 2.html#lecture-1-introduction-to-machine-learning",
    "href": "Courses/c_mlwr1_2024/Discussion of Lectures 1 and 2.html#lecture-1-introduction-to-machine-learning",
    "title": "Discussion of Lectures 1 and 2",
    "section": "Lecture 1: Introduction to Machine Learning",
    "text": "Lecture 1: Introduction to Machine Learning\n\n\nMachine Learning and its Philosophy\nDo you have any ideas about applying machine learning methods?\nSupervised Learning V.S. Unsupervised Learning\nIn a basic statistics course, what you have done that is essentially unsupervised learning?\nAccording to your previous knowledge, which part you are familiar with in the Machine learning process? Which part are you not familiar with?\nDo you have any questions? (Sia may have to control the time)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/Discussion of Lectures 1 and 2.html#lecture-2-introduction-to-r-programming",
    "href": "Courses/c_mlwr1_2024/Discussion of Lectures 1 and 2.html#lecture-2-introduction-to-r-programming",
    "title": "Discussion of Lectures 1 and 2",
    "section": "Lecture 2: Introduction to R Programming",
    "text": "Lecture 2: Introduction to R Programming\n\n\nHave you learned any other programming language? Is its syntax similar to R?\nMatrix(Array) V.S. Data frame V.S. List\nIn fact, the dimension of an array is not limited to 2.\nWhat data may have a dimension higher than 2?\nDo you have any questions? (Sia may have to control the time)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/Discussion of Lectures 1 and 2.html#thanks-for-discussion",
    "href": "Courses/c_mlwr1_2024/Discussion of Lectures 1 and 2.html#thanks-for-discussion",
    "title": "Discussion of Lectures 1 and 2",
    "section": "Thanks for discussion!",
    "text": "Thanks for discussion!\n\nLet’s start the lab session today!\n3 students in one breakout room.\nAfter creating the rooms, you have 1 minutes to jumping in a room with your friends.\nThen, I will randomly assign you into different groups.\nQuestion queue: Link to the online queue table (The link also can be found in Canvas course home page.)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_home.html#discussion",
    "href": "Courses/c_mlwr1_2024/l2/l2_home.html#discussion",
    "title": "Lecture 2: Introduction to R Programming",
    "section": "Discussion",
    "text": "Discussion\nSlides for discussion: here"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_2.html",
    "href": "Courses/c_mlwr1_2024/l4/l4_2.html",
    "title": "4.2 Classifier constructed based on Gaussian model",
    "section": "",
    "text": "Let’s summarize the basic idea of the motivating example. Essentially, we propose two candidate models based on the labels, use these two models to evaluate the object we want to predict, and then make a judgment based on the likelihood evaluation results. The candidate models here are normal distributions. Remember? A probabilistic model is essentially a probability distribution."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_2.html#gaussian-discriminan-analysis",
    "href": "Courses/c_mlwr1_2024/l4/l4_2.html#gaussian-discriminan-analysis",
    "title": "4.2 Classifier constructed based on Gaussian model",
    "section": "4.2.1 Gaussian Discriminan Analysis",
    "text": "4.2.1 Gaussian Discriminan Analysis\nUp to this point, we have actually designed a function where the input consists of the two feature variables, height and weight, and the output is the prediction of the label. A classifier constructed based on this idea is called Gaussian Discriminant Analysis (GDA). Next, let’s describe this method using precise mathematical language.\nSuppose we have a target variable, \\(Y\\) which has \\(\\{-1, 1\\}\\) as possible values. We denote the feature variables as \\(\\textbf{X} = (X_1, X_2, \\dots, X_p)^{\\top}\\). Just emphasize that \\(\\textbf{X}\\) is not a specific observation, and it is just a random variable now. Then we assume that \\[\n  \\begin{matrix}\n    \\textbf{x}|y=1 & \\sim & \\mathcal{N}(\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1)\\\\\n    \\textbf{x}|y=-1 & \\sim & \\mathcal{N}(\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}_2)\n  \\end{matrix}\n\\] With this model, we make the prediction by comparing the likelihood values of a new case with respect to each distribution, i.e. a new case \\(\\textbf{x}_{\\text{new}}\\) is predicted as category \\(1\\) if \\[\n  f(\\textbf{x}_{new}; \\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1) &gt; f(\\textbf{x}_{new}; \\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}_2)\n\\] where \\(f\\) is the density function of multivariate Gaussian distribution, otherwise, the new case will be classified to another group."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_2.html#linear-or-nonlinear",
    "href": "Courses/c_mlwr1_2024/l4/l4_2.html#linear-or-nonlinear",
    "title": "4.2 Classifier constructed based on Gaussian model",
    "section": "4.2.2 Linear or Nonlinear?",
    "text": "4.2.2 Linear or Nonlinear?\nIn Lecture 1, we explained the general form of a linear classifier, i.e., the decision boundary of the classifier is a line, a plane, or a hyperplane in higher dimensions. So far, our GDA is just an R function, kind of black box, so the question is: Is it a linear classifier? We are going to investigate it in this subsection.\nFirst, to understand the nature of the classifier’s decision boundary, it is best to express it using mathematical language. Review the decision rule presented in the previous subsection, it is very easy to see the decision boundary can be represented as \\[\n  \\ell = \\left\\{ \\textbf{x}: f(\\textbf{x}; \\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1) = f(\\textbf{x}; \\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}_2) \\right\\}\n\\] where \\(f_1\\) and \\(f_2\\) are p.d.f of normal distribution. While the notation above is in set theory language, it is not difficult to understand it. It means that, in set \\(\\ell\\), we collect all the points \\(\\textbf{x}\\) that satisfy the condition, \\(f(\\textbf{x}; \\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1) = f(\\textbf{x}; \\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}_2)\\), i.e. equal likelihood in a certain dimensional space.\nNext, I will do some experiments in R to investigate the properties the decision boundary. Obviously, the decision boundary is determined by the two density functions together. So, my plan is to assign different parameter values to the two density functions, then, using the contour graphs of the two density functions, find the points where their values are the same, and connect them. This way, we can roughly understand the shape of the decision boundary.\nBefore we begin, there’s one thing we need to clarify. We only need to focus on the effect of covariance matrix, because it controls the shape of the distribution, and the shape of the distribution determines the shape of the decision boundary.\n\n\nFirst, let’s check the simplest situation, that is the two groups all have the simplest covariance matrix, i.e.  \\[\n  \\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 =   \\left(\n  \\begin{matrix}\n    1 & 0 \\\\\n    0 & 1\n  \\end{matrix}\n  \\right)\n\\]\n\n\n\n\n\n\n\n\nFrom the animation above, we can see that if we connect all the intersection points of contours with the same color, it will roughly form a straight line. In this very special case, our decision boundary is linear. Now, let’s relax the conditions a bit so that the Gaussian model can cover more possibilities, but not too far—just a small step.\n\n\nLet’s assume that the two distributions have the same, but arbitrary, covariance matrix， i.e. \\[\n  \\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 =   \\left(\n  \\begin{matrix}\n    1 & 0.3 \\\\\n    0.3 & 0.5\n  \\end{matrix}\n  \\right)\n\\]\n\n\n\n\n\n\n\n\nWow, it is still a linear classifier. Now, let’s further relax the conditions to the most general case, where the two distributions have completely different and arbitrary covariance matrices.\n\n\n\\[\n  \\boldsymbol{\\Sigma}_1 =   \\left(\n  \\begin{matrix}\n    0.5 & 0.25 \\\\\n    0.25 & 1\n  \\end{matrix}\n  \\right)\n\\] \\[\n  \\boldsymbol{\\Sigma}_2 =   \\left(\n  \\begin{matrix}\n    1 & -0.3 \\\\\n    -0.3 & 0.5\n  \\end{matrix}\n  \\right)\n\\]\n\n\n\n\n\n\n\n\nFrom the animation above, we can that the decision boundary is not linear anymore. Alright, we can draw a conclusion: the decision boundary in GDA is determined by the covariance matrices of the two distributions. If the two distributions share the same covariance matrix, then the classifier is linear, often referred to as Linear Discriminant Analysis (LDA). Otherwise, it is nonlinear, though not overly flexible—the decision boundary is a quadratic function, which is why it’s also known as Quadratic Discriminant Analysis (QDA).\nOf course, this is just an empirical conclusion and not strictly formal. If you’re interested, you can try deriving the decision boundary using the density function of the multivariate normal distribution. You can refer to a textbook for a deeper understanding—we won’t go into detail here. (Or leave it to Sia in future. ToDo4Sia, write a separate note to explain. )"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_2.html#training-algorithm-and-example-in-r",
    "href": "Courses/c_mlwr1_2024/l4/l4_2.html#training-algorithm-and-example-in-r",
    "title": "4.2 Classifier constructed based on Gaussian model",
    "section": "4.2.3 Training algorithm and Example in R",
    "text": "4.2.3 Training algorithm and Example in R\nOne relatively unique aspect is that the algorithm for training this classifier is very simple—it simply estimates the parameters of the two populations based on the data. More specifically, suppose we have a data set containing both target variable y and the data matrix of feature variables, X. We can use the target variable to find all rows from each class in X and estimate the means and covariance matrix respectively. Then we are just ready to construct the classifier with those parameters estimation.\nThere’s just one detail to pay attention to: in LDA, how do we estimate the shared covariance matrix? This shared covariance is called the pooled covariance matrix. You might recall that in a two-sample t-test in basic statistics, we also need to calculate the pooled variance. It’s the same principle here. To estimate the pooled covariance matrix, we can first mean-center each data set separately and then use all the mean-centered data to estimate the covariance matrix.\nLet’s illustrate this with the following example in R.\nR Examples\nFirst, we import a data set for this demo. Here, we use iris data, but only consider two two species, versicolor and virginica\n\ndata = iris[51:150,]\ndata$Species = as.factor(as.numeric(data$Species))\nlevels(data$Species) = c(\"versicolor\",\"virginica\")\npairs(data[,-5], col = data[,5])\n\n\n\n\n\n\n\n\nNext, we want to write a R function to classify each flower based on the 4 feature variables.\nSecond, we can use target variable y to find out the rows of X for each species and estimate the mean and contrivance matrix.\n\n# split data to X and Y\nX = as.matrix(data[,-5])\ny = data[,5]\n# estimate the mean vectors and covariance matrices \n(mu1 = colMeans(X[1:50,]))\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       5.936        2.770        4.260        1.326 \n\n(S1 = cov(X[1:50,]))\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   0.26643265  0.08518367   0.18289796  0.05577959\nSepal.Width    0.08518367  0.09846939   0.08265306  0.04120408\nPetal.Length   0.18289796  0.08265306   0.22081633  0.07310204\nPetal.Width    0.05577959  0.04120408   0.07310204  0.03910612\n\n(mu2 = colMeans(X[-(1:50),]))\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       6.588        2.974        5.552        2.026 \n\n(S2 = cov(X[-(1:50),]))\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   0.40434286  0.09376327   0.30328980  0.04909388\nSepal.Width    0.09376327  0.10400408   0.07137959  0.04762857\nPetal.Length   0.30328980  0.07137959   0.30458776  0.04882449\nPetal.Width    0.04909388  0.04762857   0.04882449  0.07543265\n\n\nNow, we are ready to build the function of our classifier.\n\nlibrary(mvtnorm)\n# function for making decision\nclassifier = function(x,mu1,S1,mu2,S2){\n  # Here, we use function 'dmvnorm' in package 'mvtnorm'\n  ell1 = dmvnorm(x,mu1,S1)\n  ell2 = dmvnorm(x,mu2,S2)\n  res = ifelse(ell1 &gt; ell2, \"versicolor\", \"virginica\")\n  return(res)\n}\n\nWe can apply our classifier on the 27th flower and check the accuracy of the classifier.\n\nid = 27\nclassifier(X[id,],mu1, S1,mu2,S2)\n\n[1] \"versicolor\"\n\ny[id]\n\n[1] versicolor\nLevels: versicolor virginica\n\nres = numeric(100)\nfor(i in 1:100){\n  res[i] = classifier(X[i,],mu1, S1,mu2,S2)\n}\n(acc = mean(res == y))\n\n[1] 0.97\n\n\nLooks good. Quiz: is it LDA or QDA? You need to estimate the pooled contrivance matrix if you want to go for QDA.\n\ndemeanX = X\ndemeanX[1:50,] = X[1:50,] - matrix(rep(mu1,50), ncol = 4, byrow = T)\ndemeanX[51:100,] = X[51:100,] - matrix(rep(mu2,50), ncol = 4, byrow = T)\nS = cov(demeanX)\n\n\nPrevious page | Lecture 4 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_3.html",
    "href": "Courses/c_mlwr1_2024/l4/l4_3.html",
    "title": "4.3 Further Discussion",
    "section": "",
    "text": "So far so good! We have developed the first model, or algorithm, for a binary classification problem. I hope you got a real feel for classifiers. Next, I will discuss this classifier from various perspectives.\n\n4.3.1 More carefully\nLet’s go back to the initial motivating example. In fact, we are still missing a crucial piece of information for this problem: the distribution of the target variable. For example, suppose I add a condition before you make a decision: this observation is from an athlete in the Swedish delegation at the Olympic Village. Then, you certainly wouldn’t classify this person as belonging to the Chinese group. This is an extreme example, but it truly highlights an issue: the distribution of the label variable can impact your prediction. Here, the distribution of label can be presented as \\(\\Pr(Y=1) = 1\\) and \\(\\Pr(Y=-1) = 0\\). Let’s modify this example to make it less extreme. Suppose I obtained this observation in Idivuoma. You can Google this place name, and you’ll find that it would be quite reasonable to say that the proportion of Swedish people here is 95%. Then, how do you make the prediction?\nLet’s see a toy example. As shown in the image below, let’s assume we have two categories, and their proportions are not 50/50. It’s clear that the blue category appears significantly more often than the orange one. Now, I have a green point, and its distance to the orange center is significantly shorter than to the blue center. This means that the likelihood of the green point under the orange model is higher than under the blue model. So, the question is: which category do you think it belongs to?\n\n\n\n\n\n500 blue points generated from the bivariate normal distribution with mean (0,0), equal variance 1, and correlation 0. 10 orange points are observations from bivariate normal with mean (3,0), and the same covariance matrix. The green point is (2,1).\n\n\n\n\nIn this example, the green point is closer to the orange group, however, a more reasonable label is blue since we have more chance to observe a blue point than an orange point in the neighborhood of green point. Next, we discuss how to use the prior probability to correct our prediction. First, let’s clarify our decision rule. Essentially, our previous decision-making method can be summarized by an expression.\n\\[\n  \\widehat{y}=\\arg \\max_{y}\\Pr(\\textbf{x}|y)\n\\] In this expression, \\(\\arg \\max_{y}\\) means return the value of \\(y\\) such that the conditional probability (likelihood) of observation of \\(\\textbf{x}\\), \\(\\Pr(\\textbf{x}|y)\\), is larger. To take the prior probability \\(\\Pr(y = 1)\\) and \\(\\Pr(y = -1)\\) into account, we can add this prior probability on the RHS of the expression, such that the majority gets more weight for the final decision. That is \\[\n  \\widehat{y}=\\arg \\max_{y} \\Pr(\\textbf{x}|y)\\Pr(y)\n\\] If we normalize this expression by \\(\\Pr(\\textbf{x})\\), and apply Bayes formula, then we can get the final expression of our classifier, that is \\[\n  \\widehat{y} = \\arg \\max_{y} \\Pr(y| \\textbf{x})\n\\] So, instead of evaluating the conditional likelihood, we predict the value of target variable \\(y\\) as the one with larger posterior likelihood. How does this change affect the decision boundary of LDA? We still need a bit math to make it clear, but the conclusion is simple: this change only affects the bias term \\(w_0\\) of the decision boundary. That is the bias term will be corrected as \\(w_0-\\log\\left(\\Pr(y=-1)/\\Pr(y=1)\\right)\\).\nRemark: Actually, \\(\\widehat{y} = \\arg \\max_{y} \\Pr(y| \\textbf{x})\\) is the general decision rule for classification. It is not only for GDA classifier, but also for other classifier, for example, we will see it again in the discussion of logistic regression.\n\n\n4.3.2 Multiple labels\nSo far, we discussed the binary-label case, however, it can be naturally extend to a multiple-label setting. In a binary-label case, we only need to evaluate the posterior likelihood when \\(y=1\\) or \\(-1\\). In a \\(k\\)-label setting, we need to estimate \\(k\\) different Normal model and evaluate \\(k\\) posterior likelihood of observation \\(\\textbf{x}\\), then predict the label as the one holding the largest posterior likelihood.\n\n\n4.3.3 R functions for GDA\nWe have implemented this idea in subsection 4.2.3. Is there any functions that can perform LDA and QDA directly? Yes, you can apply functions lda and qda in MASS package. We use the same demo data as before.\n\ndat = iris[51:150,]\ndat$Species = as.factor(as.numeric(dat$Species))\nlevels(dat$Species) = c(\"versicolor\",\"virginica\")\nclass(dat)\n\n[1] \"data.frame\"\n\n\nRemark: Notice that the type of data is data frame. It is very essential. Typically, functions in R packages require the input data to be of type ‘data frame’. This simplifies the function syntax and makes it easier to use the resulting model for predictions.\nNext, we apply lda to train the classifier.\n\nlibrary(MASS)\nmodel_lda = lda(Species~., data = dat)\n\n\npredict_res = predict(model_lda, dat)\nstr(predict_res)\n\nList of 3\n $ class    : Factor w/ 2 levels \"versicolor\",\"virginica\": 1 1 1 1 1 1 1 1 1 1 ...\n $ posterior: num [1:100, 1:2] 1 0.999 0.997 0.994 0.991 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:100] \"51\" \"52\" \"53\" \"54\" ...\n  .. ..$ : chr [1:2] \"versicolor\" \"virginica\"\n $ x        : num [1:100, 1] -2.47 -1.94 -1.53 -1.34 -1.26 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:100] \"51\" \"52\" \"53\" \"54\" ...\n  .. ..$ : chr \"LD1\"\n\n\nThe usage of qda is very similar to lda, so I leave it for you to explore. Currently, we have a classifier in the form of an R function, which is very convenient to use. But what exactly does it look like? In other words, as a linear classifier, how can we write out its decision boundary according to the R outputs?\n\nmodel_lda\n\nCall:\nlda(Species ~ ., data = dat)\n\nPrior probabilities of groups:\nversicolor  virginica \n       0.5        0.5 \n\nGroup means:\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026\n\nCoefficients of linear discriminants:\n                    LD1\nSepal.Length -0.9431178\nSepal.Width  -1.4794287\nPetal.Length  1.8484510\nPetal.Width   3.2847304\n\n\nWe’ll discuss this in the next subsection.\n\n\n4.3.4 Fisher’s idea\nHow can we write out the decision boundary for LDA based on R outputs? Well, let’s start by discussing a simpler problem.\nQuestion 1: Suppose we have only one feature variable—how would we classify in this case?\nFirst, one thing is certain: we need to find a threshold value for comparison, just like in the coin sort example. So, the question is, from a classification perspective, how do we choose the optimal threshold value? Let’s take a look at the figure below.\n\n\n\n\n\n\n\n\n\nThe above figure shows the distribution of the feature variable \\(X\\) for two populations, both following a normal distribution with a shared variance. Now, here’s the question: among the three suggested threshold values—red, blue, and purple—which do you think is most suitable for classification? The red dashed line is definitely not suitable. Although the blue population would be correctly classified, most observations to the right of the red line would be misclassified. Similarly, the blue dashed line is slightly better but still has a 50% chance of misclassification. Given this, the purple dashed line appears to be the optimal classification boundary. The next question is, do you know how to calculate the position of the purple dashed line? From the figure, it’s clear that the purple threshold value is simply the average of the centers (means) of the two populations. Therefore, the decision boundary for a single-feature Linear Discriminant Analysis (LDA) is: \\[\n  X = \\frac{\\mu_1 + \\mu_2}{2}\n\\]\nwhere \\(\\mu_1\\) and \\(\\mu_2\\) are the means of the two populations, i.e. an observation with a value of \\(X\\) larger than \\(\\frac{\\mu_1 + \\mu_2}{2}\\) will be classified as blue.\nAll right, I can explain Fisher’s idea now. When we have multiple feature variables, we first assign a weight to each feature. Then, we calculate a weighted sum of all the feature variables to produce a score value. \\[\n  \\text{score} = w_1 x_1 + w_2x_2 + \\dots + w_px_p\n\\] In this way, the problem is reduced to the same form as in Problem 1 above, where we classify based on a threshold applied to this score. In other words, we need to find the mean score for each population separately and then calculate their average. This average serves as the threshold for classification, similar to the approach with a single feature. By comparing an observation’s score to this threshold, we can classify it accordingly.\nLet’s look at the outputs of lda function of the previous example again. Think about how to write down the decision boundary of LDA classifier?\n\nmodel_lda\n\nCall:\nlda(Species ~ ., data = dat)\n\nPrior probabilities of groups:\nversicolor  virginica \n       0.5        0.5 \n\nGroup means:\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026\n\nCoefficients of linear discriminants:\n                    LD1\nSepal.Length -0.9431178\nSepal.Width  -1.4794287\nPetal.Length  1.8484510\nPetal.Width   3.2847304\n\n\nLet me tell the quick answer. The last part of the outputs, ‘coefficients of linear discriminant’, tells us the weights for calculating the scores. Then, the mean scores of versicolor can be calculated as the weighted sum of means for versicolor in the ‘Group means’ \\[\n  \\textcolor{blue}{-0.94} \\times 5.94 + (\\textcolor{blue}{-1.48}) \\times 2.77 + \\textcolor{blue}{1.85} \\times 4.26 + \\textcolor{blue}{3.28} \\times 1.32 = 2.53\n\\] Similarly, the mean scores of virginica is \\[\n  \\textcolor{blue}{-0.94} \\times 6.59 + (\\textcolor{blue}{-1.48}) \\times 2.97 + \\textcolor{blue}{1.85} \\times 5.55 + \\textcolor{blue}{3.28} \\times 2.03 = 6.34\n\\] Quiz: Please write down the decision boundary of the LDA classifier.\nSo far, we only explained the first step of Fisher’s idea. However, how to find the optimal weights is still unclear. Let’s continue to investigate with the next question.\nQuestion 2 ( NE ): What are the conditions for the optimal weights for a classification problem?\nYou might think the answer is simple. Referring back to the single feature variable case, based on the analysis above, the further apart the centers of the two populations are, the smaller the overlapping portion of their distributions will be, and the more likely we are to get a better threshold value. However, in reality, the distance between the centers of the two distributions alone does not fully reflect the optimal situation. See the figure below.\n\n\n\n\n\n\n\n\n\nFirst, which situation do you think would lead to a better classifier? Clearly, in the case on the RHS, we can obtain a better classifier than in the case in LHS, because the two populations on the RHS have very little overlap. However, let’s take a look at their mean differences. The mean difference on the LHS is 15, which is greater than the 5 on the RHS.\nSummary: We need to find a set of weights through which we can calculate the score values for the two populations. The best set of weights is the one that minimizes the distance between the centers of the score values for the two classes, while also minimizing the variance of the score values within each class. This optimal weights is called Fisher’s projection.\nFor now, let’s leave the algorithm for calculating the optimal weights to your future self, or ToDo4Sia. Here, you only need to know that the decision boundary obtained through Fisher’s projection is the same as the decision boundary from LDA. This is why LDA is often referred to as Fisher’s Linear Discriminant Analysis.\n\nPrevious page | Lecture 4 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_4.html",
    "href": "Courses/c_mlwr1_2024/l4/l4_4.html",
    "title": "4.4 Model Evaluation",
    "section": "",
    "text": "The evaluation of a classification model is crucial to assess its performance and ensure its effectiveness in real-world applications. Accuracy is the first metric that comes to mind when evaluating a model, but it is not sufficient. For example, in a study with 100 observations, where 95 are healthy individuals and 5 are cancer patients, a model that simply classifies every observation as healthy would achieve 95% accuracy. However, this would fail to identify the cancer patients, making the model useless for the task at hand. Next, we will explore some model evaluation methods to better understand and measure the performance of classification models.\n\n4.4.1 Confusion Matrix and related statistics\nA confusion matrix is a powerful tool used to evaluate the performance of a classification model. It shows the counts of actual versus predicted classifications, providing insights into how well the model performs across different classes.\nA general confusion matrix for a binary classification problem has the following form:\n\n\n\n\n\n\n\n\n\nPredicted Positive (P)\nPredicted Negative (N)\n\n\n\n\nActual Positive (P)\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative (N)\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nIn the matrix, the rows represent the actual class of the observations in the data set, i.e. the true labels. The first row (Actual Positive) contains all cases that actually belong to the positive class (e.g., cancer patients), while, the second row contains all cases that actually belong to the negative class (e.g., healthy individuals).\nThe columns represent the predicted class according to the model. The first column (Predicted Positive) contains all cases that the model predicted to be positive (e.g., predicted cancer), and the second column (Predicted Negative) contains all cases that the model predicted to be negative (e.g., predicted healthy).\nWith this structure, each cell in the matrix contains different meaning:\n\nTP: The number of correct predictions where the actual class is positive and the model predicted positive.\nFP: The number of incorrect predictions where the actual class is negative but the model predicted positive.\nFN: The number of incorrect predictions where the actual class is positive but the model predicted negative.\nTN: The number of correct predictions where the actual class is negative and the model predicted negative.\n\nThese metrics provide a comprehensive way to assess the performance of a classification model, especially when dealing with imbalanced data sets. For example, the confusion matrix of the useless classifier mentioned above is displayed below.\n\n\n\n\nPredicted Cancer\nPredicted Healthy\n\n\n\n\nActual Cancer\n0\n5\n\n\nActual Healthy\n0\n95\n\n\n\nIn this example, in addition to accuracy, we can further calculate other statistics to comprehensively evaluate the performance of this classifier.\n\nSensitivity: (True positive rate) The proportion of true positive predictions out of all actual positive cases. This statistic indicates how effectively the classifier identifies the cases of interest, showing how sensitive it is to detecting positive instances.\n\\[\n\\text{Sensitivity} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{0}{0+5} = 0\n\\]\nSpecificity: (True negative rate) The proportion of true negative predictions out of all actual negative cases. \\[\n\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{95}{0+95} = 1\n\\] In the lazy classifier example, although this lazy classifier has very extremely high specificity, 100%, and high accuracy, 95%, we can’t say it is good at all as the extremely low sensitivity, 0. So, people usually simultaneously use the three statistics, i.e. accuracy, sensitivity, and specificity, to evaluate the performance of a classifier.\nPrecision: Sometime, people are also interested in the quality of positive predictions, then the proportion of true positive predictions out of all predicted positive cases, i.e. precision, is used. \\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\\] In the lazy classifier example, it is an extreme cases and precision is not defined as no case is predicted as positive. Mathematically, we also arrive at the same conclusion. Since there are no positive predictions, 0 appears in the denominator, and therefore this ratio is not defined.\n\n\n\n4.4.2 More Choices\nIt is often difficult and inconvenient to compare different things by considering several dimensions at once. The best approach is to find a statistic that can simultaneously evaluate a classifier from multiple perspectives.\nF-score: it is a statistic that combines precision and sensitivity into a single measure to evaluate the performance of a classifier, especially in situations where both false positives and false negatives are important. Essentially, it is the harmonic mean of precision and recall, giving a balance between the two metrics.\n\\[\n  \\text{F-score} = 2 \\times \\frac{\\text{Precision} \\times \\text{sensitivity}}{\\text{Precision} + \\text{sensitivity}}\n\\] F-score ranges from 0 to 1, and it indicates the perfect precision and sensitivity (best performance) for a classifier when it is \\(1\\), but worst performance when it is \\(0\\) With the same example above, suppose we have a classifier always predict a person as a cancer patient, then this classifier has perfect sensitivity but very low precision which is 0.05. The F-score is \\(2\\times\\frac{0.05 \\times 1}{0.05 + 1} = 0.095\\). If someone is willing to use this classifier, they must have ignored the negative effects of misclassifying a healthy person as a cancer patient.\nCohen Kappa Statistics: it is another option that can be used to comprehensively evaluate a classifier. Essentially, it is used to measure the agreement between two raters (classifiers). For example, suppose there are two classifiers both classify 100 cases. If the two classifiers agree with each other, then we can get the following matrix that is similar to the idea of confusion matrix.\n\n\n\n\n\n\n\n\n\nClassifier 2: Positive\nClassifier 2: Negative\n\n\n\n\nClassifier 1: Positive\n30\n0\n\n\nClassifier 1: Negative\n0\n70\n\n\n\nIgnoring whether they are good classifiers, we can say that the two classifiers have the exactly same predictions, in another word, the two classifiers agree with each other. Let’s see another example,\n\n\n\n\n\n\n\n\n\nClassifier 2: Positive\nClassifier 2: Negative\n\n\n\n\nClassifier 1: Positive\n30\n10\n\n\nClassifier 1: Negative\n5\n55\n\n\n\nIn this case, apparently the two classifiers don’t have exactly the same predictions, since there are 5 cases that are predicted as negative by classifier 1 but positive by classifier 2, also 10 disagreements are in an opposite way. However, they still show a certain level of agreement. So, the question is can we design a statistic to quantify the agreement. Of course, the answer is Cohen Kappa statistic. Before showing you the formula of Kappa statistic, let’s clarify one thing. If we set ‘Classifier 1’ as the classifier you want to evaluate, and ‘Classifier 2’ as the ground truth, then this statistic will measure the agreement between your model and the ground truth, and the matrix becomes the confusion matrix.\nNext, let’s have a look at the calculations of this statistic with the notations in a confusion matrix. \\[\n  \\kappa = \\frac{P_o - P_e}{1 - P_e}\n\\]\n\n\\(P_o\\) is the observed agreement: the proportion of times the two raters agree, i.e. the accuracy \\[\nP_o = \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\n\\(P_e\\) is the expected agreement: the proportion of times the two raters would be expected to agree by chance \\[\nP_e = \\left( \\frac{(TP + FP)(TP + FN)}{(TP + TN + FP + FN)^2} \\right) + \\left( \\frac{(TN + FP)(TN + FN)}{(TP + TN + FP + FN)^2} \\right)\n\\] In general, we can use the following table as reference to evaluate a classifier. Here is the information in table format:\n\n\n\n\nKappa (κ) Value\nInterpretation\n\n\n\n\nκ ≥ 0.81\nAlmost perfect agreement\n\n\n0.61 ≤ κ &lt; 0.80\nSubstantial agreement\n\n\n0.41 ≤ κ &lt; 0.60\nModerate agreement\n\n\n0.21 ≤ κ &lt; 0.40\nFair agreement\n\n\nκ ≤ 0.20\nSlight agreement\n\n\nNegative\nWorse than random chance\n\n\n\nLet’s go back to the previous example:\n\n\n\n\n\n\n\n\n\nClassifier 2: Positive\nClassifier 2: Negative\n\n\n\n\nClassifier 1: Positive\n30 (TP)\n10 (FP)\n\n\nClassifier 1: Negative\n5 (FN)\n55 (TN)\n\n\n\nIn this case, \\(\\kappa = 0.68\\), and it suggests a substantial agreement between the two classifiers. If the ‘classifier 2’ represents the ground truth, then \\(\\kappa\\) indicates that ‘classifier 1’ is a rather good classifier.\n\nPrevious page | Lecture 4 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_0.html",
    "href": "Courses/c_mlwr1_2024/l4/l4_0.html",
    "title": "Lecture 4: Gaussian Discriminant Analysis",
    "section": "",
    "text": "In this lecture, we will learn about the first set of machine learning models, which are also the first group of classifiers, namely Gaussian Discriminant Analysis. In addition, we will also learn how to evaluate the performance of a classifier. We will introduce several different statistics to assess a classifier from different perspectives, thereby enriching model evaluation methods beyond just accuracy.\nOutline:\n\n4.1 Basic Ideas\n4.2 Classifier constructed based on Gaussian model\n4.3 Further Discussion\n4.4 Model Evaluation\n\n\nLecture 4 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/Discussion of Lectures 3.html#lecture-3-basic-knowledge-in-probability-theory",
    "href": "Courses/c_mlwr1_2024/Discussion of Lectures 3.html#lecture-3-basic-knowledge-in-probability-theory",
    "title": "Discussion of Lectures 3",
    "section": "Lecture 3: Basic Knowledge in Probability Theory",
    "text": "Lecture 3: Basic Knowledge in Probability Theory\n\n\nWhat is the difference between prior probability and posterior probability? Explain it with your own examples.\nWhat is the difference between probability and likelihood?\nDiscrete random variables V.S. Continuous random variables\nHow to explain what a normal distribution is to family members in simple words?\nHow to understand the contour graph of bivariate normal distribution density function?"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_home.html#discussion",
    "href": "Courses/c_mlwr1_2024/l3/l3_home.html#discussion",
    "title": "Lecture 3: Basic Knowledge in Probability Theory",
    "section": "Discussion",
    "text": "Discussion\nSlides for discussion: here"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l3/l3_lesson_solutions.html",
    "href": "Courses/c_mlwr1_2024/l3/l3_lesson_solutions.html",
    "title": "Solutions to the exercises on Probability Knowledge",
    "section": "",
    "text": "Task 1: Probabilities of events\nTask 1.1 A bag contains 5 red balls, 3 blue balls, and 2 green balls. If you randomly draw one marble from the bag:\n\nWhat is the probability of drawing a red ball?\n\n \\(\\Pr(\\text{drawing a red ball}) = \\frac{5}{5+3+2} = 0.5\\) \n\nWhat is the probability of drawing a blue or green ball?\n\n We want to calculate the event with a keyword OR, so we apply sum rule here, i.e. \\[\n  \\Pr(\\text{drawing a blue or green ball}) = \\Pr(\\text{a blue ball}) + \\Pr(\\text{a green ball}) = 0.5\n\\] Or, this event can be represented as ‘drawing a ball which is not red’, i.e. \\[\n  \\Pr(\\text{drawing a blue or green ball}) = 1 - \\Pr(\\text{drawing a red ball}) = 0.5\n\\] \nTask 1.2 A fair six-sided dice is rolled once.\n\nWhat is the probability of getting an even number?\n\n There are same numbers of even and odd numbers between 1 and 6, so the probability is \\(\\frac{3}{6} = 0.5\\). \n\nWhat is the conditional probability of getting a number larger than 3 given the number is an even number?\n\n When we know the it is a even number, then the number of all possible values become 3 and 2 of them are greater than 3, so the probability is \\(\\frac{2}{3}\\). \n\nWhat is the probability that you first time get a number less than 2 after throwing the dice 5 times?\n\n Let \\(X_i = 1\\) denotes we got a number less than 2 at the \\(i\\)th time, then the probability can be represented as \\[\n  \\Pr(X_1 = 0 \\text{ AND } X_2 = 0 \\text{ AND } X_3 = 0 \\text{ AND } X_4 = 0 \\text{ AND } X_5 = 1)\n\\] By product rule to key words ‘AND’, this probability is equal to \\(0.08\\). \n\n\nTask 2: Discrete Random Variable and Distribution\nTask 2.1: In R, write a function returning the values of  p.m.f  of an arbitrary Binomial distribution.\n\nProbBin = function(k, N=10, p=0.7){\n  (factorial(N)/( factorial(k)*factorial(N-k) ))*p^(k)*(1-p)^(N-k)\n}\n# test my function with the R function `dbinom`\nProbBin(3)\n\n[1] 0.009001692\n\ndbinom(3, 10, 0.7) \n\n[1] 0.009001692\n\n\nTask 2.2: Apply your function to print out the distribution of Binomial distribution \\(\\text{Bin}(10,0.7)\\).\n\nBinDisribution = numeric(11) # since there are 11 possible values for Bin(10,0.7) \nfor(i in 1:11){\n  BinDisribution[i] = round(ProbBin(k = i-1),3)\n}\nBinDisribution = data.frame(X = 0:10, P = BinDisribution)\nprint(BinDisribution)\n\n    X     P\n1   0 0.000\n2   1 0.000\n3   2 0.001\n4   3 0.009\n5   4 0.037\n6   5 0.103\n7   6 0.200\n8   7 0.267\n9   8 0.233\n10  9 0.121\n11 10 0.028\n\n\n\n\nTask 3: Characteristic Values\nTask 3.1: Explain why the expected value of a Bernoulli distributed random variable, \\(X \\sim \\text{Ber}(p)\\), is \\(p\\).\n The expected value of a random variable is the weighted sum of all possible values, the weights are the probability \\[\n  \\text{E}(X) = 1\\times p + 0 \\times (1-p) = p\n\\] \nTask 3.2: Explain why the expected value of a Binomial distributed random variable, \\(X \\sim \\text{Bin}(N, p)\\), is \\(Np\\).\n Binomial distribution presents the probability of the number of success in \\(N\\) independent binary outcome experiment. Suppose \\(X_i\\) for \\(i = 1,\\dots,N\\), then \\(\\sum_{i = 1}^N X_i\\) is binomial distributed. So, by the properties of expected value, we have \\[\n  \\text{E}(X) = \\text{E}\\left( \\sum_{i = 1}^N X_i \\right) = \\sum_{i = 1}^N \\text{E}(X_i) = \\sum_{i = 1}^N p = Np\n\\] \nTask 3.3: Explain why the variance of a Bernoulli distributed random variable is \\(p(1-p)\\).\n Variance is the expected value of the squared distance between \\(X\\) and its expected value \\[\n  \\text{Var}(X) = \\text{E} \\left( X - \\text{E}(X) \\right)^2 = \\text{E}(X^2) - (\\text{E}(X))^2 = p - p^2 = p(1-p)\n\\] \nTask 3.4: Explain why the variance of a Binomial distributed random variable is \\(Np(1-p)\\).\n With the same idea of 3.2, \\[\n  \\text{Var}(X) = \\text{Var}\\left( \\sum_{i = 1}^N X_i \\right) = \\sum_{i = 1}^N \\text{Var}(X_i) = \\sum_{i = 1}^N p(1-p) = Np(1-p)\n\\] \nTask 3.5 (  HS  ): In Section 3.4.5, we discussed the covariance between two random variables and got formulas of calculating the characteristic values for the weighted sum (linear combination) of random variables. Actually, these formula can be represented in a matrix form, and the matrix form can help us to easily generalize it to multiple setting (more than two random variables). For example, the mean and variance of a linear combination of two random variables are \\[\n  \\text{E}(a_1X_1+a_2X_2) = a_1\\text{E}(X_1) + a_2\\text{E}(X_2)\n\\] It can be represented as \\[\n  \\text{E}(\\textbf{a}^{\\top}\\textbf{X}) = \\textbf{a}^{\\top}\\text{E}(\\textbf{X})\n\\] where \\(\\textbf{a} = (a_1, a_2)^{\\top}\\) and \\(\\textbf{X} = (X_1, X_2)^{\\top}\\). In a multivariate setting, we usually call \\(\\textbf{X}\\) as a random vector, it is a 2 dimensional random vector in this case, however, it can be arbitrary dimension in general.\nNow, it is your turn. Can you represent the following result of variance in a matrix form? \\[\n  \\text{Var}(a_1X_1+a_2X_2) = a_1^2\\text{Var}(X_1) + 2a_1a_2\\text{Cov}(X_1,X_2) + a_2^2\\text{Var}(X_2)\n\\]\n First, we write the formula above in the matrix form \\[\n  \\text{Var}(a_1X_1+a_2X_2) = (a_1, a_2)\\left(\n  \\begin{matrix}\n    \\text{Var}(X_1) & \\text{Cov}(X_1, X_2) \\\\\n    \\text{Cov}(X_1, X_2) & \\text{Var}(X_2)\n  \\end{matrix}\n  \\right)\n  \\left(\n  \\begin{matrix}\n    a_1  \\\\\n    a_2  \n  \\end{matrix}\n  \\right)\n\\] In general, we have \\[\n  \\text{Var}(\\textbf{a}^{\\top}\\textbf{X}) = \\textbf{a}^{\\top} \\boldsymbol{\\Sigma} \\textbf{a}\n\\] where \\(\\boldsymbol{\\Sigma}\\) is the covariance matrix of \\(\\textbf{X}\\). \n\n\nTask 4: Joint distribution\nTask 4.1: In Section 3.4.2, I only show you how to calculate the value in the first cell, i.e. \\(\\Pr( X=1, Y=1 )\\). Please calculate the values of the rest 3 cells, i.e. \\(\\Pr( X=1, Y=0 )\\), \\(\\Pr( X=0, Y=1 )\\), and \\(\\Pr( X=0, Y=0 )\\)\nTask 4.2: With the same background problem, calculate the probability that you eventually get an orange.\n We apply sum rule to key word ‘OR’ first: \\[\n\\Pr (Y = 0) = \\Pr \\left( (Y = 0 \\text{ AND } X = 1) \\text{ OR } (Y = 0 \\text{ AND } X = 0) \\right)\n\\] i.e.  \\[\n  \\Pr (Y = 0) =\\Pr \\left( Y = 0 \\text{ AND } X = 1 \\right) + \\Pr \\left( Y = 0 \\text{ AND } X = 0 \\right)\n\\] Then we apply product rule to key word ‘AND’ \\[\n  \\Pr (Y = 0) =\\Pr \\left( Y = 0 | X = 1 \\right) \\Pr(X = 1) + \\Pr \\left( Y = 0 | X = 1 \\right) \\Pr(X = 0)\n\\] Then the final answer is \\[\n  \\Pr (Y = 0) = \\frac{6}{8}\\frac{4}{6} + \\frac{1}{4}\\frac{2}{6} = \\frac{7}{12}\n\\] \nTask 4.3: Apply Bayes Formula to calculate the posterior probability that we chose the red box if we get an apple, i.e. \\(\\Pr( X=1 | Y=1 )\\).\nTask 4.4: Discuss with your group mates, propose a new example to explain what is the difference between prior and posterior probabilities.\n\n\nTask 5: Continuous Random Variable and Distribution\nTask 5.1: \\(X \\sim \\mathcal{N}(1.2, 1)\\), use R to calculate \\(\\Pr(-1.5 &lt; X &lt; 2.2)\\).\n\npnorm(2.2, 1.2, 1) - pnorm(-1.5, 1.2, 1)\n\n[1] 0.8378778\n\n\nTask 5.2: Suppose \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then what is the distribution of \\(\\frac{X - \\mu}{\\sigma}\\)?\n \\(\\frac{X - \\mu}{\\sigma}\\) is called standardization of a Normally distributed variable, since the resulting random variable has zero mean and varaince \\(1\\)\n\nTask 5.3: Explain why 95% of the probability is covered within two SDs around the mean in a Normal distribution.\n \\[\n  \\Pr \\left(\\mu + 2\\sigma \\leq X \\leq \\mu + 2\\sigma \\right) = \\Pr\\left(2 \\leq \\frac{X-\\mu}{\\sigma} \\leq 2 \\right)\n\\] This probability can be calculated as\n\npnorm(2) - pnorm(-2)\n\n[1] 0.9544997\n\n\n\n\n\nTask 6: Likelihood Analysis\nTask 6.1: With the “Box-Fruits” the background problem, let’s make a small adjustment: we use the flip of a fair coin to decide which box to choose.\nSuppose you got an apple—then which box do you think you are more likely to have chosen?\n Since we have equal probability to choose the red or blue box, i.e. the priors of blue and red are the same, we only need to evaluate the likelihood of geting an apple conditional on the color of box. So, blue box is more reasonable since \\(\\Pr(\\text{apple} | \\text{blue}) = \\frac{3}{4} &gt;  \\Pr(\\text{apple} | \\text{red}) = \\frac{1}{4}\\) \nTask 6.2 Now, we change back the original setting that we throw a dice to decide the box, but we get red box when getting a number less than 6. Suppose you got an apple—then which box do you think you are more likely to have chosen?\n In the new setting, since we have different probability to choose the color, i.e. \\(\\Pr(\\text{blue}) = \\frac{1}{6}\\) and \\(\\Pr(\\text{red}) = \\frac{5}{6}\\), only considering the likelihood of getting apple conditional on the color is not sufficient. Simply speaking, although we have higher probability to get an apply in blue box than the red, we can’t simply draw concolusion since we also have higher chance to get blue than red. So, in this setting, we need to apply the posterior probability to make decision, i.e. we need to calculate \\[\n  \\Pr(\\text{Blue}|\\text{Apple}) = \\frac{\\Pr(\\text{Apple}|\\text{Blue}) \\Pr(\\text{Blue})}{\\Pr(\\text{Apple}|\\text{Blue}) \\Pr(\\text{Blue})+\\Pr(\\text{Apple}|\\text{red}) \\Pr(\\text{red})} = \\frac{\\frac{3}{4} \\times \\frac{1}{6} }{\\frac{3}{4} \\times \\frac{1}{6} + \\frac{1}{4} \\times \\frac{5}{6}} = \\frac{3}{8}\n\\] and \\[\n  \\Pr(\\text{Red}|\\text{Apple}) = \\frac{\\Pr(\\text{Apple}|\\text{red}) \\Pr(\\text{red})}{\\Pr(\\text{Apple}|\\text{red}) \\Pr(\\text{red})+\\Pr(\\text{Apple}|\\text{blue}) \\Pr(\\text{blue})} = \\frac{\\frac{1}{4} \\times \\frac{5}{6} }{\\frac{3}{4} \\times \\frac{1}{6} + \\frac{1}{4} \\times \\frac{5}{6}} = \\frac{5}{8}\n\\] Therefore, this time we are more likely to have drawn the red box. \n\nLecture 3 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_lab_solutions.html",
    "href": "Courses/c_mlwr1_2024/l2/l2_lab_solutions.html",
    "title": "Solutions to Exercises in Lab 1",
    "section": "",
    "text": "Install R-studio and R-base. Launch Rstudio, install package tidyverse. It integrates multiple packages and has revolutionary significance in data processing.\nImport a data set, FCourse.txt. You can download it here. This data set includes students’ preference levels for different subjects. Do you have any comments? Delete the last 10 rows of the data set and save it as a txt file in your disk. The function you need is write.table.\n\n\n\nTask 2.1: Write a program to calculate the sum of integer from 1 to 100. I’m not sure if you’ve heard the story about the great mathematician Gauss, who was the first pupil to finish this calculation and the first to go home for dinner in his class room. I’m sure you must be even faster than him!\n You can apply for loop\n\nres = 0\nfor(i in 1:100){\n  res = res + i\n}\ncat(\"The sum of integers from 1 to 100 is:\", res, \"\\n\")\n\nThe sum of integers from 1 to 100 is: 5050 \n\n\n or a simpler solution is\n\ncat(\"The sum of integers from 1 to 100 is:\", sum(1:100), \"\\n\")\n\nThe sum of integers from 1 to 100 is: 5050 \n\n\n\nTask 2.2: After this, modify your code such that you can calculate the factorial of 100 (the product of integers from 1 to 100) by the program.\n For loop solution:\n\nres = 1\nfor(i in 2:100){\n  res = res * i\n}\ncat(\"The product of integers from 1 to 100 is:\", res, \"\\n\")\n\nThe product of integers from 1 to 100 is: 9.332622e+157 \n\n\n\n\n\nR is excellent at graphics, especially taking power of the ggplot2 package into account. We don’t have time to study this package, but will do some simple exercises.\nTask 3.1: One can visualize a math function by the following code\nx = seq(-pi,pi,0.01)\nplot(x, sin(x), type = \"l\")\nabline(h = 0)\nabline(v = 0)\nNow, it is your turn. Visualize the density function of the normal distribution with mean 5 and  sd  2.\n\nx = seq(-1, 11, 0.01)\nplot(x, dnorm(x, 5, 2), type = \"l\", ylab = \"f(x)\", main = \"Density function of normal distribution\")\n\n\n\n\n\n\n\n\nTask 3.2: Learn and practice the following basic plotting functions, hist, boxplot, and pie.\n\n\n\nUsing the following code to generate the example data for this task. BTW, you may also have a closer look at the functions that you have not seen before or you are not familiar with.\n\nn = 100 # sample size\ntreatment = rbinom(n, 1, 0.5)\nblock = sample(c(1,2), n, replace = T)\nsex = sample(c(\"F\", \"M\"), n, replace = T)\nage = round( runif(n, 18, 40) )\noutcome = round( rnorm(n, 30, 10) )\nDat = data.frame(treatment, block, sex, age, outcome)\n\nTask 4.1: Sort the data set by variable age in ascending order; Filter out all the rows of female observations; Find out the values of variables age and outcome for all the rows that belongs to treatment 1 and block 2.\n\n# Sort the data by age\nhead(Dat[order(age), ], 10)\n\n   treatment block sex age outcome\n16         0     2   M  18      20\n33         0     1   F  18      34\n95         0     1   F  18      33\n34         1     1   M  19      22\n60         1     1   M  19      34\n52         1     2   M  20      30\n44         0     1   M  21      20\n57         1     2   F  21      14\n77         1     1   M  21      32\n42         0     2   F  22      38\n\n# Filter out all the rows of female obs\nhead(Dat[which(Dat$sex != \"F\"), ])\n\n   treatment block sex age outcome\n3          0     2   M  35      30\n4          0     1   M  25      27\n6          1     1   M  34      25\n7          1     2   M  29      18\n8          0     2   M  29      48\n10         1     2   M  27      42\n\n#\nhead(Dat[which(Dat$treatment == 1 & Dat$block == 2), 4:5], 10)\n\n   age outcome\n7   29      18\n10  27      42\n11  30      37\n15  39      39\n26  29      35\n28  29      39\n39  29      21\n46  39      35\n52  20      30\n53  36      18\n\n\nTask 4.2: Randomly draw a sample with 80 observations from the data set and set them as sub-dataset 1 and the rest of them as sub-dataset 2.\n\nid = sample(1:dim(Dat)[1], 80)\nDat_sample1 = Dat[id, ]\nDat_sample2 = Dat[-id, ]"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_lab_solutions.html#warm-up",
    "href": "Courses/c_mlwr1_2024/l2/l2_lab_solutions.html#warm-up",
    "title": "Solutions to Exercises in Lab 1",
    "section": "",
    "text": "Install R-studio and R-base. Launch Rstudio, install package tidyverse. It integrates multiple packages and has revolutionary significance in data processing.\nImport a data set, FCourse.txt. You can download it here. This data set includes students’ preference levels for different subjects. Do you have any comments? Delete the last 10 rows of the data set and save it as a txt file in your disk. The function you need is write.table.\n\n\n\nTask 2.1: Write a program to calculate the sum of integer from 1 to 100. I’m not sure if you’ve heard the story about the great mathematician Gauss, who was the first pupil to finish this calculation and the first to go home for dinner in his class room. I’m sure you must be even faster than him!\n You can apply for loop\n\nres = 0\nfor(i in 1:100){\n  res = res + i\n}\ncat(\"The sum of integers from 1 to 100 is:\", res, \"\\n\")\n\nThe sum of integers from 1 to 100 is: 5050 \n\n\n or a simpler solution is\n\ncat(\"The sum of integers from 1 to 100 is:\", sum(1:100), \"\\n\")\n\nThe sum of integers from 1 to 100 is: 5050 \n\n\n\nTask 2.2: After this, modify your code such that you can calculate the factorial of 100 (the product of integers from 1 to 100) by the program.\n For loop solution:\n\nres = 1\nfor(i in 2:100){\n  res = res * i\n}\ncat(\"The product of integers from 1 to 100 is:\", res, \"\\n\")\n\nThe product of integers from 1 to 100 is: 9.332622e+157 \n\n\n\n\n\nR is excellent at graphics, especially taking power of the ggplot2 package into account. We don’t have time to study this package, but will do some simple exercises.\nTask 3.1: One can visualize a math function by the following code\nx = seq(-pi,pi,0.01)\nplot(x, sin(x), type = \"l\")\nabline(h = 0)\nabline(v = 0)\nNow, it is your turn. Visualize the density function of the normal distribution with mean 5 and  sd  2.\n\nx = seq(-1, 11, 0.01)\nplot(x, dnorm(x, 5, 2), type = \"l\", ylab = \"f(x)\", main = \"Density function of normal distribution\")\n\n\n\n\n\n\n\n\nTask 3.2: Learn and practice the following basic plotting functions, hist, boxplot, and pie.\n\n\n\nUsing the following code to generate the example data for this task. BTW, you may also have a closer look at the functions that you have not seen before or you are not familiar with.\n\nn = 100 # sample size\ntreatment = rbinom(n, 1, 0.5)\nblock = sample(c(1,2), n, replace = T)\nsex = sample(c(\"F\", \"M\"), n, replace = T)\nage = round( runif(n, 18, 40) )\noutcome = round( rnorm(n, 30, 10) )\nDat = data.frame(treatment, block, sex, age, outcome)\n\nTask 4.1: Sort the data set by variable age in ascending order; Filter out all the rows of female observations; Find out the values of variables age and outcome for all the rows that belongs to treatment 1 and block 2.\n\n# Sort the data by age\nhead(Dat[order(age), ], 10)\n\n   treatment block sex age outcome\n16         0     2   M  18      20\n33         0     1   F  18      34\n95         0     1   F  18      33\n34         1     1   M  19      22\n60         1     1   M  19      34\n52         1     2   M  20      30\n44         0     1   M  21      20\n57         1     2   F  21      14\n77         1     1   M  21      32\n42         0     2   F  22      38\n\n# Filter out all the rows of female obs\nhead(Dat[which(Dat$sex != \"F\"), ])\n\n   treatment block sex age outcome\n3          0     2   M  35      30\n4          0     1   M  25      27\n6          1     1   M  34      25\n7          1     2   M  29      18\n8          0     2   M  29      48\n10         1     2   M  27      42\n\n#\nhead(Dat[which(Dat$treatment == 1 & Dat$block == 2), 4:5], 10)\n\n   age outcome\n7   29      18\n10  27      42\n11  30      37\n15  39      39\n26  29      35\n28  29      39\n39  29      21\n46  39      35\n52  20      30\n53  36      18\n\n\nTask 4.2: Randomly draw a sample with 80 observations from the data set and set them as sub-dataset 1 and the rest of them as sub-dataset 2.\n\nid = sample(1:dim(Dat)[1], 80)\nDat_sample1 = Dat[id, ]\nDat_sample2 = Dat[-id, ]"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_lab_solutions.html#strength-training",
    "href": "Courses/c_mlwr1_2024/l2/l2_lab_solutions.html#strength-training",
    "title": "Solutions to Exercises in Lab 1",
    "section": "2. Strength Training",
    "text": "2. Strength Training\n\nTask 5: About Numbers\nIn this task, we solve some problems about numbers\nTask 5.1: Finding prime numbers Write a program to find all prime numbers up to 100. A prime number is a number that has only two factors, that is, 1 and the number itself.\n\n# Function to check if a number is prime\nis.prime = function(num) {\n  if (num &lt;= 1) {\n    return(FALSE)  # Numbers less than or equal to 1 are not prime\n  }\n  for (i in 2:sqrt(num)) {\n    if (num %% i == 0) {\n      return(FALSE)  # If the number is divisible by any number between 2 and sqrt(num), it's not prime\n    }\n  }\n  return(TRUE)  # If no divisors were found, it's prime\n}\n# Find and print all prime numbers up to 100\nprime_numbers = c()\nfor (i in 2:100) {\n  if (is.prime(i)) {\n    prime_numbers = c(prime_numbers, i)\n  }\n}\ncat(\"Prime numbers up to 100:\", prime_numbers, \"\\n\")\n\nPrime numbers up to 100: 3 5 7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 73 79 83 89 97 \n\n\nTask 5.2: Write a function that can convert a binary number to a decimal integer.\n\n# Function to convert binary to decimal\nb2d = function(binary) {\n  # Easy to work with a character string\n  binary = as.character(binary)\n  # Reverse the binary string to handle the least significant bit first\n  binary = rev(strsplit(binary, NULL)[[1]])\n  # Initialize the decimal value\n  decimal = 0\n  # Loop over each bit in the binary string\n  for (i in 1:length(binary)) {\n    if (binary[i] == \"1\") {\n      # Add the value corresponding to the bit's position\n      decimal = decimal + 2^(i - 1)\n    }\n  }\n  return(decimal)\n}\n\nTask 5.3: Write a function that can convert a decimal integer to a binary number.\n\n# Function to convert decimal to binary\nd2b = function(decimal) {\n  # Initialize an empty string to store the binary representation\n  binary = \"\"\n  # Edge case for 0\n  if (decimal == 0) {\n    return(\"0\")\n  }\n  # Loop to divide the decimal number by 2 and store the remainder\n  while (decimal &gt; 0) {\n    remainder = decimal %% 2  # Find remainder (0 or 1)\n    binary = paste0(remainder, binary)  # Prepend the remainder to binary string\n    decimal = decimal %/% 2  # Perform integer division by 2\n  }\n  return(binary)\n}\n\n\n\nTask 6: Law of Large Numbers\nLaw of large numbers (LLN) is one of the foundations of probability theory. It states that as the number of trials or observations increases, the average of the results approaches the expected value. Simply put, suppose you have a fair coin. Each flip has an equal chance—50/50—of landing heads or tails. If you flip the coin repeatedly and track the cumulative proportion of heads, this proportion will get closer and closer to 0.5 as the number of flips increases.\nIn this task, we will “prove” the LLN by doing a small simulation. Let the computer mimic flipping a fair coin (generate a random number from Bernoulli distribution with \\(p=0.5\\)). Draw a graph to show that as the number of flips increases, the cumulative proportion of getting heads or tails converges to 0.5.\n\nn = 1000 \nsum = 0\nres = numeric(n)\nfor(i in 1:n){\n  sum = sum + rbinom(1,1,0.5)\n  res[i] = sum/i\n}\n# or \nx = rbinom(n, 1, 0.5)\nfor(i in 1:n){\n  res[i] = sum(x[1:i])/i\n}\n# vis the results\nplot(res, type = \"l\")\nabline(h=0.5, col = \"red\")\n\n\n\n\n\n\n\n\n Or, you can do it without using for loop.\n\nres = cumsum(x)/(1:n)\n# vis the results\nplot(res, type = \"l\")\nabline(h=0.5, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nTask 7: Central Limit Theorem\nThe Central Limit Theorem (CLT) is a key principle in statistics that states: for a sufficiently large sample size, the distribution of the sample mean approaches a normal (bell-shaped) distribution, regardless of the shape of the original population distribution. This means that if you take repeated random samples from any population with a finite mean and variance, the means of those samples will tend to follow a normal distribution as the sample size grows.\nIn this task, we will “demonstrate” the CLT through a simulation. We’ll repeatedly draw random samples, each with a sample size of 50, from a uniform distribution between 0 and 1. For each sample, we’ll calculate and record the average value. After repeating this process 1,000 times, plot a histogram to visualize the distribution of these 1,000 sample averages.\n\n# Set parameters\nn_samples = 1000  # Number of repeated samples\nsample_size = 50  # Size of each sample\n\n# Initialize a vector to store the sample means\nsample_means = numeric(n_samples)\n\n# Repeat the sampling process\nfor (i in 1:n_samples) {\n  # Draw a random sample from a uniform distribution between 0 and 1\n  sample = runif(sample_size, min = 0, max = 1)\n  \n  # Calculate the mean of the sample\n  sample_means[i] = mean(sample)\n}\n\n# Plot a histogram of the sample means\nhist(sample_means, \n     breaks = 30,  # Number of bins for the histogram\n     main = \"Distribution of Sample Means (CLT Demonstration)\",\n     xlab = \"Sample Mean\", \n     col = \"lightblue\", \n     border = \"black\")\n\n\n\n\n\n\n\n\n\n\nTask 8: Box-Muller’s Algorithm\nWe roughly mentioned about Pseudo-random numbers in the lecture. Pseudo-random numbers are a sequence of numbers which are generated by some algorithm from an initial number, and they can mimic the behaviour of a random sample of uniform random variables. Once the pseudo uniform random number is ready, different algorithms can be applied to them to generate random numbers from certain distributions. Here, you implement Box and Muller’s algorithm to generate random numbers from an arbitrary Normal distribution. Write the implementation of this algorithm in a function, such that you can apply this function to simulate Normal random sample, just like function `rnorm``\nBox-Muller’s Algorithm\n\nStep 1: Randomly generate \\(u\\) and \\(v\\) from \\(U(0,1)\\), uniform distribution between 0 and 1\nStep 2: Set \\(x = \\sqrt{ -2 \\log (u) } \\cos (2\\pi v)\\) and \\(y = \\sqrt{ -2 \\log (u) } \\sin (2\\pi v)\\)\n\nBased on this procedure, the resulting values of \\(x\\) and \\(y\\) are independent normal distributed with mean 0 and variance 1.\n\n# write a function to generate standard Gaussian by Box-Muller \nbm = function(n = 100){\n  n = n/2\n  u1 = runif(n)\n  u2 = runif(n)\n  x1 = sqrt(-2*log(u1))*cos(2*pi*u2)\n  #x2 = sqrt(-2*log(u1))*sin(2*pi*u2) \n  return(x1)\n}\n# apply function bm to generate sample from arbitary normal\nmyNormal = function(n = 100, mu=0, sigma=1){\n  res = mu + sigma*bm(n)\n  return(res)\n}\nhist(myNormal(10000, 1, 2), \n     breaks = 30,  \n     freq = FALSE, # display proportion instead of frequence\n     main = \"Distribution of Sample Means (CLT Demonstration)\",\n     xlab = \"Normal distributed sample\", \n     col = \"lightblue\", \n     border = \"black\")\nx = seq(-5, 7, 0.01)\npoints(x, dnorm(x, 1, 2), type = \"l\", col = \"red\")\n\n\n\n\n\n\n\n\n\n\nTask 9: Newton Raphson Algorithm\nDo you know Newton Raphson’s optimization algorithm? The main idea of this algorithm is to find successively better approximations to the roots of a real-valued function. More specifically, assuming \\(f(x)\\) is differentiable and starting from an initial guess \\(x_0\\), the root of \\(f(x)=0\\) can be iteratively approximated as\n\\[\n  x_{n+1}=x_{n}-\\frac{f(x_{n})}{f'(x_{n})}\n\\] Now, you are required to apply this algorithm to find an approximated root of \\(x^3-x-1=0\\).\n\nf = function(x){x^3 - x -1}\nff = function(x){3*x^2-1} # derivative \nx_0 = 0\nerr = 10\ntau = 0.000001 #tolerance of approximation error. It helps to decide if we stop the loop\nwhile(err &gt; tau){\n  x_1 = x_0 - f(x_0)/ff(x_0)\n  err = abs(x_1 - x_0)\n  # err = abs(f(x_1)) # another way for a stop\n  x_0 = x_1\n}\nx_1 # the root\n\n[1] 1.324718\n\nf(x_1)\n\n[1] 2.747136e-12\n\n\n\n\nTask 10: Bootstrap Algorithm\nThe bootstrap algorithm is a statistical technique used to estimate the distribution of a sample statistic by resampling the observed data with replacement. In essence, it involves repeatedly drawing samples (typically of the same size as the original sample) from the dataset, calculating the desired statistic (e.g., mean, median, or standard deviation) for each resample, and then aggregating these results. This method allows for estimating the variability or confidence intervals of statistics without requiring complex mathematical formulas, making it especially useful when traditional parametric assumptions (like normality) are not met. The well known machine learning algorithm random forest was just developed based on this algorithm.\nPrepare the data set: Simulate \\(x_i,i = 1,2,...,30\\) from uniform distribution \\(U(0,5)\\) by function ’runif’ and \\(\\epsilon_i,i=1,2,...,30\\) from the standard Gaussian distribution. Calculate \\(y_i =0.5+1.5x_i+\\epsilon_i\\).\nNext, we will apply bootstrap algorithm to estimate the confidence interval (CI) of the regression coefficient, and compare it with the CI calculated by formula.\nTask 10.1: Calculate the CI by formula. Employ function ’lm’ to estimate the regression model and use the output to calculate the confidence interval of the slop term of the regression model.\nTask 10.2: Calculate the CI of the estimation of slop term by Bootstrap Algorithm.\nFirst, we generate a bootstrap sample from the data. The bootstrap sample is resampled from the simulated data with a replacement. Second, estimate the regression model with the bootstrap sample and record the estimation of the slope term. Repeat the two steps 1000 times. The bootstrap confidence interval is the upper and lower quantile values of all the estimations of the slope term. Compare the results with 1). This procedure is summarized in the following pseudo algorithm\nB = 1000\nfor(i in 1:B){\n  #Step 1: draw a bootstap sample from the data set\n  #Step 2: Apply `lm` to estimate the model with the bootstrap sample\n  #Step 3: Save the estimation of slop term\n}\n# Calculate the quantile values of 1000 estimation of slop term.  \n Solutions: Data preparation:\n\nN = 30 # sample size\nx = runif(N, 0, 5) # regressor from uniform distribution\ne = rnorm(N) # noise is from normal\ny = 0.5 + 1.5*x + e\nplot(x,y)\n\n\n\n\n\n\n\n\n Confidence intervals by regression model outputs.\n\n# lm function to estimate slop term \nm = lm(y~x)\nsummary(m) # check the std.error of slop term\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7912 -0.6825  0.1825  0.5853  1.3844 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.6628     0.3011   2.201   0.0362 *  \nx             1.4598     0.1020  14.313 2.09e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8455 on 28 degrees of freedom\nMultiple R-squared:  0.8798,    Adjusted R-squared:  0.8755 \nF-statistic: 204.9 on 1 and 28 DF,  p-value: 2.093e-14\n\n# CI: \n1.7589 + 1.96*0.1639\n\n[1] 2.080144\n\n1.7589 - 1.96*0.1639\n\n[1] 1.437656\n\n\n Confidence intervals by Bootstrap algorithm.\n\n# Bootstrap CI\nB = 1000 # the number of bootstrap replicates\nbeta_hat = m$coefficients\n\nslop_est = numeric(B) # store all 100 estimation of slop term\nfor(i in 1:B){\n  id = sample(1:N, N, replace = T) # including cases in bootstrap sample\n  bt.y = y[id]\n  bt.x = x[id]\n  slop_est[i] = lm(bt.y~bt.x)$coefficients[2]\n}\n# CI by Bootstrap\nquantile(slop_est, 0.975)\n\n   97.5% \n1.684921 \n\nquantile(slop_est, 0.025)\n\n    2.5% \n1.250433"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l2/l2_lab_solutions.html#extreme-cardio",
    "href": "Courses/c_mlwr1_2024/l2/l2_lab_solutions.html#extreme-cardio",
    "title": "Solutions to Exercises in Lab 1",
    "section": "3. Extreme Cardio",
    "text": "3. Extreme Cardio\n\nTask 11: Proceptron Algorithm\nWe have discussed this algorithm in lecture 1. Now, let’s implement it in R.\nTask 11.1: Use the following code to generate the data\nset.seed(201606)\nN = 20\nx1 = runif(N,-1,1); x2 = runif(N,-1,1); X = cbind(x1,x2)\ny = ifelse(x2&gt;x1,-1,1); id = 1:N\nt = seq(-1.5,1.5,0.1)\ndat = cbind(y, x1, x2)\nHere, you can ignore the bias (constant) term in the classifier, just like what we discussed in lecture 1. Write your function and use it to find the sword of judgment!\nTask 11.2: Use the following code to get the data\n# Here, we will train a Proceptron algorithm to a subset of iris data\nX = iris[1:100,1:2]\ny = as.numeric(iris[1:100,5])\ndat = cbind(y, X)\nWrite your function and use it to find the sword of judgment!\n\n\nTask 12: Decipher Problem\nDo you know substitution cipher? A substitution cipher is a method of encryption where each letter in a text is replaced by another letter according to a specific system. For example, we use H to present D, \\(H \\to D\\), and similarly \\(A \\to T\\), and \\(U \\to A\\), then the substitution cipher of word DATA is HUAU. Next, I ask you a secrete question in a text and encrypt it using a substitution cipher. My cipher is simple; it only includes the 26 lowercase letters and space. The task is to write a program that implement the algorithm illustrated in the notes to crack my cipher and answer my secrete question.\n\nMy ciphertext: download it here: download it here\nThe transition probability matrix: download it here\n\n\n\nCongratulations! You have become so powerful!\n\n\nLecture 2 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_lab_home.html",
    "href": "Courses/c_mlwr1_2024/l4/l4_lab_home.html",
    "title": "Lab 2: Exercises about Gaussian Discriminant Analysis",
    "section": "",
    "text": "In this lab, we implement and practice Gaussian Discriminant Analysis with Breast Cancer Data. The breast cancer data set comprises 31 variables, with “diagnosis” indicating the diagnostic results of tumor samples, where M stands for malignant and B for benign. The remaining variables are extracted from medical images. For more information, read help.\nTasks: According to the requirements in each task, using data in the file BreastCancerTrain, we train a classifier with diagnosis as the target variable and radius, texture, and smoothness as the feature variables. We also test the resulting classifier on the data set in the file BreastCancerTest."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_lab_home.html#task-1",
    "href": "Courses/c_mlwr1_2024/l4/l4_lab_home.html#task-1",
    "title": "Lab 2: Exercises about Gaussian Discriminant Analysis",
    "section": "Task 1:",
    "text": "Task 1:\nWe assumed that for different diagnostic results, the feature variables belong to different normal distributions. Write an R function to implement a classifier based on the following decision rules \\[\n    \\widehat{y}=\\arg\\max_{y} f(\\textbf{x}|y)\n\\] where \\(f(x|y)\\) is the conditional normal density function. Estimate the unknown parameter based on the training data set and then apply this classifier to the testing data set. Report the accuracy, sensitivity, specificity, and kappa statistics.\nTips:\n\nThe data can be imported by the function write.table. You need to set header as TRUE and set as ,.\nThe inputs of the function should include variables x, mu_1, S_1, mu_2, and S_2, where x is an array containing the three feature variables, mu_x is the estimated mean vector of category x, S_x is the estimated covariance matrix of category x.\nFunctions colMeans and cov can be applied to estimate the unknown parameters.\nThe function dmvnorm in the package mvtnorm can be applied to calculate the normal density value. For details, read the help document of this function.\nThe function confusionMatrix in the package caret can be used to calculate different metrics of performance.\n\nNote: The first two inputs should be type of factor. Also, you need to specify the positive as M i.e. malignant."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_lab_home.html#task-2",
    "href": "Courses/c_mlwr1_2024/l4/l4_lab_home.html#task-2",
    "title": "Lab 2: Exercises about Gaussian Discriminant Analysis",
    "section": "Task 2:",
    "text": "Task 2:\nModify the R function in task 1 such that the classifier based on the following decision rules is implemented. \\[\n    \\widehat{y}=\\arg\\max_{y} P(y|\\textbf{x})\n\\] Apply the modified classifier to the testing data set and report the accuracy, sensitivity, specificity, and kappa statistics.\nTips: The prior probability can be estimated as the rate of M in the training data set."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_lab_home.html#task-3",
    "href": "Courses/c_mlwr1_2024/l4/l4_lab_home.html#task-3",
    "title": "Lab 2: Exercises about Gaussian Discriminant Analysis",
    "section": "Task 3:",
    "text": "Task 3:\nTask 3.1: Apply lda function to estimate an LDA classifier based on the training data set. Test the resulting classifier with the testing set and report the accuracy, sensitivity, specificity, and kappa statistics.\nTask 3.2: Given the output of lda function in task 3.1, write down the expression of the corresponding classifier.\nTask 3.3: Review your solution in Task 1. Is your solution in Task 1 a LDA classifier or QDA classifier?\n\nLecture 4 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_home.html#discussion",
    "href": "Courses/c_mlwr1_2024/l4/l4_home.html#discussion",
    "title": "Lecture 4: Gaussian Discrimination Analysis",
    "section": "Discussion",
    "text": "Discussion\nSlides for discussion: click here"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_home.html#lesson",
    "href": "Courses/c_mlwr1_2024/l4/l4_home.html#lesson",
    "title": "Lecture 4: Gaussian Discrimination Analysis",
    "section": "Lesson:",
    "text": "Lesson:\nLaboratory Entrance\nSolutions: Click here\nR file: download here.\n\nCourse Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#linear-regression-model",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#linear-regression-model",
    "title": "Lecture 5: Regression Models",
    "section": "",
    "text": "Linear regression model: \\[\n  y = w_0 + w_1 x_1 + \\dots w_px_p + \\epsilon\n\\] Based on this model, we assume a mechanism the target variable is generated as the sum of \\(w_0 + w_1 x_1 + \\dots w_px_p\\) and a error term, \\(\\epsilon\\). The error term contains many things. It could be measurement errors, or random noise, or all the variations that can not be explained by all the feature variables. For the latest case, it also means we need more feature variables for a better prediction of the target variable.\nObviously, we have to apply some methods (algorithm) to learn (estimate) the coefficients \\(w_0, w_1, \\dots, w_p\\) from a data set. Here, we mainly discuss two methods, least square methods and maximum likelihood method. Eventually, the two methods are equivalent for a regression problem, however, it is still necessary to explain maximum likelihood method for understanding logistic regression in lecture 7.\n\n\nLeast Square method was proposed by the famous mathematician Gauss. He applied this method for data analysis to accurately predict the time of the second appearance of the asteroid, Ceres. His idea is quite simple: to use data to find the optimal line, represented by two coefficients, in order to minimize prediction errors. Suppose that we have a set of paired observations, \\((y_1,x_1), \\dots, (y_n, x_n)\\), then the mathematical formulation is \\[\n  \\hat{w}_0, \\hat{w}_1 = \\arg\\min_{w_0, w_1} \\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n\\] where \\(\\hat{y}_i = w_0 + w_1x_i\\).\nThe solution of this optimization problem is \\(\\widehat{w}_1=\\frac{\\sum_{i=1}^N(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^N(x_i-\\overline{x})^2}\\), and \\(\\widehat{w}_0=\\overline{y}-\\widehat{w}_1\\overline{x}\\)\n\n\n\nDifferent from least square method, next, we are going to reexamine the regression model from the perspective of probability models. To do so, we assume the error term \\(\\epsilon\\) is normally distributed, \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\). Based on this assumption, the target variable is normally distributed conditional on feature variables. Therefore, we essentially predict the expected value of the target variable conditional on \\(X_1, \\dots, X_p\\) as a linear model, i.e.\n\n\n\nRegression Model: the expected value of target variable is a linear function of X1 and X2\n\n\n\\[\n  \\text{E}(Y | X_1, \\dots, X_p) = w_0 + w_1 X_1 + w_2 X_2 + \\dots + w_p X_p\n\\] Based on the normality assumption, another estimation method, MLE, for coefficients can be discussed.\nMLE of regression model: Under the normality assumption, we have \\(y_i \\sim \\mathcal{N}( w_0 + w_1x_1 , \\sigma^2)\\). If you remember the secrete message behind the normal distribution, the likelihood of each observation, \\(y_i\\), is inversely proportionally to the distance to the expected value, i.e.  \\[\n   f( y_i | w_0, w_1, \\sigma^2 ) \\propto -(y_i - (w_0 + w_1x_1 ) )^2\n\\] Therefore the likelihood function of the sample \\(\\left\\{ y_i, x_i \\right\\}_{i=1}^n\\) is \\[\n  \\log \\left( L( w_0, w_1, \\sigma^2 | (y_i, x_i) ) \\right) \\propto -\\sum_{i=1}^n (y_i - (w_0 + w_1x_1 ) )^2\n\\]\nNotice that, on the LHS, it is sum square of residual. Therefore, minimize sum square of residuals is equivalent to maximize the log likelihood function. In other words, the two methods are equivalent."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#nonlinear-regression-models",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#nonlinear-regression-models",
    "title": "Lecture 5: Regression Models",
    "section": "5.2 Nonlinear Regression Models",
    "text": "5.2 Nonlinear Regression Models\nSo far, we have studied two linear models, however, it is by no means enough to solve the problems in reality. For example, with the two following questions, it’s challenging to find a linear model that performs well.\n\n\n\n\n\nLHS: A regression problem. RHS: A classification problem.\n\n\n\n\n\nBasic ideas of Non-linear Extension\nOf course, nonlinear models are not the focus of our course. However, here we can explore the basic approach to finding nonlinear models, that is feature mapping. Feature mapping involves the basic idea of introducing new variables by transforming the original feature variables with functions. This expands the original feature space, allowing the exploration of potential linear solutions within the augmented feature space. Let’s start from the example of classification problem.\nIn the classification problem, we can consider three new variables \\(x_1^2, \\sqrt{2}x_1x_2, x_2^2\\) instead of the two original variables \\(x_1, x_2\\). The new data set is visualzied in the following plot.\n\n\n\nAugmented Feature Space\n\n\nAs we can see from the LHS, the classification problem becomes a linearly separable case in the augmented feature space. Also, if we change the direction of our view, the linear model in the augmented feature space is eventually a nonlinear model in the original space.\n\n\nPolynomial Regression Model\nIn the regression problem, we also can consider an augmented feature space \\((x, x^2, x^3)\\) according to the data visualization. In other words, we consider the true model as \\(y = w_0 + w_1x + w_2x^2 + w_3x^3\\) which is the 3rd order polynomial function of \\(x\\). We call this regression model as polynomial regression model, however, it is an essentially a linear regression in the augmented feature space.\n\n\nSpline Regression Model\n\n\nNon-parametric Regression Model"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#over-fitting-problems",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#over-fitting-problems",
    "title": "Lecture 5: Regression Models",
    "section": "5.3 Over Fitting Problems",
    "text": "5.3 Over Fitting Problems\n\n5.3.1 Motivating Examples\n\n\n5.3.2 Over Fitting\n\n\n5.3.3\n\nLecture 5 Homepage"
  },
  {
    "objectID": "Skalds/20241115_Likelihood_Analysis.html#maximum-likelihood-estimation",
    "href": "Skalds/20241115_Likelihood_Analysis.html#maximum-likelihood-estimation",
    "title": "Introduction to Likelihood Analysis",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation"
  },
  {
    "objectID": "Skalds/20241115_Likelihood_Analysis.html#likelihood-ratio-test",
    "href": "Skalds/20241115_Likelihood_Analysis.html#likelihood-ratio-test",
    "title": "Introduction to Likelihood Analysis",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test"
  },
  {
    "objectID": "Skalds/20241115_Likelihood_Analysis.html",
    "href": "Skalds/20241115_Likelihood_Analysis.html",
    "title": "Introduction to Likelihood Analysis",
    "section": "",
    "text": "In our daily lives, we often need to infer underlying patterns from observed phenomena. Whether it’s predicting the weather, analyzing the stock market, or studying the spread of diseases, statistics provides us with powerful tools for making such inferences. Among these statistical methods, one key concept is frequently used to help us understand and estimate these patterns: likelihood.\nSimply put, likelihood is a way of quantifying the “support” for a given model. It tells us how likely a specific model or parameter is to be correct, given the observed data. Although “probability” and “likelihood” may seem similar, they have fundamental differences in both meaning and application. In this article, we will explore the basic concept of likelihood and its critical applications in statistics, including how it is used to estimate model parameters, select the best model, and even perform hypothesis testing.\nThrough simple examples and intuitive explanations, we aim to help you understand this omnipresent concept in data science and appreciate its powerful role in modern statistics."
  },
  {
    "objectID": "Skalds/20241115_Likelihood_Analysis.html#section",
    "href": "Skalds/20241115_Likelihood_Analysis.html#section",
    "title": "Introduction to Likelihood Analysis",
    "section": "1.3",
    "text": "1.3"
  },
  {
    "objectID": "Skalds/20241115_Likelihood_Analysis.html#probability-v.s.-likelihood",
    "href": "Skalds/20241115_Likelihood_Analysis.html#probability-v.s.-likelihood",
    "title": "Introduction to Likelihood Analysis",
    "section": "1.2 Probability V.S. Likelihood",
    "text": "1.2 Probability V.S. Likelihood\nTo understand the difference between these two concepts, let’s first analyze the following two scenarios.\n\nScenario 1: “Imagine you are standing in the square in the city center of Umeå, close your eyes for 30 seconds, then open your eyes and catch the first man you see. How likely is his height above 180 cm?”\n\n\nScenario 2: “You went to downtown last weekend. You stood in the square and closed your eyes, then after 30 seconds you opened your eyes and grabbed the man you saw first and measured his height. His height is 230 and you think it is unbelievable.”\n\nObviously, both scenarios involve descriptions of possibility, but their key terms determine that they are fundamentally different. The keyword in Scenario 1 is “imagine,” which suggests that this is merely a thought in our minds and has not actually occurred. In this case, we typically use probability to describe the likelihood of an event happening.\nIn contrast, the keywords in Scenario 2 are “went,” “stood,” and other past tense verbs, which imply that these are events that actually happened. Correspondingly, we use likelihood to quantify the possibility of observing this value under a certain model assumption. Therefore, the essential difference between probability and likelihood lies in whether they describe the possibility of an event or the possibility of an observation.\n\n1.2.1 Probability: Quantifying Possibilities in a Rational World\nIn Scenario 1, the focus is on an event that has not yet occurred (“catching a man taller than 180 cm”). The key question is: “Given a known distribution, how likely is this event to happen?”\nCalculating this probability relies entirely on a rational assumption: if men’s heights follow a normal distribution \\(\\mathcal{N}(180, 7^2)\\), we use the probability density function to compute \\(\\Pr(X &gt; 180)\\).\nProbability reflects a conceptual possibility that exists in the realm of thought, regardless of whether such an event actually occurs. In other words, probability belongs to the rational world of abstract reasoning and does not depend on real-world observations. In this sense, probability is a rational quantification of theoretical events. By assuming a model (e.g., a normal distribution), we derive the theoretical likelihood of an event. Even if the event never happens, its probability remains well-defined.\n\n\n1.2.2 Likelihood: Evaluating Observed Data in Reality\nIn Scenario 2, the event has already occurred: you measured a height of 230 cm. The observation is a fixed fact, and the question becomes: “Given the assumed model (e.g., \\(\\mathcal{N}(180, 7^2)\\)), how plausible is this observation?”\nThe likelihood function \\(L(\\mu, \\sigma | X = x_0)\\) quantifies how well this observation \\(x_0\\) aligns with the model. Unlike probability, likelihood is entirely dependent on the observed data. It evaluates the plausibility of the model given the data, rather than predicting hypothetical events. As mentioned earlier, for continuous random variables, the likelihood of an observation \\(x_0\\) can be quantified using the value of the density function, i.e. \\(L(\\mu, \\sigma | X = 230) = f(230; \\mu = 180, \\sigma = 7)\\), where \\(f\\) is the density function of normal distribution.\nIt is worth mentioning that the p.m.f of a discrete random variable represents both the probability of a single point and its likelihood. I summarize these discussions in the table below:\n\n\n\n\n\n\n\n\n\n\n\nConcept\nObject\nDiscrete Variable\nContinuous Variable\n\n\n\n\nProbability\nMathematical\nEvents\n\\(Pr(X = 1)\\) p.m.f.\n\\(\\Pr(X&lt;b) = \\int_{-\\infty}^bf(x)dx\\)\n\n\nLikelihood\nStatistical\nObservations\n\\(Pr(X = 1)\\) p.m.f.\n\\(f(x)\\) p.d.f."
  },
  {
    "objectID": "Skalds/20241115_Likelihood_Analysis.html#the-mystery-of-probability-density-function",
    "href": "Skalds/20241115_Likelihood_Analysis.html#the-mystery-of-probability-density-function",
    "title": "Introduction to Likelihood Analysis",
    "section": "1.1 The Mystery of Probability Density Function",
    "text": "1.1 The Mystery of Probability Density Function\nIn statistics, we typically use Probability Mass Functions (p.m.f) and Probability Density Functions (p.d.f) to describe different types of random variables. Let’s start by discussing discrete random variables.\nFor a discrete random variable, such as flipping a fair coin, we use the Probability Mass Function (p.m.f) to describe the probability of each possible outcome. For example, let \\(X\\) be a binary distributed random variable representing the number of heads in a single coin toss. The p.m.f \\(f(x) = p^{x}(1-p)^{1-x}\\), where \\(x = 1 \\text{ or } 0\\), gives the probability that the random variable \\(X\\) takes the value \\(x\\). For instance, if \\(X\\) represents the result of a coin flip, \\(f(1) = 0.5\\) indicates that the probability of getting heads is \\(0.5\\).\nFor continuous random variables, however, although we have a similar concept, i.e. one can use the Probability Density Function (p.d.f) to describe the distribution, the situation is not such straightforward as p.m.f. Suppose \\(X\\) is a continuous random variable with p.d.f \\(f(x)\\), then different from p.m.f example above, the value of \\(f(a)\\) doesn’t present the probability that random variable \\(X\\) taking value \\(x\\). As we know, the probability that a continuous random variable taking a specific value is just \\(0\\), i.e. \\[\n  \\Pr(X = a) = \\int_{a}^{a}f(x)dx = 0\n\\] So, what is the meaning of this density value? Let’s take a more specific example. I think that on the streets of Sweden, you’re more likely to encounter someone around 175 cm tall than someone 200 cm tall. Whether or not you’ve been to Sweden, I believe you’ll agree with me. Suppose the height of adult Swedish man exactly follow the Normal distribution, with mean 180 cm and SD 8 cm.\n\n\n\n\n\n\n\n\n\nThe information conveyed by the density function aligns perfectly with our intuition. The density value at 175 is higher than that at 200, indicating that it is more likely to encounter a height of 175 than 200. In other words, the density function provides a measure of the likelihood of observing a certain value. However, when it comes to describing the concept of likelihood mathematically, your mind might immediately think of probability. Yet, in this context, we are not using probability to express likelihood but rather the concept of likelihood itself. In other words, we can answer the question before, that the meaning of density value is likelihood value. You might say I’m just playing with words here—aren’t likelihood and probability just different names for the same concept? Don’t they essentially mean the same thing? Not really. Let’s take a closer look."
  },
  {
    "objectID": "Skalds/20241115_Likelihood_Analysis.html#secret-messages-behind-normal-density-function",
    "href": "Skalds/20241115_Likelihood_Analysis.html#secret-messages-behind-normal-density-function",
    "title": "Introduction to Likelihood Analysis",
    "section": "1.3 Secret Messages behind Normal Density Function",
    "text": "1.3 Secret Messages behind Normal Density Function\nYou surely remember the previous question: what does the p.d.f of the normal distribution represent?\n\\[\n  f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} } e^{- \\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma}\\right)^2 }\n\\]\nOr, to put it another way, just as the binary distribution aims to describe random events like coin tossing, what kind of random phenomenon is the normal distribution trying to describe?\nLet’s start with en elementary understanding.\n\nThe normal distribution describes a bell-shaped, symmetric distribution of random phenomena, such as human IQ. Most people’s IQs are close to the mean, a smaller portion have higher or lower IQs, and only a very few have extremely high or low IQs. This forms a bell-shaped, symmetric distribution.\nWe can use more precise statistical terminology, such as confidence intervals, to describe this bell-shaped symmetric distribution of random phenomena. The normal distribution describes a random phenomenon where 68% of the probability is covered by the confidence interval of one SD around the mean, while the confidence interval of two SDs can cover 95% of the probability, and the confidence interval of three SDs can cover nearly all possibilities, as shown in the figure below.”\n\n\n\n\n\n\nIf you answer the question in such ways, then congratulations. You have rather good learning outputs from the previous basic statistics course. Next, let me introduce to you a deeper understanding based on the concept of likelihood. Let’s review the p.d.f of the normal distribution. Let’s watch an animation first.\n\n\n\nNote that the negative sign determines the inversely proportional relationship. The letter \\(e\\) represents the exponential function, which determines the rate of change.\n\n\nNotice that \\(\\left(\\frac{x-u}{\\sigma}\\right)^2\\) is the normalized distance between an observation \\(x\\) and the center point \\(\\mu\\). As mentioned before, the value of density function is the likelihood value of one observation, so the p.d.f is presenting the relationship between a specific observation \\(x\\) and its likelihood value. Then Normal distribution is describing such kind of random phenomenon:\n\nThe likelihood of an observation \\(x_0\\) is inversely proportion to the normalized distance between observed value \\(x_0\\) and the mean value \\(\\mu\\).\nMore precisely, when the observed value is far from the center point, the likelihood of observing it will decrease rapidly, and this decrease is exponential, as displayed below.\n\n\n\n\n\n\nIn summary, the normal distribution essentially describes the relationship between the distance of observed values from the center point and likelihood within a class of random phenomena. It connects the geometric concept of distance with the statistical concept of likelihood."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#polynomial-regression-model",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#polynomial-regression-model",
    "title": "Lecture 5: Regression Models",
    "section": "5.2.2 Polynomial Regression Model",
    "text": "5.2.2 Polynomial Regression Model\nIn this section, we will introduce a nonlinear regression model using the feature mapping idea, specifically focusing on polynomial regression. Let’s begin by looking at a simple example.\n\n\n\n\n\nRegression Toy Example\n\n\n\n\nIn this example, we are dealing with a nonlinear regression problem, where the relationship between the feature variable and the target variable is clearly nonlinear. A linear model would not provide satisfactory results because it assumes a straight-line relationship between the features and the target. As you can see, the blue line in the plot, i.e. the simple linear model has totally unacceptable performance on the whole domain. However, by using the feature mapping idea, we can transform the feature space into a higher-dimensional space, where the relationship becomes linear, making it possible to apply linear regression successfully. To do so, we introduce the augmented feature space by the feature mapping \\(\\phi(x) = (x, x^2)\\), i.e. a mapping from 1D to 2D, see the 3D scatter plot below\n\n\n\n\n\n\nYou can see that all the points stand on a plane in the augmented space which means that we can find a linear model, i.e.  \\[\n  y = w_0 + w_1x + w_2x^2,\n\\] to solve the problem.\n\nRemark: I believe you can see that the choice of feature mapping, \\(\\phi()\\), or basis functions, is extremely important. If we choose an inappropriate set of feature mappings, we may end up with very poor results. As shown in the figure below, we apply \\(\\phi(x) = (x, x^5)\\) to obtain the augmented feature space, then we eventually switch to another nonlinear problem. In other words, we can’t find a plane in the new space such that all the sample points roughly all stand on it. So, we naturally have the following question and we will answer it in the next lecture. ¢ Question: How to choose an appropriate feature mapping?\n\n\n\n\n\n\n\nFrom another perspective, essentially, our idea in solving the above problem is to use polynomial functions to represent a nonlinear curve and then fit the data with this function. We refer to this regression model as polynomial regression model. The generic form of \\(p\\)-th order polynomial regression is \\[\n  y = w_0 + w_1x + w_2x^2 + \\dots + w_px^p + \\epsilon\n\\]"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#spline-regression-model",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#spline-regression-model",
    "title": "Lecture 5: Regression Models",
    "section": "5.2.3 Spline Regression Model",
    "text": "5.2.3 Spline Regression Model"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#non-parametric-regression-model",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#non-parametric-regression-model",
    "title": "Lecture 5: Regression Models",
    "section": "5.2.4 Non-parametric Regression Model",
    "text": "5.2.4 Non-parametric Regression Model"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_lab_solutions.html",
    "href": "Courses/c_mlwr1_2024/l4/l4_lab_solutions.html",
    "title": "Solutions to Exercises in Lab 2",
    "section": "",
    "text": "In this lab, we implement and practice Gaussian Discriminant Analysis with Breast Cancer Data. The breast cancer data set comprises 31 variables, with “diagnosis” indicating the diagnostic results of tumor samples, where M stands for malignant and B for benign. The remaining variables are extracted from medical images. For more information, read help.\nTasks: According to the requirements in each task, using data in the file BreastCancerTrain, we train a classifier with diagnosis as the target variable and radius, texture, and smoothness as the feature variables. We also test the resulting classifier on the data set in the file BreastCancerTest.\nImport data:\n# Import data\ndat_tr = read.table(\"BreastCancerTrain.txt\", header = T, sep = \",\")\ndat_te = read.table(\"BreastCancerTest.txt\", header = T, sep = \",\")"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_lab_solutions.html#task-1",
    "href": "Courses/c_mlwr1_2024/l4/l4_lab_solutions.html#task-1",
    "title": "Solutions to Exercises in Lab 2",
    "section": "Task 1:",
    "text": "Task 1:\nWe assumed that for different diagnostic results, the feature variables belong to different normal distributions. Write an R function to implement a classifier based on the following decision rules \\[\n    \\widehat{y}=\\arg\\max_{y} f(\\textbf{x}|y)\n\\] where \\(f(x|y)\\) is the conditional normal density function. Estimate the unknown parameter based on the training data set and then apply this classifier to the testing data set. Report the accuracy, sensitivity, specificity, and kappa statistics.\n Solutions to Task 1: \nPrepare the target variable y and the data matrix of feature variables X\n\ny = dat_tr$Diagnosis \nX = as.matrix(dat_tr[,c(2,3,6)]) # choose 'radius', 'texture', and 'smoothness'\n\n Estimate the mean vector and covariance matrix for each class.\n\n# estimate all means and covaraince matrix first. \n# We use them for evaluate the likelihood value in line 26 and 27\nmu_m = colMeans(X[which(y == \"M\"), ]) # estimate the means for M groups\nS_m = cov(X[which(y == \"M\"), ]) # estimate the covariance matrix for M groups\nmu_b = colMeans(X[which(y == \"B\"), ]) # estimate the means for B groups\nS_b = cov(X[which(y == \"B\"), ]) # estimate the covariance matrix for B groups\n\n Write the function classifier with values of feature variables for a new case x and model parameters mu1, S1, mu2, S2 as inputs. Here, we need function dmvnorm in package mvtnorm to evaluate the density value for the new case x, i.e. calculate the conditional likelihood of the new case. The final prediction is made by comparing the two conditional likelihood values.\n\nlibrary(mvtnorm) # need function 'dmvnorm' in this package \n# function for making decision based on the idea of GDA\nclassifier = function(x,mu1,S1,mu2,S2){\n  ell1 = dmvnorm(x,mu1,S1) # the likelihood of x if assume it is from group 1\n  ell2 = dmvnorm(x,mu2,S2) # the likelihood of x if assume it is from group 2\n  res = ifelse(ell1 &gt; ell2, \"M\", \"B\") # make decision\n  return(res)\n}\n\n In the testing stage, we prepare the target variable and the feature variables data matrix for testing first.\n\ny_te = dat_te$Diagnosis \nX_te = as.matrix(dat_te[,c(2,3,6)])\n\n Apply the function classifier to each row of feature variable data matrix X_te and record the prediction results in y_pre.\n\nN_te = dim(dat_te)[1] # Test Sample size\ny_pre = numeric(N_te)\nfor(i in 1:N_te){ # loop over all observations in the testing set\n  y_pre[i] = classifier(X_te[i, ], mu_m, S_m, mu_b, S_b) # apply our classifier\n}\n\n Compare the prediction results with truth y_pre == y_te. The results are a vector of logical values TRUE or FALSE. If we calculate the mean of this array, logical values will be transformed as 1 or 0 first, and then calculate the mean, i.e. the accuracy.\n\nmean(y_pre == y_te) # accuracy\n\n[1] 0.9035088\n\n\n For more model performance statistics, we can apply the function confusionMatrix in package caret. It will return not only confusion matrix, but other statistics, like sensitivity, specificity, Kappa, and so on.\n\nlibrary(caret) # to calculate the confusion matrix and kappa statistic\nconfusionMatrix(as.factor(y_pre), as.factor(y_te), positive = \"M\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  B  M\n         B 69  5\n         M  6 34\n                                          \n               Accuracy : 0.9035          \n                 95% CI : (0.8339, 0.9508)\n    No Information Rate : 0.6579          \n    P-Value [Acc &gt; NIR] : 1.123e-09       \n                                          \n                  Kappa : 0.787           \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.8718          \n            Specificity : 0.9200          \n         Pos Pred Value : 0.8500          \n         Neg Pred Value : 0.9324          \n             Prevalence : 0.3421          \n         Detection Rate : 0.2982          \n   Detection Prevalence : 0.3509          \n      Balanced Accuracy : 0.8959          \n                                          \n       'Positive' Class : M"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_lab_solutions.html#task-2",
    "href": "Courses/c_mlwr1_2024/l4/l4_lab_solutions.html#task-2",
    "title": "Solutions to Exercises in Lab 2",
    "section": "Task 2:",
    "text": "Task 2:\nModify the R function in task 1 such that the classifier based on the following decision rules is implemented. \\[\n    \\widehat{y}=\\arg\\max_{y} \\Pr(y|\\textbf{x})\n\\] Apply the modified classifier to the testing data set and report the accuracy, sensitivity, specificity, and kappa statistics.\n Solutions to Task 2:\n To make the decision with posterior probability, \\(\\Pr(y|\\textbf{x})\\), we need to correct the prediction with the prior probability \\(\\Pr(y)\\). To do so, we need to estimate the prior probability with the training data first, then correct the prediction accordingly.\n\np_m = mean(y == \"M\") # estimate the prior probability\nclassifier = function(x,mu1,S1,mu2,S2,p_m){\n  # Here, we use function 'dmvnorm' in package 'mvtnorm'\n  ell1 = dmvnorm(x,mu1,S1)*p_m # modify the likelihood by prior probability \n  ell2 = dmvnorm(x,mu2,S2)*(1-p_m)\n  res = ifelse(ell1 &gt; ell2, \"M\", \"B\")\n  return(res)\n}\n\nfor(i in 1:N_te){\n  y_pre[i] = classifier(X_te[i, ], mu_m, S_m, mu_b, S_b, p_m)\n}\nmean(y_pre == y_te)\n\n[1] 0.9210526\n\nconfusionMatrix(as.factor(y_pre), as.factor(y_te), positive = \"M\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  B  M\n         B 71  5\n         M  4 34\n                                          \n               Accuracy : 0.9211          \n                 95% CI : (0.8554, 0.9633)\n    No Information Rate : 0.6579          \n    P-Value [Acc &gt; NIR] : 3.991e-11       \n                                          \n                  Kappa : 0.8235          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.8718          \n            Specificity : 0.9467          \n         Pos Pred Value : 0.8947          \n         Neg Pred Value : 0.9342          \n             Prevalence : 0.3421          \n         Detection Rate : 0.2982          \n   Detection Prevalence : 0.3333          \n      Balanced Accuracy : 0.9092          \n                                          \n       'Positive' Class : M"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_lab_solutions.html#task-3",
    "href": "Courses/c_mlwr1_2024/l4/l4_lab_solutions.html#task-3",
    "title": "Solutions to Exercises in Lab 2",
    "section": "Task 3:",
    "text": "Task 3:\nTask 3.1: Apply lda function to estimate an LDA classifier based on the training data set. Test the resulting classifier with the testing set and report the accuracy, sensitivity, specificity, and kappa statistics.\n Solutions to Task 3.1:\n Now, we directly apply the function lda in MASS package. First, we need to make sure the class of dat_tr is data frame. With this type of data, the usage of lda is very simple. First, we use variable names to create the model, e.g. Diagnosis~radius+texture+smoothness, then specify the data set. If you have proper prior probability from previous study, you also can specify it with argument prior, otherwise, the prior will be estimated from the input data.\n\nlibrary(MASS)\n# estimate the LDA model by function 'lda'\nm = lda(Diagnosis~radius+texture+smoothness, data = dat_tr, prior = c(0.5,0.5))\n\n The prediction stage is also very straightforward. We can apply function predict for prediction. However, you have to make sure the input data for newdata argument should have the same variable names as the training data dat_tr and it also has to be a data frame.\n\n# apply the output model 'm' on observations in the testing set.\nres = predict(m, newdata = dat_te)\n\n The prediction results are saved in res. By checking the structure of res, we can see that there are 3 slots in the list, and the first two are the most important. $class can be directly used as the prediction results. $posterior is the predicted posterior probability for each case.\n\nstr(res)\n\nList of 3\n $ class    : Factor w/ 2 levels \"B\",\"M\": 2 2 2 2 1 1 2 1 1 1 ...\n $ posterior: num [1:114, 1:2] 0.02072 0.00151 0.16858 0.43011 0.99986 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:114] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:2] \"B\" \"M\"\n $ x        : num [1:114, 1] 1.4 2.358 0.58 0.102 -3.213 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:114] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr \"LD1\"\n\n\n\ny_pre = res$class\nmean(y_pre == y_te)\n\n[1] 0.9210526\n\n\nTask 3.2: Given the output of lda function in task 3.1, write down the expression of the corresponding classifier.\n\n# weights:\n(w = m$scaling)\n\n                  LD1\nradius      0.3593405\ntexture     0.1225361\nsmoothness 38.4264427\n\n# bias:\n(w_0 = mean(m$means%*%m$scaling))\n\n[1] 11.50199\n\n\nTask 3.3: Review your solution in Task 1. Is your solution in Task 1 a LDA classifier or QDA classifier?  In fact, the solution in Task 1 is a QDA classifier as we estimate the covariance matrix for each class separately. If one want to create the same function as lda, the pooled covaraince matrix has to be estimated. To do so, we remove the means from the feature varaibles data matrix for each class first, then use them to estimate the pooled contrivance matrix.\n\nX_M = X[which(y == \"M\"), ]\nN_XM = dim(X_M)[1]\nX_M = X_M - matrix(rep(mu_m, N_XM), nrow = N_XM)\n\nX_B = X[which(y == \"B\"), ]\nN_XB = dim(X_B)[1]\nX_B = X_B - matrix(rep(mu_b, N_XB), nrow = N_XB)\n\nS_pooled = cov(rbind(X_M, X_B))\n\nclassifier = function(x,mu1,mu2,S,p_m = 0.5){\n  # Here, we use function 'dmvnorm' in package 'mvtnorm'\n  ell1 = dmvnorm(x,mu1,S)*p_m # modify the likelihood by prior probability \n  ell2 = dmvnorm(x,mu2,S)*(1-p_m)\n  res = ifelse(ell1 &gt; ell2, \"M\", \"B\")\n  return(res)\n}\n\nclassifier(X_te[1, ], mu_m, mu_b, S_pooled, 0.5)\n\n[1] \"M\"\n\n\n After the modification, the function classifier has exactly same prediction results as the model obtained by function lda.\n\nLecture 4 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#maximum-likelihood-method-ne",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#maximum-likelihood-method-ne",
    "title": "Lecture 5: Regression Models",
    "section": "5.1.3 Maximum Likelihood Method ( NE )",
    "text": "5.1.3 Maximum Likelihood Method ( NE )\nDifferent from least square method, next, we are going to reexamine the regression model from the perspective of probability models. To do so, we assume the error term \\(\\epsilon\\) is normally distributed, \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\). Based on this assumption, the target variable is normally distributed conditional on feature variables. Therefore, we essentially predict the expected value of the target variable conditional on \\(X_1, \\dots, X_p\\) as a linear model, i.e.\n\n\n\nRegression Model: the expected value of target variable is a linear function of X1 and X2\n\n\n\\[\n  \\text{E}(Y | X_1, \\dots, X_p) = w_0 + w_1 X_1 + w_2 X_2 + \\dots + w_p X_p\n\\] Based on the normality assumption, another estimation method, MLE, for coefficients can be discussed.\nMLE of regression model: Under the normality assumption, we have \\(y_i \\sim \\mathcal{N}( w_0 + w_1x_1 , \\sigma^2)\\). If you remember the secrete message behind the normal distribution, the likelihood of each observation, \\(y_i\\), is inversely proportionally to the distance to the expected value, i.e.  \\[\n   f( y_i | w_0, w_1, \\sigma^2 ) \\propto -(y_i - (w_0 + w_1x_1 ) )^2\n\\] Therefore the likelihood function of the sample \\(\\left\\{ y_i, x_i \\right\\}_{i=1}^n\\) is \\[\n  \\log \\left( L( w_0, w_1, \\sigma^2 | (y_i, x_i) ) \\right) \\propto -\\sum_{i=1}^n (y_i - (w_0 + w_1x_1 ) )^2\n\\]\nNotice that, on the LHS, it is sum square of residual. Therefore, minimize sum square of residuals is equivalent to maximize the log likelihood function. In other words, the two methods are equivalent."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#motivating-examples",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#motivating-examples",
    "title": "Lecture 5: Regression Models",
    "section": "5.3.1 Motivating Examples",
    "text": "5.3.1 Motivating Examples\n\n\n\n\n\n\n\n\n\nHere we have a set of data (blue triangles), and we want to use the values of the horizontal coordinate \\(X\\) to predict the vertical coordinate \\(Y\\). There are two candidate models.\n\nRed model: 2nd-degree polynomial regression, which performs well enough, but makes some errors on the three points on the right side.\nOrange model: 4th-degree polynomial regression, which performs perfectly because it passes exactly through every observation point, meaning it makes no errors at all.\n\nSo, the question is, which model do you think is better?\nYou might like the orange model because it is “perfect.” Indeed, people often pursue perfection, but behind perfection often lies a trap. Let me tell you the truth. As shown in the figure below, all the observation points are generated by the light blue dashed line plus a normal noise term. Now, let’s generate another observation point, the green triangle. At this point, the red model still performs well, but the prediction of the perfect orange model seems rather absurd."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#over-fitting",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#over-fitting",
    "title": "Lecture 5: Regression Models",
    "section": "5.3.2 Over Fitting",
    "text": "5.3.2 Over Fitting\nLet’s analyze this phenomenon. First, we need to weigh the pros and cons from two aspects, i.e. considering both the performance on the training samples and the testing samples. Therefore, I summarize the analysis of this phenomenon in the table below.\n\n\n\n\n\n\n\n\n\n\nOrange (4th order)\nRed (2nd order)\n\n\n\n\nTraining set\nPerfect\nGood enough\n\n\nTesting set\nPoor\nGood\n\n\nNumber of coefficients\n5\n3\n\n\nComplexity\nComplex\nSimple\n\n\n\nTraining Set Performance\n\nThe orange model perfectly fits the training data, capturing every fluctuation in the observations. This is often seen as an indicator of a model that is too complex, as it adjusts to every detail in the data, including noise or outliers.\nOn the other hand, the red model provides a “good enough” fit. It captures the overall trend of the data without fitting every minor variation, resulting in a simpler and more generalizable model.\n\nPerformance on testing set\n\nWhen we introduce new observations (those not included in the training set), the orange model’s performance drops significantly. Despite its “perfect” fit on the training data, it fails to generalize well to unseen data, indicating that it has overfitted the training set. Overfitting occurs when a model learns not only the true underlying pattern but also random fluctuations or noise in the training data. As a result, the model performs poorly on new, real-world data.\nConversely, the red model, with its simpler structure, generalizes better to new data points. It performs adequately on the training data and maintains reasonable performance on new observations, as it avoids overfitting by not capturing unnecessary complexity.\n\nThis phenomenon is called the overfitting problem. Simply put, if a model performs “perfectly” on the training samples but fails to generalize to new observations, it means we cannot apply the model for prediction, or in other words, we cannot extend the model to other observations.\nConsequences of Overfitting:\n\nThe model becomes overly sensitive to small variations in the training data, which doesn’t reflect the true distribution of data in the population.\nIt fails to make accurate predictions when new data points are introduced, making it less useful for real-world applications.\n\nSo, what causes the overfitting problem? Let me introduce a new concept, model complexity。 It refers to how intricate or detailed a model is in terms of its structure and the number of parameters it uses. A more complex model has more parameters or can capture more intricate patterns in the data.\nModel Complexity and Overfitting\n\nThe orange model has 5 coefficients, making it more complex and prone to overfitting. The extra flexibility allowed by the 4th-order polynomial gives the model the ability to fit noise, leading to poor generalization.\nThe red model, with only 3 coefficients, is simpler and less likely to overfit. While it may not fit the training data perfectly, its simpler structure helps it to better handle new observations.\n\nSummary: This example demonstrates the classic overfitting problem: while complex models (like the orange model) may perform well on the training data, they fail to generalize to new data. Simpler models (like the red model) strike a better balance between fitting the data and maintaining the ability to generalize. This highlights the importance of selecting the appropriate model complexity to ensure that the model performs well not just on the training data, but also in real-world scenarios where new, unseen data is encountered."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#section",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#section",
    "title": "Lecture 5: Regression Models",
    "section": "5.3.3",
    "text": "5.3.3\n\nLecture 5 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#spline-regression-model-ne",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#spline-regression-model-ne",
    "title": "Lecture 5: Regression Models",
    "section": "5.2.3 Spline Regression Model ( NE )",
    "text": "5.2.3 Spline Regression Model ( NE )"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#variance-and-bias-view",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#variance-and-bias-view",
    "title": "Lecture 5: Regression Models",
    "section": "5.3.4 Variance and Bias View",
    "text": "5.3.4 Variance and Bias View\nA 2nd-order polynomial regression model exhibits some prediction error for the \\(y\\) values in the training sample, whereas a 4th-order polynomial regression model always predicts the \\(y\\) values perfectly. As a result, the 4th-order model provides highly accurate predictions, while the 2nd-order model performs slightly worse. In statistical terms, the predictions of the 4th-order model have zero bias, whereas the 2nd-order model exhibits some bias.\n\n\n\n\n\n\n\nHowever, if we repeatedly generate five observations from the light blue model and estimate the regression model each time, we will observe that the 2nd-order model produces very stable predictions. As shown in the figure, all its curves remain approximately within a certain range. In contrast, the 4th-order model is highly unstable. In statistical terms, the 4th-order model has very high variance, while the variance of the 2nd-order model is much smaller. Therefore, overfitting occurs when the model has low bias and high variance. It fits the training data too closely, including noise, leading to excellent training performance but poor generalization on unseen data. So how can we avoid the overfitting problem? We will discuss this in the next section.\n\n\n\n\n\nVariance-Bias Tradeoff: We have five sets of data generated by the true model, represented by the light blue dashed line, each shown in a different color. On the left-hand side (LHS), we fit a 2nd-order polynomial regression model to each dataset, while on the right-hand side (RHS), we fit a 4th-order polynomial regression model. As you can see, the fitted models on the LHS are very stable. However, on the RHS, although the models perfectly pass through the observation points, they are extremely unstable.\n\n\n\n\n\nLecture 5 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#model-evaluation",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#model-evaluation",
    "title": "Lecture 5: Regression Models",
    "section": "5.1.4 Model Evaluation",
    "text": "5.1.4 Model Evaluation\nUnlike classification problems, model evaluation for regression problems is quite straightforward. From the purpose of regression, our ultimate goal is to use feature variables to estimate a continuous target variable. A good regression model naturally minimizes prediction error. Therefore, we typically use the mean squared error (MSE) of the model predictions for evaluation. That is \\[\n  \\text{MSE} = \\frac{1}{N} \\sum_{i = 1}^N \\left( y_i - \\hat{y}_i\\right)^2\n\\]\nwhere \\(\\hat{y}_i\\) is the prediction of the \\(i\\)th case, i.e. \\(\\hat{w}_0 + \\hat{w}_1 x_{i,1} + \\dots + \\hat{w}_p x_{i,p}\\), and therefore \\(y_i - \\hat{y}_i\\) is the prediction error of the \\(i\\)th case. To gain a more intuitive understanding, people often use RMSE (Root Mean Squared Error), i.e. \\(\\text{RMSE} = \\sqrt{\\text{MSE}}\\). Sometimes, people also use Mean Absolute Error (MAE) to evaluate a regression model. \\[\n  \\text{MAE} = \\frac{1}{N} \\sum_{i = 1}^N | y_i - \\hat{y}_i|\n\\] For all metrics, lower value indicates better model performance.\n\nQuiz: Do you know why we don’t use the average errors, i.e. \\(\\frac{1}{N} \\sum_{i = 1}^N \\left( y_i - \\hat{y}_i\\right)\\) to evaluate a regression model?\nTips: The simplest regression model is \\(y = w_0 + \\epsilon\\) and the estimation is \\(\\hat{w}_0 = \\bar{y}\\), i.e. \\(\\hat{y} = \\bar{y}\\)."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/Discussion of Lectures 4.html#lecture-4-gaussian-discreminant-analysis",
    "href": "Courses/c_mlwr1_2024/Discussion of Lectures 4.html#lecture-4-gaussian-discreminant-analysis",
    "title": "Discussion of Lectures 4",
    "section": "Lecture 4: Gaussian Discreminant Analysis",
    "text": "Lecture 4: Gaussian Discreminant Analysis\n\n\nCan you describe the basic idea of GDA to your families?\nIs GDA a linear classifier or non-linear classifier?\nCan we apply GDA to multiple class problems?\nWhat is the role of prior in GDA?\nDo you understand Fisher’s idea?\nWhy only calculating the accuracy of a classifier is not sufficient?\nDo you know any other statistics for model evaluation for classification problem?"
  },
  {
    "objectID": "test_area/test.html",
    "href": "test_area/test.html",
    "title": "Shiny in Quarto",
    "section": "",
    "text": "This is an example of embedding a Shiny app in Quarto.\n\n```{r} library(shiny)\n\n\nui &lt;- fluidPage( titlePanel(“Simple Shiny App”), sidebarLayout( sidebarPanel( sliderInput(“num”, “Choose a number:”, min = 1, max = 100, value = 50) ), mainPanel( plotOutput(“plot”) ) ) )\n\n\n\nserver &lt;- function(input, output, session) { output\\(plot &lt;- renderPlot({\n    hist(rnorm(input\\)num), main = “Histogram”, col = “steelblue”, border = “white”) }) }\nshinyApp(ui, server)"
  },
  {
    "objectID": "test_area/test.html#shiny-app-demo",
    "href": "test_area/test.html#shiny-app-demo",
    "title": "Shiny in Quarto",
    "section": "",
    "text": "This is an example of embedding a Shiny app in Quarto.\n\n```{r} library(shiny)\n\n\nui &lt;- fluidPage( titlePanel(“Simple Shiny App”), sidebarLayout( sidebarPanel( sliderInput(“num”, “Choose a number:”, min = 1, max = 100, value = 50) ), mainPanel( plotOutput(“plot”) ) ) )\n\n\n\nserver &lt;- function(input, output, session) { output\\(plot &lt;- renderPlot({\n    hist(rnorm(input\\)num), main = “Histogram”, col = “steelblue”, border = “white”) }) }\nshinyApp(ui, server)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#matrix-form-ne",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#matrix-form-ne",
    "title": "Lecture 5: Regression Models",
    "section": "5.1.2 Matrix Form ( NE )",
    "text": "5.1.2 Matrix Form ( NE )\nWhen we consider multiple feature variables in a linear regression model, the above calculation formula becomes somewhat cumbersome and inefficient. To compute all the regression coefficients more effectively, we typically need to consider the matrix form of the model. By expressing the linear regression model in matrix form, we can simplify the computation and make it more efficient. The model can be written as: \\[\n  \\textbf{y} = \\textbf{Xw} + \\epsilon\n\\] where: - \\(\\textbf{y}\\) is the vector of observed values (target variable), \\(\\textbf{y} = (y_1, \\dots, y_n)^{\\top}\\), - \\(\\textbf{X}\\) is the design matrix, which contains the feature variables (including a column of ones for the intercept), i.e. \\[\n    \\textbf{X}=\n    \\begin{pmatrix}\n      1 & x_{1,1} & \\cdots  & x_{1,p}\\\\\n      \\vdots  & \\vdots  & \\ddots  & \\vdots \\\\\n      1 & x_{n,1} & \\cdots  & x_{n,p}\n    \\end{pmatrix}\n  \\]\n\n\\(\\textbf{w}\\) is the vector of regression coefficients, \\(\\textbf{w} = (w_0, w_1, \\dots, w_p)^{\\top}\\)\n\\(\\boldsymbol{\\epsilon}\\) is the vector of errors, \\(\\boldsymbol{\\epsilon} = (\\epsilon_1, \\dots, \\epsilon_n)^{\\top}\\).\n\nTo estimate the regression coefficients , we use the least squares method, which minimizes the sum of squared errors. The solution to this optimization problem is: \\[\n  \\widehat{\\textbf{w}}=(\\textbf{X}^{\\top}\\textbf{X})^{-1}\\textbf{X}^{\\top}\\textbf{y}\n\\]\nThis formula provides an efficient way to calculate the coefficients for a linear regression model with multiple features. Using matrix operations not only simplifies the calculations but also allows for faster computation, especially when dealing with large data sets or numerous features. Thus, the matrix form of the model allows for a more streamlined approach to regression analysis, making it easier to implement and compute the necessary coefficients, particularly when working with multiple variables."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#an-analogy-for-overfitting",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#an-analogy-for-overfitting",
    "title": "Lecture 5: Regression Models",
    "section": "5.3.3 An Analogy for Overfitting",
    "text": "5.3.3 An Analogy for Overfitting\nI have had many frustrating experiences assembling IKEA furniture, and one of them is strikingly similar to the overfitting problem in machine learning. Imagine I’m putting together a chair, trying to screw the wooden slats into the metal frame. After tightening the first three screws, I feel like I’m almost done, but when I try to insert the fourth screw, it doesn’t fit. I’ve tightened the first three screws so much that there’s no room left to insert the last one.\n\n\n\nThe Pain of Assembling IKEA Furniture: An Analogy for Overfitting\n\n\nThis situation mirrors the concept of overfitting. By focusing too much on perfectly fitting the training data (the first three screws), I leave no flexibility for new, unseen data (the last screw). The model becomes too rigid, capturing every detail of the training data—just like I over-tightened the first screws—but fails to adapt when it encounters new observations."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#more-options-for-basis-functions",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#more-options-for-basis-functions",
    "title": "Lecture 5: Regression Models",
    "section": "5.2.3 More Options for Basis functions",
    "text": "5.2.3 More Options for Basis functions\nBasis functions are an essential concept in regression modeling, helping us to represent more complex relationships between the features and the target variable. In addition to polynomial basis functions, which we discussed earlier, there are other types of basis functions, such as spline basis functions. They are a set of piecewise polynomial functions used to model non-linear relationships. They are often preferred because they can offer a good balance between flexibility and smoothness. Cubic splines, for example, divide the data range into intervals and fit a cubic polynomial to each interval. The key advantage of splines is that they ensure continuity and smooth derivatives at the boundaries between these intervals, preventing abrupt changes in the model’s behavior. For spline basis, you are recommended to read the textbook 7.2 to 7.4, and we will also study it through exercises in the Lab."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_2.html",
    "href": "Courses/c_mlwr1_2024/l5/l5_2.html",
    "title": "5.2 Nonlinear Regression Models",
    "section": "",
    "text": "So far, we have studied two linear models, however, it is by no means enough to solve the problems in reality. For example, the plot below displays a classification problem, and clearly, we cannot find a suitable linear classifier. In other words, we cannot identify a straight line that can divide this 2D feature space in such a way that the two classes of sample points are located on opposite sides of the line. At the same time, through data visualization, we can roughly observe that the boundary between the two classes is an ellipse. Therefore, to address this problem, we must extend our linear model to find a solution.\nClassification Toy Example"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_2.html#basic-ideas-of-non-linear-extension",
    "href": "Courses/c_mlwr1_2024/l5/l5_2.html#basic-ideas-of-non-linear-extension",
    "title": "5.2 Nonlinear Regression Models",
    "section": "5.2.1 Basic ideas of Non-linear Extension",
    "text": "5.2.1 Basic ideas of Non-linear Extension\nNonlinear models are not the focus of our course, however, here we can explore the basic approach to finding nonlinear models, that is feature mapping. Feature mapping involves the basic idea of introducing new variables by transforming the original feature variables with functions. This expands the original feature space, allowing the exploration of potential linear solutions within the augmented feature space. Let’s start with the toy example of classification problem above.\nIn the classification problem, we can consider three new variables \\(\\left(h_1 = x_1^2, h_2 = \\sqrt{2}x_1x_2, h_3 = x_2^2 \\right)\\) instead of the two original variables \\(x_1, x_2\\). The new data set is visualized in the following 3D plot.\n\n\n\n\n\n\nIf you rotate the 3D scatter plot above, you may notice that the same set of observed cases becomes linearly separable in the new feature space, see the LHS of plot below. For example, we can train an LDA classifier using three new feature variables, \\(\\left(h_1, h_2, h_3 \\right)\\). The decision boundary of this classifier would correspond to the gray plane in the 3D space. By building the new feature space, the previously non-linearly separable data points may now become linearly separable, allowing the LDA classifier to effectively separate the two classes. Also, if we change the direction of our view, the linear model in the augmented feature space is eventually a nonlinear model in the original space, see the LHS of the plots below.\n\n\n\nAugmented Feature Space\n\n\nWe can refer to this idea as the feature mapping idea. In simple terms, we need to find an appropriate new space, which we call the augmented feature space, and train our linear model within it. This augmented feature space is entirely determined by a transformation function, \\(\\phi(\\textbf{x}) : \\text{p-D space} \\to \\text{q-D space}\\) which we refer to as feature mappings.\nThis concept plays a significant role in machine learning, and almost all advanced nonlinear models are based on this idea. For example:\n\nBefore deep learning dominated AI, the Support Vector Machine (SVM) applied this idea indirectly through the kernel function. The kernel allows the SVM to operate in a higher-dimensional space without explicitly computing the coordinates in that space.\nIn the world of ensemble methods, which dominate structured data tasks, each single model can be seen as a form of feature mapping.\nIn the foundation deep learning model, the neural network, the chain of transformations through neurons can also be seen as a feature transformation.\n\nThis idea of transforming data into a higher-dimensional space (whether directly or indirectly) enables models to handle complex, nonlinear relationships that would otherwise be difficult to capture in the original space."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_2.html#polynomial-regression-model",
    "href": "Courses/c_mlwr1_2024/l5/l5_2.html#polynomial-regression-model",
    "title": "5.2 Nonlinear Regression Models",
    "section": "5.2.2 Polynomial Regression Model",
    "text": "5.2.2 Polynomial Regression Model\nIn this section, we will introduce a nonlinear regression model using the feature mapping idea, specifically focusing on polynomial regression. Let’s begin by looking at a simple example.\n\n\n\n\n\nRegression Toy Example\n\n\n\n\nIn this example, we are dealing with a nonlinear regression problem, where the relationship between the feature variable and the target variable is clearly nonlinear. A linear model would not provide satisfactory results because it assumes a straight-line relationship between the features and the target. As you can see, the blue line in the plot, i.e. the simple linear model has totally unacceptable performance on the whole domain. However, by using the feature mapping idea, we can transform the feature space into a higher-dimensional space, where the relationship becomes linear, making it possible to apply linear regression successfully. To do so, we introduce the augmented feature space by the feature mapping \\(\\phi(x) = (x, x^2)\\), i.e. a mapping from 1D to 2D, see the 3D scatter plot below\n\n\n\n\n\n\nYou can see that all the points stand on a plane in the augmented space which means that we can find a linear model, i.e.  \\[\n  y = w_0 + w_1x + w_2x^2,\n\\] to solve the problem.\n\nRemark: I believe you can see that the choice of feature mapping, \\(\\phi()\\), or basis functions, is extremely important. If we choose an inappropriate set of feature mappings, we may end up with very poor results. As shown in the figure below, we apply \\(\\phi(x) = (x, x^5)\\) to obtain the augmented feature space, then we eventually switch to another nonlinear problem. In other words, we can’t find a plane in the new space such that all the sample points roughly all stand on it. So, we naturally have the following question and we will answer it in the next lecture.\nQuestion: How to choose an appropriate feature mapping?\n\n\n\n\n\n\n\nFrom another perspective, essentially, our idea in solving the above problem is to use polynomial functions to represent a nonlinear curve and then fit the data with this function. We refer to this regression model as polynomial regression model. The generic form of \\(p\\)-th order polynomial regression is \\[\n  y = w_0 + w_1x + w_2x^2 + \\dots + w_px^p + \\epsilon\n\\]"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_2.html#more-options-for-basis-functions",
    "href": "Courses/c_mlwr1_2024/l5/l5_2.html#more-options-for-basis-functions",
    "title": "5.2 Nonlinear Regression Models",
    "section": "5.2.3 More Options for Basis functions",
    "text": "5.2.3 More Options for Basis functions\nBasis functions are an essential concept in regression modeling, helping us to represent more complex relationships between the features and the target variable. In addition to polynomial basis functions, which we discussed earlier, there are other types of basis functions, such as spline basis functions. They are a set of piecewise polynomial functions used to model non-linear relationships. They are often preferred because they can offer a good balance between flexibility and smoothness. Cubic splines, for example, divide the data range into intervals and fit a cubic polynomial to each interval. The key advantage of splines is that they ensure continuity and smooth derivatives at the boundaries between these intervals, preventing abrupt changes in the model’s behavior. For spline basis, you are recommended to read the textbook 7.2 to 7.4, and we will also study it through exercises in the Lab.\n\n\nPrevious page | Lecture 5 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_1.html",
    "href": "Courses/c_mlwr1_2024/l5/l5_1.html",
    "title": "5.1 Linear Regression Model",
    "section": "",
    "text": "Unlike classification problems, the target variable in regression is continuous, but it inherits the basic idea of classification problems, which is to predict the target variable using a weighted combination of feature variables, i.e. \\[\n  y = w_0 + w_1 x_1 + \\dots w_px_p + \\epsilon\n\\] where \\(\\epsilon\\) is a error term. The error term contains many things. It could be measurement errors, or random noise, or all the variations that can not be explained by all the feature variables. For the latest case, it also means we need more feature variables for a better prediction of the target variable.\nObviously, we have to apply some methods (algorithm) to learn the model, i.e. estimate the coefficients \\(w_0, w_1, \\dots, w_p\\) from a data set. Here, we mainly discuss two methods, least square methods and maximum likelihood method. Eventually, the two methods are equivalent for a regression problem, however, it is still necessary to explain maximum likelihood method for understanding logistic regression in lecture 7."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_1.html#least-square-method",
    "href": "Courses/c_mlwr1_2024/l5/l5_1.html#least-square-method",
    "title": "5.1 Linear Regression Model",
    "section": "5.1.1 Least Square Method",
    "text": "5.1.1 Least Square Method\nLeast Square method was proposed by the famous mathematician Gauss. He applied this method for data analysis to accurately predict the time of the second appearance of the asteroid, Ceres. His idea is quite simple: to use data to find the optimal line, represented by two coefficients, in order to minimize prediction errors, see the plot below. \n\n\n\n\n\nSimple Linear Regression Model: Red circles are observations (x,y), Black dashline is the regression model, Blue Dots on the dash line are prediction y_hat, the lenght of segment in purple is the prediction error.\n\n\n\n\nSuppose that we have a set of paired observations, \\((y_1,x_1), \\dots, (y_n, x_n)\\), then the mathematical formulation is \\[\n  \\hat{w}_0, \\hat{w}_1 = \\arg\\min_{w_0, w_1} \\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n\\] where \\(\\hat{y}_i = w_0 + w_1x_i\\).\nThe solution of this optimization problem is \\(\\widehat{w}_1=\\frac{\\sum_{i=1}^N(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^N(x_i-\\overline{x})^2}\\), and \\(\\widehat{w}_0=\\overline{y}-\\widehat{w}_1\\overline{x}\\)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_1.html#matrix-form-ne",
    "href": "Courses/c_mlwr1_2024/l5/l5_1.html#matrix-form-ne",
    "title": "5.1 Linear Regression Model",
    "section": "5.1.2 Matrix Form ( NE )",
    "text": "5.1.2 Matrix Form ( NE )\nWhen we consider multiple feature variables in a linear regression model, the above calculation formula becomes somewhat cumbersome and inefficient. To compute all the regression coefficients more effectively, we typically need to consider the matrix form of the model. By expressing the linear regression model in matrix form, we can simplify the computation and make it more efficient. The model can be written as: \\[\n  \\textbf{y} = \\textbf{Xw} + \\epsilon\n\\] where: - \\(\\textbf{y}\\) is the vector of observed values (target variable), \\(\\textbf{y} = (y_1, \\dots, y_n)^{\\top}\\), - \\(\\textbf{X}\\) is the design matrix, which contains the feature variables (including a column of ones for the intercept), i.e. \\[\n    \\textbf{X}=\n    \\begin{pmatrix}\n      1 & x_{1,1} & \\cdots  & x_{1,p}\\\\\n      \\vdots  & \\vdots  & \\ddots  & \\vdots \\\\\n      1 & x_{n,1} & \\cdots  & x_{n,p}\n    \\end{pmatrix}\n  \\]\n\n\\(\\textbf{w}\\) is the vector of regression coefficients, \\(\\textbf{w} = (w_0, w_1, \\dots, w_p)^{\\top}\\)\n\\(\\boldsymbol{\\epsilon}\\) is the vector of errors, \\(\\boldsymbol{\\epsilon} = (\\epsilon_1, \\dots, \\epsilon_n)^{\\top}\\).\n\nTo estimate the regression coefficients , we use the least squares method, which minimizes the sum of squared errors. The solution to this optimization problem is: \\[\n  \\widehat{\\textbf{w}}=(\\textbf{X}^{\\top}\\textbf{X})^{-1}\\textbf{X}^{\\top}\\textbf{y}\n\\]\nThis formula provides an efficient way to calculate the coefficients for a linear regression model with multiple features. Using matrix operations not only simplifies the calculations but also allows for faster computation, especially when dealing with large data sets or numerous features. Thus, the matrix form of the model allows for a more streamlined approach to regression analysis, making it easier to implement and compute the necessary coefficients, particularly when working with multiple variables."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_1.html#maximum-likelihood-method",
    "href": "Courses/c_mlwr1_2024/l5/l5_1.html#maximum-likelihood-method",
    "title": "5.1 Linear Regression Model",
    "section": "5.1.3 Maximum Likelihood Method",
    "text": "5.1.3 Maximum Likelihood Method\nDifferent from least square method, next, we are going to reexamine the regression model from the perspective of probability models. To do so, we assume the error term \\(\\epsilon\\) is normally distributed, \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\). Based on this assumption, the target variable is normally distributed conditional on feature variables. Therefore, we essentially predict the expected value of the target variable conditional on \\(X_1, \\dots, X_p\\) as a linear model, i.e.\n\n\n\nRegression Model: the expected value of target variable is a linear function of X1 and X2\n\n\n\\[\n  \\text{E}(Y | X_1, \\dots, X_p) = w_0 + w_1 X_1 + w_2 X_2 + \\dots + w_p X_p\n\\] Based on the normality assumption, another estimation method, MLE, for coefficients can be discussed.\nMLE of regression model: Under the normality assumption, we have \\(y_i \\sim \\mathcal{N}( w_0 + w_1x_1 , \\sigma^2)\\). If you remember the secrete message behind the normal distribution, the likelihood of each observation, \\(y_i\\), is inversely proportionally to the distance to the expected value, i.e.  \\[\n   f( y_i | w_0, w_1, \\sigma^2 ) \\propto -(y_i - (w_0 + w_1x_1 ) )^2\n\\] Therefore the likelihood function of the sample \\(\\left\\{ y_i, x_i \\right\\}_{i=1}^n\\) is \\[\n  \\log \\left( L( w_0, w_1, \\sigma^2 | (y_i, x_i) ) \\right) \\propto -\\sum_{i=1}^n (y_i - (w_0 + w_1x_1 ) )^2\n\\]\nNotice that, on the LHS, it is sum square of residual. Therefore, minimize sum square of residuals is equivalent to maximize the log likelihood function. In other words, the two methods are equivalent."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_1.html#model-evaluation",
    "href": "Courses/c_mlwr1_2024/l5/l5_1.html#model-evaluation",
    "title": "5.1 Linear Regression Model",
    "section": "5.1.4 Model Evaluation",
    "text": "5.1.4 Model Evaluation\nUnlike classification problems, model evaluation for regression problems is quite straightforward. From the purpose of regression, our ultimate goal is to use feature variables to estimate a continuous target variable. A good regression model naturally minimizes prediction error. Therefore, we typically use the mean squared error (MSE) of the model predictions for evaluation. That is \\[\n  \\text{MSE} = \\frac{1}{N} \\sum_{i = 1}^N \\left( y_i - \\hat{y}_i\\right)^2\n\\]\nwhere \\(\\hat{y}_i\\) is the prediction of the \\(i\\)th case, i.e. \\(\\hat{w}_0 + \\hat{w}_1 x_{i,1} + \\dots + \\hat{w}_p x_{i,p}\\), and therefore \\(y_i - \\hat{y}_i\\) is the prediction error of the \\(i\\)th case. To gain a more intuitive understanding, people often use RMSE (Root Mean Squared Error), i.e. \\(\\text{RMSE} = \\sqrt{\\text{MSE}}\\). Sometimes, people also use Mean Absolute Error (MAE) to evaluate a regression model. \\[\n  \\text{MAE} = \\frac{1}{N} \\sum_{i = 1}^N | y_i - \\hat{y}_i|\n\\] For all metrics, lower value indicates better model performance.\n\nQuiz: Do you know why we don’t use the average errors, i.e. \\(\\frac{1}{N} \\sum_{i = 1}^N \\left( y_i - \\hat{y}_i\\right)\\) to evaluate a regression model?\nTips: The simplest regression model is \\(y = w_0 + \\epsilon\\) and the estimation is \\(\\hat{w}_0 = \\bar{y}\\), i.e. \\(\\hat{y} = \\bar{y}\\)."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_home.html#lecture-notes",
    "href": "Courses/c_mlwr1_2024/l5/l5_home.html#lecture-notes",
    "title": "Lecture 5: Regression Models",
    "section": "Lecture notes:",
    "text": "Lecture notes:\n\nRead the integrated notes: here\nRead the pagination notes: here\nDownload the PDF notes here."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_home.html#reading-guidelines-you-can-read-the-book",
    "href": "Courses/c_mlwr1_2024/l5/l5_home.html#reading-guidelines-you-can-read-the-book",
    "title": "Lecture 5: Regression Models",
    "section": "Reading Guidelines: You can read the book…",
    "text": "Reading Guidelines: You can read the book…\nFor lecture 5, it is recommended that you read the following sections in the textbook.\n\nlinear regression: Sections 3.1 and 3.2\nPolynomial regression: Sections 7.1\nOverfitting problems: pages 22-23"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_home.html#discussion",
    "href": "Courses/c_mlwr1_2024/l5/l5_home.html#discussion",
    "title": "Lecture 5: Regression Models",
    "section": "Discussion",
    "text": "Discussion\nSlides for discussion: click here\nVideo of discussion: click here"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_home.html#lab",
    "href": "Courses/c_mlwr1_2024/l5/l5_home.html#lab",
    "title": "Lecture 5: Regression Models",
    "section": "Lab:",
    "text": "Lab:\nLaboratory: Entrance\nSolutions: click here\nR file: download here.\n\nCourse Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_0.html",
    "href": "Courses/c_mlwr1_2024/l5/l5_0.html",
    "title": "Lecture 5: Regression Models",
    "section": "",
    "text": "In this lecture, we discuss regression problems. First, we review the training methods of linear regression models from different perspectives. Then, a basic nonlinear extension idea is introduced, feature mapping, and use it to introduce the first nonlinear regression model, polynomial regression. Afterward, we understand polynomial regression from the perspective of basis functions, and then introduce another commonly used nonlinear model, spline regression. At the end of this lecture, we introduce a new concept, the overfitting problem. It is a core challenge in machine learning from the training perspective, and we focus on it in the next lecture.\nOutline:\n\n5.1 Linear Regression Model\n5.2 Nonlinear Regression Model\n5.3 Ovrfitting Problem\n\n\nLecture 5 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_3.html",
    "href": "Courses/c_mlwr1_2024/l5/l5_3.html",
    "title": "5.3 Over Fitting Problems",
    "section": "",
    "text": "With the help of feature mapping or basis functions, it seems like we can achieve anything. It appears that as long as we continuously expand our feature space through feature mapping, we can always find a perfect linear model in a vast augmented feature space to solve the problem. But is it really that simple? Let’s take a look at the following example."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_3.html#motivating-examples",
    "href": "Courses/c_mlwr1_2024/l5/l5_3.html#motivating-examples",
    "title": "5.3 Over Fitting Problems",
    "section": "5.3.1 Motivating Examples",
    "text": "5.3.1 Motivating Examples\n\n\n\n\n\n\n\n\n\nHere we have a set of data (blue triangles), and we want to use the values of the horizontal coordinate \\(X\\) to predict the vertical coordinate \\(Y\\). There are two candidate models.\n\nRed model: 2nd-degree polynomial regression, which performs well enough, but makes some errors on the three points on the right side.\nOrange model: 4th-degree polynomial regression, which performs perfectly because it passes exactly through every observation point, meaning it makes no errors at all.\n\nSo, the question is, which model do you think is better?\nYou might like the orange model because it is “perfect.” Indeed, people often pursue perfection, but behind perfection often lies a trap. Let me tell you the truth. As shown in the figure below, all the observation points are generated by the light blue dashed line plus a normal noise term. Now, let’s generate another observation point, the green triangle. At this point, the red model still performs well, but the prediction of the perfect orange model seems rather absurd."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_3.html#over-fitting",
    "href": "Courses/c_mlwr1_2024/l5/l5_3.html#over-fitting",
    "title": "5.3 Over Fitting Problems",
    "section": "5.3.2 Over Fitting",
    "text": "5.3.2 Over Fitting\nLet’s analyze this phenomenon. First, we need to weigh the pros and cons from two aspects, i.e. considering both the performance on the training samples and the testing samples. Therefore, I summarize the analysis of this phenomenon in the table below.\n\n\n\n\n\n\n\n\n\n\nOrange (4th order)\nRed (2nd order)\n\n\n\n\nTraining set\nPerfect\nGood enough\n\n\nTesting set\nPoor\nGood\n\n\nNumber of coefficients\n5\n3\n\n\nComplexity\nComplex\nSimple\n\n\n\nTraining Set Performance\n\nThe orange model perfectly fits the training data, capturing every fluctuation in the observations. This is often seen as an indicator of a model that is too complex, as it adjusts to every detail in the data, including noise or outliers.\nOn the other hand, the red model provides a “good enough” fit. It captures the overall trend of the data without fitting every minor variation, resulting in a simpler and more generalizable model.\n\nPerformance on testing set\n\nWhen we introduce new observations (those not included in the training set), the orange model’s performance drops significantly. Despite its “perfect” fit on the training data, it fails to generalize well to unseen data, indicating that it has overfitted the training set. Overfitting occurs when a model learns not only the true underlying pattern but also random fluctuations or noise in the training data. As a result, the model performs poorly on new, real-world data.\nConversely, the red model, with its simpler structure, generalizes better to new data points. It performs adequately on the training data and maintains reasonable performance on new observations, as it avoids overfitting by not capturing unnecessary complexity.\n\nThis phenomenon is called the overfitting problem. Simply put, if a model performs “perfectly” on the training samples but fails to generalize to new observations, it means we cannot apply the model for prediction, or in other words, we cannot extend the model to other observations.\nConsequences of Overfitting:\n\nThe model becomes overly sensitive to small variations in the training data, which doesn’t reflect the true distribution of data in the population.\nIt fails to make accurate predictions when new data points are introduced, making it less useful for real-world applications.\n\nSo, what causes the overfitting problem? Let me introduce a new concept, model complexity。 It refers to how intricate or detailed a model is in terms of its structure and the number of parameters it uses. A more complex model has more parameters or can capture more intricate patterns in the data.\nModel Complexity and Overfitting\n\nThe orange model has 5 coefficients, making it more complex and prone to overfitting. The extra flexibility allowed by the 4th-order polynomial gives the model the ability to fit noise, leading to poor generalization.\nThe red model, with only 3 coefficients, is simpler and less likely to overfit. While it may not fit the training data perfectly, its simpler structure helps it to better handle new observations.\n\nSummary: This example demonstrates the classic overfitting problem: while complex models (like the orange model) may perform well on the training data, they fail to generalize to new data. Simpler models (like the red model) strike a better balance between fitting the data and maintaining the ability to generalize. This highlights the importance of selecting the appropriate model complexity to ensure that the model performs well not just on the training data, but also in real-world scenarios where new, unseen data is encountered."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_3.html#an-analogy-for-overfitting",
    "href": "Courses/c_mlwr1_2024/l5/l5_3.html#an-analogy-for-overfitting",
    "title": "5.3 Over Fitting Problems",
    "section": "5.3.3 An Analogy for Overfitting",
    "text": "5.3.3 An Analogy for Overfitting\nI have had many frustrating experiences assembling IKEA furniture, and one of them is strikingly similar to the overfitting problem in machine learning. Imagine I’m putting together a chair, trying to screw the wooden slats into the metal frame. After tightening the first three screws, I feel like I’m almost done, but when I try to insert the fourth screw, it doesn’t fit. I’ve tightened the first three screws so much that there’s no room left to insert the last one.\n\n\n\nThe Pain of Assembling IKEA Furniture: An Analogy for Overfitting\n\n\nThis situation mirrors the concept of overfitting. By focusing too much on perfectly fitting the training data (the first three screws), I leave no flexibility for new, unseen data (the last screw). The model becomes too rigid, capturing every detail of the training data—just like I over-tightened the first screws—but fails to adapt when it encounters new observations."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_3.html#variance-and-bias-view",
    "href": "Courses/c_mlwr1_2024/l5/l5_3.html#variance-and-bias-view",
    "title": "5.3 Over Fitting Problems",
    "section": "5.3.4 Variance and Bias View",
    "text": "5.3.4 Variance and Bias View\nA 2nd-order polynomial regression model exhibits some prediction error for the \\(y\\) values in the training sample, whereas a 4th-order polynomial regression model always predicts the \\(y\\) values perfectly. As a result, the 4th-order model provides highly accurate predictions, while the 2nd-order model performs slightly worse. In statistical terms, the predictions of the 4th-order model have zero bias, whereas the 2nd-order model exhibits some bias.\n\n\n\n\n\n\n\nHowever, if we repeatedly generate five observations from the light blue model and estimate the regression model each time, we will observe that the 2nd-order model produces very stable predictions. As shown in the figure, all its curves remain approximately within a certain range. In contrast, the 4th-order model is highly unstable. In statistical terms, the 4th-order model has very high variance, while the variance of the 2nd-order model is much smaller. Therefore, overfitting occurs when the model has low bias and high variance. It fits the training data too closely, including noise, leading to excellent training performance but poor generalization on unseen data. So how can we avoid the overfitting problem? We will discuss this in the next section.\n\n\n\n\n\nVariance-Bias Tradeoff: We have five sets of data generated by the true model, represented by the light blue dashed line, each shown in a different color. On the left-hand side (LHS), we fit a 2nd-order polynomial regression model to each dataset, while on the right-hand side (RHS), we fit a 4th-order polynomial regression model. As you can see, the fitted models on the LHS are very stable. However, on the RHS, although the models perfectly pass through the observation points, they are extremely unstable.\n\n\n\n\n\nPrevious page | Lecture 5 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5.html#loss-function",
    "href": "Courses/c_mlwr1_2024/l5/l5.html#loss-function",
    "title": "Lecture 5: Regression Models",
    "section": "5.1.5 Loss function",
    "text": "5.1.5 Loss function\nThe attentive among you may have noticed that the objective function of the least squares method in regression problems is the same as the model evaluation metric, MSE. From another perspective, we can understand the estimation of regression models as finding a set of regression coefficients that minimize the MSE. In machine learning, estimating regression coefficients is framed as an optimization problem, where MSE is interpreted as the objective function of this optimization problem, also referred to as the model’s loss function. For a regression problem, it is called MSE loss: \\[\n  \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n\\] For different problem, we could have different loss functions, for example, Huber loss, cross entropy loss, hinge loss. We will explore this further in the lab exercises. This concept, connecting to Maximum Likelihood Estimation, will reappear and be discussed again in the context of logistic regression models."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_1.html#loss-function",
    "href": "Courses/c_mlwr1_2024/l5/l5_1.html#loss-function",
    "title": "5.1 Linear Regression Model",
    "section": "5.1.5 Loss function",
    "text": "5.1.5 Loss function\nThe attentive among you may have noticed that the objective function of the least squares method in regression problems is the same as the model evaluation metric, MSE. From another perspective, we can understand the estimation of regression models as finding a set of regression coefficients that minimize the MSE. In machine learning, estimating regression coefficients is framed as an optimization problem, where MSE is interpreted as the objective function of this optimization problem, also referred to as the model’s loss function. For a regression problem, it is called MSE loss: \\[\n  \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n\\] For different problem, we could have different loss functions, for example, Huber loss, cross entropy loss, hinge loss. We will explore this further in the lab exercises. This concept, connecting to Maximum Likelihood Estimation, will reappear and be discussed again in the context of logistic regression models.\n\nPrevious page | Lecture 5 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_home.html",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_home.html",
    "title": "Lab 3: Exercises about Regression Models",
    "section": "",
    "text": "In this lab, we implement and practice regression models with a famous benchmark data set for regression problem in machine learning, Boston data set. The data set can be found in package ‘MASS’. For the background information for the Boston data, check the help document by typing ?Boston in the R console."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-1.1",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-1.1",
    "title": "Lab 3: Exercises about Regression Models",
    "section": "Task 1.1:",
    "text": "Task 1.1:\nFirst, we try the most naive or brute force method, grid search.\n\nDefine the Search Range:\n\nSpecify a range of possible values for each regression coefficient, i.e. \\(w_0 \\in [34, 35]\\) and \\(w_1 \\in [-1,0]\\).\nDivide the range into sufficiently small intervals, such as steps of 0.1.\n\nCompute All Combinations:\n\nFor each possible combination of regression coefficients, calculate the predicted values \\(\\hat{y}\\) using the regression formula. Tips: You can implement it by a double loop or try to use expand.grid() function to create all the combination of possible values of two coefficients.\n\nCalculate Errors:\n\nFor each combination, compute the corresponding MSE: \\[\n\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n\\]\nRecord the MSE for each combination.\n\nFind the Optimal Solution:\n\nIterate through all combinations and select the set of coefficients that yields the smallest MSE."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-1.2",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-1.2",
    "title": "Lab 3: Exercises about Regression Models",
    "section": "Task 1.2:",
    "text": "Task 1.2:\nClearly, brute force methods like grid search are highly inefficient and often struggle to guarantee precision. In machine learning, numerical algorithms are commonly used to approximate the optimal solution, such as gradient descent. However, we will not delve into such algorithms here. Interested students can explore them on their own, and we will discuss them further in the context of logistic regression.\nNow, let’s return to the regression problem. As mentioned in the lecture notes, when using MSE as the objective function or loss function, there is a classic analytical solution: the Gauss’s least squares solution. In other words, we can derive a formula to precisely compute the optimal solution， i.e. \\[\n  \\widehat{w}_1=\\frac{\\sum_{i=1}^N(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^N(x_i-\\overline{x})^2} \\text{ and } \\widehat{w}_0=\\overline{y}-\\widehat{w}_1\\overline{x}\n\\] So, in this sub task, you need to calculate the regression coefficients according to the formula of least square solutions.\nNote: This formula only works for simple linear regression. For multivariate regression model, you need the formula in matrix form."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-1.3",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-1.3",
    "title": "Lab 3: Exercises about Regression Models",
    "section": "Task 1.3:",
    "text": "Task 1.3:\nApply lm function to estimate the regression coefficients. There are two usages of the lm function, however, the better way is using it with a data frame. With a data frame, you need to specify:\n\nThe model: medv ~ lstat\nThe data: set argument data = Boston"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-1.4",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-1.4",
    "title": "Lab 3: Exercises about Regression Models",
    "section": "Task 1.4:",
    "text": "Task 1.4:\nCompare the results from sub tasks 1.1 to 1.3 and draw a conclusion."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-2.1",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-2.1",
    "title": "Lab 3: Exercises about Regression Models",
    "section": "Task 2.1:",
    "text": "Task 2.1:\nUse lstat and age as feature variables. Calculate the mean square errors of the model based on the data."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-2.2",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-2.2",
    "title": "Lab 3: Exercises about Regression Models",
    "section": "Task 2.2:",
    "text": "Task 2.2:\nUse all the variables except medv as feature variables. Calculate the mean square errors of the model based on the data.\nTips: With a data frame, if you want to predict one variable with all the rest of variables in the data frame, then the model can specified as medv~."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-3.1",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-3.1",
    "title": "Lab 3: Exercises about Regression Models",
    "section": "Task 3.1:",
    "text": "Task 3.1:\nTrain the same model in task 1 with the training set and evaluate the performance of the resulting model with both the training set and the testing set."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-3.2",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-3.2",
    "title": "Lab 3: Exercises about Regression Models",
    "section": "Task 3.2:",
    "text": "Task 3.2:\nTrain the second, 7th, and 20th-order polynomial regression with the training set and respectively evaluate the resulting models with both the training set and the testing set."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-3.3",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_home.html#task-3.3",
    "title": "Lab 3: Exercises about Regression Models",
    "section": "Task 3.3:",
    "text": "Task 3.3:\nCompare the results from sub tasks 3.1 to 3.2 and draw a conclusion.\n\nLecture 5 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html",
    "title": "Solutions to Exercises in Lab 3",
    "section": "",
    "text": "In this lab, we implement and practice regression models with a famous benchmark data set for regression problem in machine learning, Boston data set. The data set can be found in package ‘MASS’. For the background information for the Boston data, check the help document by typing ?Boston in the R console.\nImport data\nlibrary(MASS)\ndat = Boston"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-1.1",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-1.1",
    "title": "Solutions to Exercises in Lab 3",
    "section": "Task 1.1:",
    "text": "Task 1.1:\nFirst, we try the most naive or brute force method, grid search.\n\nDefine the Search Range:\n\nSpecify a range of possible values for each regression coefficient, i.e. \\(w_0 \\in [34, 35]\\) and \\(w_1 \\in [-1,0]\\).\nDivide the range into sufficiently small intervals, such as steps of 0.1.\n\nCompute All Combinations:\n\nFor each possible combination of regression coefficients, calculate the predicted values \\(\\hat{y}\\) using the regression formula. Tips: You can implement it by a double loop or try to use expand.grid() function to create all the combination of possible values of two coefficients.\n\nCalculate Errors:\n\nFor each combination, compute the corresponding MSE: \\[\n\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n\\]\nRecord the MSE for each combination.\n\nFind the Optimal Solution:\n\nIterate through all combinations and select the set of coefficients that yields the smallest MSE.\n\n\n Solutions Prepare grid for \\(w_0\\) and \\(w_1\\), and create a matrix res to keep MSEs for all possible combination of candidate values.\n\nn = dim(dat)[1]\ny = dat$medv\nx = dat$lstat\nw_0_candidate = seq(34,35, 0.1)\nw_1_candidate = seq(-1,0, 0.1)\n# define a matrix 'res' to store all the mse\nr0 = length(w_0_candidate)\nr1 = length(w_1_candidate)\nres = matrix(0, r0, r1)\n\n Do a double loop to go through all the possible combinations and record the MSEs.\n\n# try all the combinations\nfor(i in 1:r0){\n  for(j in 1:r1){\n    residual = y - (w_0_candidate[i] + w_1_candidate[j]*x)\n    res[i,j] = mean(residual^2)\n  }\n}\nres # all MSEs\n\n          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]      [,7]     [,8]\n [1,] 40.01624 38.61676 41.43717 48.47745 59.73762 75.21767  94.91759 118.8374\n [2,] 39.78906 38.64265 41.71612 49.00946 60.52269 76.25580  96.20878 120.3816\n [3,] 39.58189 38.68854 42.01507 49.56147 61.32776 77.31393  97.51998 121.9459\n [4,] 39.39472 38.75443 42.33402 50.13348 62.15283 78.39206  98.85117 123.5302\n [5,] 39.22754 38.84031 42.67296 50.72549 62.99790 79.49019 100.20236 125.1344\n [6,] 39.08037 38.94620 43.03191 51.33750 63.86298 80.60833 101.57356 126.7587\n [7,] 38.95319 39.07209 43.41086 51.96951 64.74805 81.74646 102.96475 128.4029\n [8,] 38.84602 39.21798 43.80981 52.62152 65.65312 82.90459 104.37595 130.0672\n [9,] 38.75885 39.38386 44.22876 53.29353 66.57819 84.08272 105.80714 131.7514\n[10,] 38.69167 39.56975 44.66771 53.98554 67.52326 85.28086 107.25833 133.4557\n[11,] 38.64450 39.77564 45.12666 54.69755 68.48833 86.49899 108.72953 135.1799\n          [,9]    [,10]    [,11]\n [1,] 146.9771 179.3366 215.9161\n [2,] 148.7744 181.3870 218.2195\n [3,] 150.5917 183.4574 220.5430\n [4,] 152.4290 185.5478 222.8864\n [5,] 154.2863 187.6582 225.2498\n [6,] 156.1637 189.7885 227.6333\n [7,] 158.0610 191.9389 230.0367\n [8,] 159.9783 194.1093 232.4602\n [9,] 161.9156 196.2997 234.9036\n[10,] 163.8729 198.5100 237.3670\n[11,] 165.8502 200.7404 239.8505\n\n\n Find the optimal solution and print out the solutions\n\nmin(res)\n\n[1] 38.61676\n\n(id = which(res == min(res), arr.ind = T))\n\n     row col\n[1,]   1   2\n\n# print the optimal solutions\nw_0_candidate[id[1]]\n\n[1] 34\n\nw_1_candidate[id[2]]\n\n[1] -0.9\n\n\n Alternative way: We also can apply expand.grid function to create all possible combinations of the values of \\(w_0\\) and \\(w_1\\).\n\nall_w_combinations = expand.grid(w_0_candidate, w_1_candidate)\nhead(all_w_combinations)\n\n  Var1 Var2\n1 34.0   -1\n2 34.1   -1\n3 34.2   -1\n4 34.3   -1\n5 34.4   -1\n6 34.5   -1\n\nr = dim(all_w_combinations)[1]\nres = numeric(r)\nfor(i in 1:r){\n  residual = y - (all_w_combinations[i,1] + all_w_combinations[i,2]*x)\n  res[i] = mean(residual^2)\n}\nid = which(res == min(res))\n# print the optimal\nall_w_combinations[id,]\n\n   Var1 Var2\n12   34 -0.9"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-1.2",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-1.2",
    "title": "Solutions to Exercises in Lab 3",
    "section": "Task 1.2:",
    "text": "Task 1.2:\nClearly, brute force methods like grid search are highly inefficient and often struggle to guarantee precision. In machine learning, numerical algorithms are commonly used to approximate the optimal solution, such as gradient descent. However, we will not delve into such algorithms here. Interested students can explore them on their own, and we will discuss them further in the context of logistic regression.\nNow, let’s return to the regression problem. As mentioned in the lecture notes, when using MSE as the objective function or loss function, there is a classic analytical solution: the Gauss’s least squares solution. In other words, we can derive a formula to precisely compute the optimal solution， i.e. \\[\n  \\widehat{w}_1=\\frac{\\sum_{i=1}^N(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^N(x_i-\\overline{x})^2} \\text{ and } \\widehat{w}_0=\\overline{y}-\\widehat{w}_1\\overline{x}\n\\] So, in this sub task, you need to calculate the regression coefficients according to the formula of least square solutions.\nNote: This formula only works for simple linear regression. For multivariate regression model, you need the formula in matrix form.\n Solutions\n\nn = dim(dat)[1]\ny = dat$medv\nx = dat$lstat\nw1 = sum((x-mean(x))*(y-mean(y)))/sum((x-mean(x))^2)\n# beta1 = cov(x,y)/var(x)\nprint(paste(\"w1:\", w1))\n\n[1] \"w1: -0.950049353757991\"\n\nw0 = mean(y)-w1*mean(x)\nprint(paste(\"w0:\", w0))\n\n[1] \"w0: 34.5538408793831\""
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-1.3",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-1.3",
    "title": "Solutions to Exercises in Lab 3",
    "section": "Task 1.3:",
    "text": "Task 1.3:\nApply lm function to estimate the regression coefficients. There are two usages of the lm function, however, the better way is using it with a data frame. With a data frame, you need to specify:\n\nThe model: medv ~ lstat\nThe data: set argument data = Boston\n\n Solutions\n\nm = lm(medv~lstat, data = dat)\nm$coefficients\n\n(Intercept)       lstat \n 34.5538409  -0.9500494"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-1.4",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-1.4",
    "title": "Solutions to Exercises in Lab 3",
    "section": "Task 1.4:",
    "text": "Task 1.4:\nCompare the results from sub tasks 1.1 to 1.3 and draw a conclusion.\n Solutions: we can see that, both our own code in 1.2 and applying lm function in 1.3 produce the same answer. However, the solution of 1.1, grid search method, can only provide a very rough estimation. If one want to get a better estimation, then have to increase the dense of grid, however, it will be very costy."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-2.1",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-2.1",
    "title": "Solutions to Exercises in Lab 3",
    "section": "Task 2.1:",
    "text": "Task 2.1:\nUse lstat and age as feature variables. Calculate the mean square errors of the model based on the data.\n Solutions\n\nm1 = lm(medv~lstat+age, data = dat)\nsummary(m1)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\ny_pre = predict(m1, dat)\nmean((dat$medv - y_pre)^2)\n\n[1] 37.88168"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-2.2",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-2.2",
    "title": "Solutions to Exercises in Lab 3",
    "section": "Task 2.2:",
    "text": "Task 2.2:\nUse all the variables except medv as feature variables. Calculate the mean square errors of the model based on the data.\nTips: With a data frame, if you want to predict one variable with all the rest of variables in the data frame, then the model can specified as medv~.\n Solutions\n\nm1 = lm(medv~., data = dat)\nsummary(m1)\n\n\nCall:\nlm(formula = medv ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\ny_pre = predict(m1, dat)\nmean((dat$medv - y_pre)^2)\n\n[1] 21.89483"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-3.1",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-3.1",
    "title": "Solutions to Exercises in Lab 3",
    "section": "Task 3.1:",
    "text": "Task 3.1:\nTrain the same model in task 1 with the training set and evaluate the performance of the resulting model with both the training set and the testing set.\n Solutions\n\nm1 = lm(medv~lstat, data = dat_tr)\n# training set\nmse1_tr = mean( (dat_tr$medv - m1$fitted.value)^2 )\nmse1_tr\n\n[1] 39.80271\n\n# testing set\ny_pre = predict(m1, newdata = dat_te)\nmse1_te = mean((dat_te$medv - y_pre)^2)\nmse1_te\n\n[1] 33.22158"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-3.2",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-3.2",
    "title": "Solutions to Exercises in Lab 3",
    "section": "Task 3.2:",
    "text": "Task 3.2:\nTrain the second, 7th, and 20th-order polynomial regression with the training set and respectively evaluate the resulting models with both the training set and the testing set.\n Solutions\n\nm2 = lm(medv~poly(lstat,2), data = dat_tr)\n#m2 = lm(dat_tr$medv~I(dat_tr$lstat) + I(dat_tr$lstat^2)) # it is an alternative way\n# training set\nmse2_tr = mean( (dat_tr$medv - m2$fitted.value)^2 )\nmse2_tr\n\n[1] 30.16128\n\n# testing set\ny_pre = predict(m2, newdata = dat_te)\nmse2_te = mean((dat_te$medv - y_pre)^2)\nmse2_te\n\n[1] 31.1774\n\nm7 = lm(medv~poly(lstat,7), data = dat_tr)\n# training set\nmse7_tr = mean((m7$residuals)^2)\nmse7_tr\n\n[1] 26.82469\n\n# testing set\ny_pre = predict(m7, newdata = dat_te)\nmse7_te = mean((dat_te$medv - y_pre)^2)\nmse7_te\n\n[1] 26.77753\n\nm20= lm(medv~poly(lstat,20), data = dat_tr)\n# training set\nmse20_tr = mean((m20$residuals)^2)\nmse20_tr\n\n[1] 25.60726\n\n# testing set\ny_pre = predict(m20, newdata = dat_te)\nmse20_te = mean((dat_te$medv - y_pre)^2)\nmse20_te\n\n[1] 56949982980"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-3.3",
    "href": "Courses/c_mlwr1_2024/l5/l5_lab_solutions.html#task-3.3",
    "title": "Solutions to Exercises in Lab 3",
    "section": "Task 3.3:",
    "text": "Task 3.3:\nCompare the results from sub tasks 3.1 to 3.2 and draw a conclusion.\n Solutions: From the results we can see that, 7th order polynomial regression has better predicting performance than simple linear regression model. However, the 20th order polynomial has the minimum MSE in training test, however very big MSE in testing set, therefore it definitely over fits the data set. \n\nLecture 5 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6.html#hyper-parameters",
    "href": "Courses/c_mlwr1_2024/l6/l6.html#hyper-parameters",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "2.1 Hyper-parameters",
    "text": "2.1 Hyper-parameters\nThink of a machine learning model like a car. Cars come in broad categories, such as SUVs, Hatchbacks, Sports cars, etc., each designed for a specific purpose. Within these categories, there are further variations, for instance, an SUV might be a compact crossover, a mid-size, or a full-size SUV, and the specific features like engine size or drivetrain vary between models.\n\n\n\n\nHave you ever bought a car? My experience was that I searched online and picked the cheapest used car.\n\n\n\nSimilarly, machine learning models have different types or families, often referred to as hypotheses. Examples that we have studied so far include linear regression, polynomial regression, Gaussian Discriminant Analysis (GDA), k-Nearest Neighbors (KNN), and so on. Each family represents a broad class of models. However, within each family, the exact form of the model depends on parameters that we choose or estimate. For example, in linear regression, we need to determine the slope and intercept of the line; in polynomial regression model, we need to determine the order of polynomial terms and estimate the regression coefficients; in GDA, we need to determine the assumption of contrivance structure and estimate the weights for each feature, and so on. Among the many parameters, we can further divide them into two categories: model parameters and hyper-parameters.\n\nModel Parameters: These are the parameters that the model learns directly from the data during training. Once the model structure is decided, these parameters can be computed using algorithms. Examples include regression coefficients in linear regression, feature weights in GDA, and so on.\nHyper-parameters: These are parameters that need to be set before the model is trained. They control aspects of the model or the learning process itself. Examples include the degree of the polynomial in polynomial regression or the value of \\(k\\) in KNN. Unlike model parameters, hyper-parameters are not learned from the data directly but require careful tuning through methods like cross-validation.\n\nIn essence, building and training a machine learning model involves selecting a model type, defining its structure through hyper-parameters, and then using data to learn the model parameters. Before we begin training a model using data, we need not only to choose the type of model but also to determine the hyper-parameters that can not be set by the algorithm itself. In other words, determining the optimal values for hyper-parameters is essentially a model selection problem.\n\n\n\n\n\n\n\n\nThe procedure of tuning hyper-parameters\n\n\n\n\n\n\n\n\nRecall: In previous labs, we used the so-called brute-force method, the grid search method, to estimate the parameters of regression models. Do you sense a similar vibe here? In other words, without those clever algorithms, wouldn’t the coefficients of a regression model also become hyper-parameters?"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6.html#model-validation-methods",
    "href": "Courses/c_mlwr1_2024/l6/l6.html#model-validation-methods",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "2.2 Model Validation Methods",
    "text": "2.2 Model Validation Methods\nFrom the above process of tuning hyper-parameters, the most critical step is Step 3, which is how to evaluate the trained model. Of course, choosing an appropriate model performance metric is important, but even more crucial is avoiding the trap of over fitting. So, how can we evaluate a model to avoid over fitting? If you recall the main characteristic of over fitting (excellent performance on the training set but poor performance on the test set), the conclusion becomes clear. The principle is:\n\nPrinciple: Avoid using the training set to evaluate the trained model.\n\nSpecifically, if a sample set is involved in model training, it should not be used for model evaluation. Based on this principle, we have the following methods for evaluating model performance: the validation set approach, cross-validation approach, and bootstrap method.\n\n2.2.1 The Validation Set Approach\nThe validation set approach is a simple method for model evaluation where the dataset is split into two parts: typically, 80% for training the model and 20% for validation to assess its performance. This allows us to test the model on unseen data and estimate its generalization ability. Recall that in the previous lab, we have actually applied this approach to find the best polynomial regression model.\nHowever, this approach has some notable drawbacks. The performance evaluation can be highly sensitive to how the data is split, as different splits may lead to varying results. Additionally, since only a portion of the data is used for training, the model might not fully leverage all available information, potentially reducing its performance.\nTo address these limitations, we can use cross-validation methods, which provide a more robust and reliable estimate of model performance by systematically rotating through different training and validation splits.\n\n\n2.2.2 Cross Validation Methods\nWhile adhering to the principle of model validation, we can adopt a more flexible approach to defining the training set and validation set. To ensure that all samples contribute to both model training and evaluation, we can dynamically adjust these two sets. There are two methods in this category, i.e. leave one out cross validation and k-fold cross validation\n\nLeave One Out Cross Validation (LOOCV) \nIn this method, the validation set is iteratively defined: each iteration holds out one sample as the test set, while the remaining samples are used to train the model. The model is then evaluated by making a prediction on the held-out sample, and the prediction error is recorded. By iteratively recording the prediction errors for all dynamic validation sets, we can aggregate these errors using an appropriate evaluation metric to compute the final cross-validation results.\nBelow is the corresponding pseudo-code and demo for this approach.\nPseudo-code of LOOCV:\n# n: training sample size\ncv_res = numeric(n) # for keeping all the cross validation errors\nfor(i in 1:n){\n  model = Algorithm(dat[-i,]) # 'dat' contains y and x\n  cv_res[i] = dat$y[i] - model(dat$x[i])\n}\nCV_performance = metric(cv_res)\nAnimation-Demo of LOOCV:\n\n\n\n    Replay\n\n\n\n\nThe main drawback of LOOCV is its computational cost, as the model must be trained as many times as there are data points, which can be inefficient for large datasets. Additionally, LOOCV may result in high variance in error estimates due to its reliance on a single data point for validation in each iteration.\n\n\n\nK-fold Cross Validation (kFCV)\nTo address these issues, kFCV offers a more efficient alternative by randomly splitting the dataset into k equally sized folds, allowing multiple samples to be used for validation in each iteration, reducing computational cost and variance. Below is the corresponding pseudo-code and demo for this approach.\nPseudo-code of KFCV:\n# n: training sample size\n# k: number of flods\n# nn = n/k: size of dynamic vliadtion set\n# Step 1: shuffle observation points\nid = sample(1:n)  \nid = matrix(id, nrow = k)\n# Step 2: doing cross validation\ncv_res = numeric(n) # for keeping all the cross validation errors\nfor(i in 1:k){\n  model = Algorithm(dat[-id[k,],]) # 'dat' contains y and x\n  cv_res[ ((i-1)*nn+1) : (i*nn) ] = dat$y[id[k,]] - model(dat$x[id[k,]])\n}\nCV_performance = metric(cv_res)\nAnimation-Demo of KFCV:\n\n\n\n    Replay\n\n\n\n\n\nQuiz: What is the relationship between LOOCV and kFCV?\n\n\n\n2.2.3 Bootstrap Method\nWhile cross-validation splits the dataset into distinct training and validation sets to evaluate model performance, bootstrap takes a different approach by focusing on resampling. Instead of partitioning the data, bootstrap generates multiple training sets by randomly sampling with replacement from the original dataset, allowing some samples to appear multiple times while others are left out. In a statistical terminology, the prepared temporary training set is refereed to bootstrap sample.\nBelow is the corresponding pseudo-code and demo for this approach.\nPseudo-code of Bootstrap:\n# n: training sample size\n# B: number of bootstrap samples (temporary training set)\nbt_res = numeric(n*B, B, n) # for keeping all the bootstrap errors\nfor(i in 1:B){\n  id = sample(1:n, n, replace = T)\n  model = Algorithm(dat[-id,]) # 'dat' contains y and x\n  cv_res[i,] = dat$y[id] - model(dat$x[id])\n}\nCV_performance = metric(cv_res)\nAnimation-Demo of Bootstrap:\n\n\n\n    Replay\n\n\n\n\nThis approach is particularly useful in small datasets where partitioning into training and validation sets could lead to a loss of valuable data for training. In addition, it is worth mentioning that the bootstrap algorithm not only provides a method for model validation but also offers an idea for creating nonlinear models. The famous random forest model is based on the bootstrap algorithm. We will revisit this topic in the second part of this course."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6.html#standard-procedure",
    "href": "Courses/c_mlwr1_2024/l6/l6.html#standard-procedure",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "2.3 Standard Procedure",
    "text": "2.3 Standard Procedure\nThe standard procedure in machine learning involves four main steps:\n\nSplit the data into three sets: training, validation, and testing.\nTune hyper-parameters using the training and validation sets.\nTrain the final model using all available data from the training and validation sets.\nEvaluate the model on the testing set to estimate its generalization performance.\n\nDiscussion:\n\nThe boundary between the training and validation sets can be vague, as it depends on the specific validation method used (e.g., k-fold, bootstrap, etc.).\n\nThe testing set is primarily used to estimate the model’s performance on unseen data. If you’re only interested in selecting the best model and don’t care about performance on a completely new data set, the testing set can be ignored."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6.html#overview",
    "href": "Courses/c_mlwr1_2024/l6/l6.html#overview",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nFeature selection is an important step in building machine learning models. First, reducing the dimensionality of the dataset is often necessary, and there are two main approaches to achieve this: feature extraction and feature selection. Second, feature selection also helps identify the most relevant features for training a model, and it can be particularly useful in choosing the appropriate feature mapping for training nonlinear models.\nIn practice, there are many convenient methods for selecting variables. For example, statistical tests like the t-test can be used to assess whether a variable is informative. In this section, we will frame feature selection as a model selection problem and introduce subset selection methods first. Second, we will show how feature selection can be transformed into a more manageable hyper-parameter tuning problem with penalty method."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6.html#subset-selection",
    "href": "Courses/c_mlwr1_2024/l6/l6.html#subset-selection",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "3.2 Subset Selection",
    "text": "3.2 Subset Selection\nLet’s recall the main challenge of the nonlinear expansion idea mentioned in the previous lecture, which is feature mapping — how to correctly choose the feature mapping in order to obtain an appropriate augmented feature mapping.\n\n\n\n\n\nGo left or right? Right is not always right.\n\n\n\n\nIntuitively, deciding whether to go left or right is a model selection problem, \\[\n  y = w_0 + w_1x + w_2x^2 \\text{ V.S. } y = w_0 + w_1x + w_2x^5\n\\] but essentially it is a feature selection problem. From the perspective of feature selection, we have three feature variables, \\(\\{ x, x^2, x^5 \\}\\), to choose from, and the variables that end up in the optimal model are the selected ones. Therefore, the feature selection problem can be formulated as a model selection problem. Following this approach, we introduce three methods: best subset selection, forward stepwise selection, and backward stepwise selection.\n\n3.2.1 Best Subset Selection\nThis method involves evaluating all possible subsets of features and selecting the one that results in the best model performance.\n\n\nAlgorithm: Best Subset Selection\n\n\nInputs:\n\n\\(\\textbf{X}\\), a \\(n\\times p\\) matrix, containing all \\(p\\) feature variables.\n\\(y\\) the target variable.\n\\(\\phi\\): the metric to evaluate the model performance.\nDenote \\(\\mathcal{M}_0\\) is the model without any feature variable, i.e. \\(y = w_0 + \\epsilon\\)\n\n\nSteps:\nFor \\(k=1,2,\\dots, p\\):\n\nConsider all possible models that contain \\(k\\) feature variables.\nTrain all the models and evaluate them with an evaluation metric \\(\\phi\\).\nRecord the best model and denote it as \\(\\mathcal{M}_k\\).\n\nCompare models across all \\(\\left\\{\\mathcal{M}_k\\right\\}_{k=1}^{p}\\) and determine the overall best model.\n\nOutput: Return the index of feature variables in the overall best model.\n\nLet’s have a look at the following concrete example:\n\nAnimation-Demo of Best Subset Selection:\n\n    Replay\n\n\n\n\nRemark: While it guarantees the optimal subset, it is computationally expensive, especially when \\(p\\), the number of features is large. Indeed, evaluating all \\(2^p\\) possible subsets of feature variables becomes infeasible as the number of predictors, \\(p\\), grows large. Although one can set an upper limit on the number of feature variables included in the model, e.g. set the upper limit of the for loop as \\(k_{max} &lt; p\\), there is a potential risk of missing better models and thereby excluding important features.\n\n\n\n3.2.2 Stepwise Selection\nTo overcome the main drawback of best subset selection, stepwise selection is a heuristic method that iteratively builds or refines a model by either adding or removing predictors.\n\nForward Stepwise Selection (FSS):\nIt is an iterative method for feature selection. It starts with no feature variables in the model and adds them one at a time, selecting the one that most improves the model’s performance metric. This process continues iteratively, updating the model with the best combination of predictors, until the full model is reached, i.e., the model with all feature variables. More specifically, let’s read the algorithm below:\n\n\nAlgorithm: Forward Stepwise Selection\n\n\nInputs:\n\n\\(\\textbf{X}\\), a \\(n\\times p\\) matrix, containing all \\(p\\) feature variables.\n\\(y\\) the target variable.\n\\(\\phi\\): the metric to evaluate the model performance.\nDenote \\(\\mathcal{M}_0\\) is the model without any feature variable, i.e. \\(y = w_0 + \\epsilon\\)\n\n\nSteps:\nFor \\(k=1,2,\\dots, p\\):\n\nConsider all possible models that model \\(\\mathcal{M}_{k-1}\\) plus one more feature variable.\nTrain all the models and evaluate them with an evaluation metric \\(\\phi\\).\nRecord the best model and denote it as \\(\\mathcal{M}_k\\).\n\nCompare models across all \\(\\left\\{\\mathcal{M}_k\\right\\}_{k=1}^{p}\\) and determine the overall best model.\n\nOutput: Return the index of feature variables in the overall best model.\n\nThe concrete example:\n\n\n    Replay\n\n\n\nFrom this specific example, it can be seen that, unlike best subset selection, forward stepwise selection does not evaluate all possible models, which makes the algorithm more efficient. However, this also means that the algorithm is a greedy solution, making locally optimal decisions at each step. This trade-off sacrifices the guarantee of finding the best subset but significantly reduces computational burden.\n\n\nBackward Stepwise Selection (BSS):\nSimilar to FSS, Backward Stepwise Selection also offers another possible greedy solution. Unlike FSS, BSS starts with the full model and then iteratively removes features to select the optimal models in each step and find the overall best model across all steps. The algorithm is as follows:\n\n\nAlgorithm: Backward Stepwise Selection\n\n\nInputs:\n\n\\(\\textbf{X}\\), a \\(n\\times p\\) matrix, containing all \\(p\\) feature variables.\n\\(y\\) the target variable.\n\\(\\phi\\): the metric to evaluate the model performance.\nDenote \\(\\mathcal{M}_p\\) as the full model that containing all feature variables.\n\n\nSteps:\nFor \\(k= p-1, p-2, \\dots, 1, 0\\):\n\nConsider all possible models that model \\(\\mathcal{M}_{k+1}\\) ignoring one feature variable.\nTrain all the models and evaluate them with an evaluation metric \\(\\phi\\).\nRecord the best model and denote it as \\(\\mathcal{M}_k\\).\n\nCompare models across all \\(\\left\\{\\mathcal{M}_k\\right\\}_{k=1}^{p}\\) and determine the overall best model.\n\nOutput: Return the index of feature variables in the overall best model.\n\nThe concrete example:\n\n\n    Replay"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6.html#regularization",
    "href": "Courses/c_mlwr1_2024/l6/l6.html#regularization",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "3.3 Regularization",
    "text": "3.3 Regularization\nWe already have some feature selection methods, but subset selection algorithms have two major shortcomings. First, these methods either require extensive computation or risk getting stuck in local optima. Second, we often use loops to iterate over a series of models with varying complexity, which is both tedious and inefficient.\nThis leads us to two key questions:\n\nFirst, can we control model complexity with a single hyperparameter, similar to KNN?\nSecond, can this approach avoid the risk of getting stuck in local optima?\n\nThe answer is yes—we can achieve this using regularization methods.\n\n3.3.1 Conceptrual Ideas\nLet’s review the toy example in the previous lecture.\n\n\n\n\n\n\n\n\n\n\n\nIn the previous lecture, we used it to introduce the problem of overfitting. Now, let’s look at this issue from the perspective of feature selection. Do you remember the main challenge of the feature mapping idea? That’s right—choosing an appropriate feature mapping is the key challenge. In other words, here we are considering selecting some suitable variables from a set of feature variables, \\(\\{x, x^2, x^3, x^4\\}\\), to predict the \\(y\\) variable.\nNow, let’s take a look at the relationship between the two models. The Orange Model can be viewed as the full model, \\[\n  y_i=w_0+w_1x_i+w_2x_i^2+w_3x_i^3+w_4x_i^4+\\epsilon_i\n\\] The ideal model, Red Model, \\(y_i=w_0+w_1x_i+w_2x_i^2+\\epsilon_i\\), can be seen as a special case of Orange Model, or the full model with an added constraint on model parameters, i.e. \\[\n  \\textcolor[rgb]{1.00,0.00,0.00}{\\text{Model}} = \\textcolor[rgb]{1.00,0.50,0.00}{\\text{Model}} + \\text{Constraint} (w_3=w_4=0)\n\\] If the Red Model is considered as one of the candidate models in the model selection process, we can gain an insight: candidate models can be expressed as \\[\n  \\textcolor[rgb]{1.00,0.00,0.00}{\\text{Candidate Models}} = \\textcolor[rgb]{1.00,0.50,0.00}{\\text{Full Model}} + \\text{Constraint}(\\textbf{w}).\n\\] Of course, this Constraint, \\(w_3=w_4=0\\), is too specific. What we aim for is to use a single expression to represent a set of candidate models and then select the best one through an algorithm. Let’s look at another example. In the remark from Section 3.2.1, we mentioned that to reduce the computational cost in the best subset algorithm, we can set an upper limit on the number of features, \\(k_{max}\\), included in the model. It can be expressed as \\[\n  \\text{Constraint}(\\textbf{w}): \\sum_{j = 1}^4 \\textbf{1} \\{ w_j \\neq 0 \\} \\leq 3\n\\]\nThis constraint can be interpreted as the total number of non-zero parameters being less than or equal to 3. In a figurative way, it’s like saying to the cute little parameters, “Hey, our data is limited, so only three of you can have non-zero values!” From this perspective, the restriction in our formula represents the “budget” for non-zero parameter values. If you agree with me, let’s rewrite the formula of candidate models as\n\\[\n  \\textcolor[rgb]{1.00,0.00,0.00}{\\text{Candidate Models}} = \\textcolor[rgb]{1.00,0.50,0.00}{\\text{Full Model}} + \\textbf{Budget}(\\textbf{w}).\n\\] What practical significance does this formula have? It’s significant because if we can represent a set of candidate models with a single formula, we can substitute it into the loss function when estimating model parameters. This allows us to frame the model selection problem as an optimization problem.\n\nNote: The “budget” term is a function of model parameters, and it is referred to as penalty term in the formal language.\n\nIn this way, we may have the opportunity to develop a smart algorithm to find the optimal model, rather than relying on brute-force methods like best subset selection, see the figure below.\n\n\n\n\\[\n  \\begin{matrix}\n  \\text{Best subset selection:} \\\\\n  \\\\\n  \\mathcal{L}_{mse}(y_i, w_0+w_1x_i) \\\\\n  \\mathcal{L}_{mse}(y_i, w_0+w_1x_i+w_2x_i^2) \\\\\n  \\mathcal{L}_{mse}(y_i, w_0+w_1x_i+w_2x_i^2+w_3x_i^3) \\\\\n  \\mathcal{L}_{mse}(y_i, w_0+w_1x_i+w_2x_i^2+w_4x_i^4) \\\\\n  \\vdots  \\\\\n  \\end{matrix}\n\\]\n\n\\[\n  \\begin{matrix}\n  \\text{Regularization methods:}  \\\\\n    \\\\\n    \\\\\n    \\\\\n  \\mathcal{L}_{mse}(y_i, f(x_i，\\textbf{w})+\\text{Budget}(\\textbf{w})) \\\\\n    \\\\\n    \\\\\n    \\\\\n  \\end{matrix}\n\\]\n\n\nwhere \\(\\mathcal{L}_{mse}\\) is the mse-loss function and \\(f(x_i)\\) is the full model, \\[\n  f(x_i)=w_0+w_1x_i+w_2x_i^2+w_3x_i^3+w_4x_i^4+\\epsilon_i\n\\]  LHS: In best subset selection, we need to solve multiple optimization problems, one for each candidate model. RHS: However, in regularization methods, with the help of the budget term, we only need to solve a single integrated optimization problem. \n\nHow exactly does regularization methods work? Let’s discuss it further in the next subsection.\n\n\n3.3.2 Ridge Regression\nOverall, ridge regression belongs to the family of regularization methods. It represents the budget term using the \\(l_2\\)-norm, which has favorable mathematical properties, making the optimization problem solvable. To better illustrate this, let’s first revisit the previous discussion and express the best subset selection method using an optimization formula.\n\\[\n  \\begin{matrix}\n    \\min_{\\textbf{w}} & \\sum_{i=1}^n (y_i- w_0+w_1x_i+w_2x_i^2+w_3x_i^3+w_4x_i^4)^2 \\\\\n    s.t. & \\sum_{j = 1}^4 \\textbf{1} \\{ w_j \\neq 0 \\} \\leq 3\n  \\end{matrix}\n\\] i.e. consider a constraint while minimizing the MSE loss. However, this constraint lacks favorable mathematical properties, such as differentiability, making it impossible to solve the optimization problem. Therefore, we need to modify the constraint. The \\(l_2\\)-norm is a good option, \\[\n  \\sum_{j = 1}^4 w_i^2 \\leq 3\n\\] With the \\(l_2\\)-norm constraint, we can modify the problem as a ridge regression problem \\[\n  \\begin{matrix}\n    \\min_{\\textbf{w}} & \\sum_{i=1}^n (y_i- w_0+w_1x_i+w_2x_i^2+w_3x_i^3+w_4x_i^4)^2 \\\\\n    s.t. & \\sum_{j = 1}^4 w_j^2 \\leq C\n  \\end{matrix}\n\\] Here, \\(C\\) represents a budget for all the parameter values and will be treated as a hyper-parameter. When \\(C\\) is large, we have a generous budget and will consider more complex models. Conversely, when \\(C\\) is small, our choices are limited, and the resulting model will have lower complexity.\nThis formulation of the optimization problem is typically called the budget form, and we need to solve it using the method of Lagrange multipliers. The optimization results are the regression coefficients of ridge regression.\nIt also has an equivalent form, known as the penalty form\n\\[\n  \\min_{\\textbf{w}} \\left\\{ \\sum_{i=1}^n (y_i- w_0+w_1x_i+w_2x_i^2+w_3x_i^3+w_4x_i^4)^2 + \\lambda \\sum_{j = 1}^4 w_j^2 \\right\\}\n\\]\nIn this form, \\(\\lambda\\) is the given penalty weight, which corresponds to the hyper-parameter \\(C\\), but with the opposite meaning. When \\(\\lambda\\) is large, to minimize the loss function, we need to consider smaller model parameters, meaning our budget \\(C\\) is small, and as a result, we obtain a model with lower complexity. Conversely, when \\(\\lambda\\) is small, our budget \\(C\\) is large, and we end up with a model of higher complexity. Let’s see the following example.\n\nExample:\nNext, we will apply ridge regression to our toy example. Here, we choose the full model as a 4th-order polynomial regression and experiment with different penalty parameters (\\(\\lambda\\)). The fitting results and estimated regression coefficients are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModels\n\\(\\lambda\\)\n\\(w_0\\)\n\\(w_1\\)\n\\(w_2\\)\n\\(w_3\\)\n\\(w_4\\)\n\n\n\n\nOrange\n0\n-22.438\n44.151\n-25.390\n5.681\n-0.438\n\n\nRed\n0.001\n3.624\n-2.307\n0.195\n0.051\n-0.005\n\n\nBlue\n0.01\n2.931\n-1.463\n0.027\n0.022\n0.002\n\n\nGreen\n10\n0.626\n-0.083\n-0.007\n0.000\n0.000\n\n\n\n\nThe table summarizes the results of a ridge regression experiment, illustrating the relationship between the penalty parameter \\(\\lambda\\) and model complexity. As \\(\\lambda\\) increases, the penalty for larger parameter values grows stronger, resulting in smaller coefficients for all parameters (\\(w_0, w_1, w_2, w_3, w_4\\)).\nFor example, with \\(\\lambda = 0\\) (orange model), the model has no penalty, leading to large parameter values and a highly complex model. As \\(\\lambda\\) increases to \\(0.001\\) (red model) and \\(0.01\\) (blue model ), the parameters shrink, indicating a reduction in model complexity. When \\(\\lambda = 10\\) (green model), most parameter values approach zero, yielding a very simple model.\nThis experiment demonstrates that ridge regression effectively controls model complexity through the hyper-parameter \\(\\lambda\\), where larger \\(\\lambda\\) values correspond to simpler models with lower complexity.\nWe can also observe that as \\(\\lambda\\) increases, the estimated regression coefficients continue to shrink. This is why the ridge regression model is also referred to as a shrinkage method. It is primarily used as a robust regression model to address over fitting issues.\nIf we wish to use it as a feature selection tool, an additional threshold value needs to be set. For instance, feature variables corresponding to regression coefficients smaller than \\(0.01\\) can be excluded. For the purpose of feature selection, there are other regularization methods available, such as the LASSO, which we will discuss next.\n\n\n\n3.3.3 LASSO\nThe LASSO (Least Absolute Shrinkage and Selection Operator) is a regularization method that adds an \\(l_1\\)-norm penalty to the loss function, encouraging sparsity in the model coefficients. This unique property enables LASSO to perform both shrinkage and feature selection simultaneously, making it a powerful tool for high-dimensional data analysis.\nCompared to the \\(l_2\\)-norm penalty used in ridge regression, LASSO employs the \\(l_1\\)-norm to calculate the coefficients budget, which is \\[\n  \\sum_{j = 1}^p |w_j| \\leq C\n\\] So, LASSO problem can be expressed as in the budget form, \\[\n  \\begin{matrix}\n    \\min_{\\textbf{w}} & \\sum_{i=1}^n (y_i- w_0+w_1x_i+w_2x_2+\\dots+w_px_p)^2 \\\\\n    s.t. & \\sum_{j = 1}^p |w_j| \\leq C\n  \\end{matrix}\n\\] or the penalty form \\[\n  \\min_{\\textbf{w}} \\left\\{ \\sum_{i=1}^n (y_i- w_0+w_1x_i+w_2x_2+\\dots+w_px_p)^2 + \\lambda \\sum_{j = 1}^p |w_j| \\right\\}\n\\]\nSimilar to Ridge regression, the budget parameter \\(C\\) and penalty parameter \\(\\lambda\\) here are treated as hyper parameters. They carry the same significance as the hyper parameters in Ridge regression. By solving this optimization problem, we can obtain the estimated LASSO parameters.\nIn the laboratory exercises, we will find that, unlike the shrinkage results of ridge regression, the estimates from LASSO are called sparse results, meaning that most regression coefficients are zero, while a few coefficients are non-zero. Therefore LASSO is also called Sparse method. This characteristic of LASSO makes it an important tool for feature selection.\nThe following diagram conceptually explains why LASSO produces sparse results.\n\n\n\n\n\nRidge Regression VS LASSO\n\n\n\n\nThe left and right sides represent the optimization problem for the same regression model. We can see that in this optimization problem, we have two optimization variables, and the ellipse represents the contour plot of the loss function for the regression problem, where the closer to the center, the lower the loss value. Therefore, without considering the pink figure, the dark blue point represents the optimal solution to the optimization problem, which is the least squares solution, i.e., the regression coefficients for the regular regression model.\nHowever, when we consider the constraint, we need to limit the possible points to a specific region, which is the pink area. On the left, the circle represents the feasible region for the two coefficients under the \\(l_2\\) norm constraint, while on the right, the diamond shape represents the feasible region for the two coefficients under the \\(l_1\\) norm constraint. In other words, we can only consider the optimal solution within the pink region. As a result, the optimal solution is the light blue point, which is the point closest to the minimum loss function value within the feasible region.\nFrom the above diagram, it is clear that the geometric characteristics of the two penalty functions determine the nature of their solutions. Under the \\(l_2\\) norm, the solution is closer to the y-axis, indicating that, due to the penalty term, the estimate of \\(w_1\\) undergoes shrinkage toward zero. Under the \\(l_1\\) norm, the solution lies exactly on the y-axis, meaning that, due to the penalty term, \\(w_1\\) becomes exactly zero. This explains why the \\(l_1\\) norm leads to sparse results, while the \\(l_2\\) norm leads to shrinkage results.\n\n\nLecture 6 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6.html",
    "href": "Courses/c_mlwr1_2024/l6/l6.html",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "",
    "text": "In this lecture, we focus on model validation and selection problem. Firstly, we will introduce KNN method and use it to interpret the over fitting problem in classification problems. At the same time, we also explained different types of parameters through the discussion of KNN, leading to the model selection problem. Then, we introduced various model validation methods. Finally, we discussed a specific model selection problem, namely feature selection. Here, we will introduce ridge regression and LASSO."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6.html#overfitting-problem",
    "href": "Courses/c_mlwr1_2024/l6/l6.html#overfitting-problem",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "",
    "text": "Obviously, the parameter \\(K\\) plays a critical role in KNN method. The number of neighbors we consider to make the final decision significantly impacts the model’s performance. Let’s look at the example below.\nThis is a binary classification problem where the true decision boundary is a sine curve. Due to the influence of noise variables, some observations cross the boundary. By using KNN with \\(k=1\\) and \\(k=9\\), we can observe the resulting decision boundaries as shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is evident that the result with \\(k=9\\) is far better than that with \\(k=1\\). When \\(k=9\\), the true decision boundary of the sine curve can be largely identified or approximated by the KNN algorithm. However, when \\(k=1\\), we notice the emergence of many isolated “islands” on both sides of the true decision boundary. Errors occur in these “islands” because we focus excessively on individual observations in the training data, leading to overfitting."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6.html#k-nearest-neighbors",
    "href": "Courses/c_mlwr1_2024/l6/l6.html#k-nearest-neighbors",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "1.1 K Nearest Neighbors",
    "text": "1.1 K Nearest Neighbors\nThe k-Nearest Neighbors (KNN) algorithm is a unique classifier that stands apart from linear classifiers. It’s based on one of the most intuitive ideas in machine learning: decisions are influenced by proximity. Unlike complex parametric methods, KNN operates in a way that mirrors human decision-making in everyday life.\nBasic Idea: The fundamental concept of KNN can be likened to how we often follow trends or social cues. Imagine your neighbor buys a new type of lawn-mowing robot. Your spouse, noticing their satisfaction, might also want to buy one. KNN applies a similar principle in classification: it assigns a label to a data point based on the labels of its nearest neighbors in the feature space. This idea of “going with the flow” underpins the simplicity of KNN.\n\n\nAlgorithm: K-Nearest Neighbors Method\n\n\nInputs:\n\n\\(\\textbf{X}\\), \\(n\\times p\\) matrix, containing all \\(p\\) feature variables.\n\\(y\\), \\(n \\times 1\\) array, the target variable.\n\\(x_{new}\\), \\(p \\times 1\\) array, the feature values of a new observation.\n\\(k\\), the number of neighbors.\n\n\nSteps:\n\nCalculate the distance between \\(x_{new}\\) and each sample points in \\(\\textbf{X}\\).\nFind \\(k\\) sample points that have shortest distance to \\(x_{new}\\)\nAssign the label that is most common among the neighbors.\n\n\nOutput: The assigned label.\n\nSee the following demonstration. In this toy example, we have a binary classification problem with 2 feature variables. There are 10 observations in the training set. We apply 3-NN method to classify an arbitrary point in the 2D space according to the training samples.\n\n\n\n\n\n\n\n\nDemo of KNN method\n\n\n\n\n\n\n\nFormal formulation( NE ): KNN method can be easily generalized to multiple classification problems, \\(M\\) classes. For a multiple classification problem, suppose we have a training set \\(\\left\\{\\textbf{x}_i, y_i \\right\\}_{i = 1}^{N}\\), where \\(y_i\\) takes value in a set of labels \\(\\left\\{1, 2, \\dots, M \\right\\}\\). For a new observation point, \\(\\textbf{x}_{\\text{new}}\\), the posterior probability that the new point belongs to the \\(j\\)th class given the feature values can be evaluated as \\[\n  \\Pr(y=j|\\textbf{x}_{\\text{new}}) = \\frac{1}{K}\\sum_{i \\in \\mathbb{N}_k(\\textbf{x}_{\\text{new}})} \\textbf{1}\\{y_i = j\\}\n\\] where \\(\\mathbb{N}_k(\\textbf{x}_{\\text{new}})\\) denotes the index set of k nearest neighbors of \\(\\textbf{x}_{\\text{new}}\\) in the training set, the indicator function \\(\\textbf{1}\\{y_i = j\\} = 1\\) if \\(y_i = j\\) otherwise \\(0\\), and \\(j = 1,2,\\dots,M\\). After evaluating the posterior probabilities, one can make the decision accordingly.\nDiscussion:\n\nKNN method also can be applied to a regression problem. In a regression scenario, the prediction is the average value of the k nearest neighbor’s values of target variable. \\[\ny|\\textbf{x}_{\\text{new}} = \\frac{1}{K}\\sum_{i \\in \\mathbb{N}_k(\\textbf{x}_{\\text{new}})} y_i\n\\]\nKNN stands out as a special type of classifier for several reasons.\n\nMemory-Based Model: KNN is often called a memory-based model because it requires storing all the training data. Without the training data, predictions cannot be made. This contrasts with classifiers like Gaussian Discriminant Analysis (GDA), where information from the training data is condensed into model parameters, eliminating the need to retain the original dataset. While many memory-based models exist, KNN is among the simplest.\nLazy Learner: Unlike most classifiers that estimate parameters during a training phase, KNN does not perform explicit training. Once the value of \\(K\\) is chosen, the algorithm simply relies on the stored data for prediction, making it a “lazy learner.”\n\n\nDespite its simplicity, KNN is a powerful algorithm in many contexts, particularly when the dataset is small or when a straightforward decision boundary suffices."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_home.html#lecture-notes",
    "href": "Courses/c_mlwr1_2024/l6/l6_home.html#lecture-notes",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "Lecture notes:",
    "text": "Lecture notes:\n\nRead the integrated notes: here\nRead the pagination notes: here\nDownload the PDF notes here."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_home.html#reading-guidelines-you-can-read-the-book",
    "href": "Courses/c_mlwr1_2024/l6/l6_home.html#reading-guidelines-you-can-read-the-book",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "Reading Guidelines: You can read the book…",
    "text": "Reading Guidelines: You can read the book…\nFor lecture 6, it is recommended that you read the following sections in the textbook.\n\nAbout KNN, read subsection 4.7.6, page 181\nAbout validation methods, read section 5.1\nAbout model selection and feature selection, read sections 6.1 and 6.2."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_home.html#discussion",
    "href": "Courses/c_mlwr1_2024/l6/l6_home.html#discussion",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "Discussion",
    "text": "Discussion\nSlides for discussion: click here"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_home.html#lab",
    "href": "Courses/c_mlwr1_2024/l6/l6_home.html#lab",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "Lab:",
    "text": "Lab:\nLaboratory: Entrance\nSolutions: click here\nR file: download here.\n\nCourse Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l4/l4_home.html#lab",
    "href": "Courses/c_mlwr1_2024/l4/l4_home.html#lab",
    "title": "Lecture 4: Gaussian Discrimination Analysis",
    "section": "Lab:",
    "text": "Lab:\nLaboratory Entrance\nSolutions: Click here\nR file: download here.\n\nCourse Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/Discussion of Lectures 5.html#lecture-5-regression-models",
    "href": "Courses/c_mlwr1_2024/Discussion of Lectures 5.html#lecture-5-regression-models",
    "title": "Discussion of Lectures 5",
    "section": "",
    "text": "Do you remember the names of prediction and prediction errors in the regression analysis?\nMSE V.S. Likelihood function V.S. Loss function\nCan you describe the basic idea of nonlinear extension in simple words?\nWhat are the main challenges of this idea?\nCan you explain what is overfitting problem?\nWhat are the main reasons?\nHow do you interpret overfitting problem from variance-bias trade off point of view?"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6.html#overfitting-in-classification-problems",
    "href": "Courses/c_mlwr1_2024/l6/l6.html#overfitting-in-classification-problems",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "1.2 Overfitting in Classification Problems",
    "text": "1.2 Overfitting in Classification Problems\nObviously, the parameter \\(K\\) plays a critical role in KNN method. The number of neighbors we consider to make the final decision significantly impacts the model’s performance. Let’s look at the example below.\nThis is a binary classification problem where the true decision boundary is a sine curve. Due to the influence of noise variables, some observations cross the boundary. By using KNN with \\(k=1\\) and \\(k=9\\), we can observe the resulting decision boundaries as shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is evident that the result with \\(k=9\\) is far better than that with \\(k=1\\). When \\(k=9\\), the true decision boundary of the sine curve can be largely identified or approximated by the KNN algorithm. However, when \\(k=1\\), we notice the emergence of many isolated “islands” on both sides of the true decision boundary. Errors occur in these “islands” because we focus excessively on individual observations in the training data, leading to overfitting."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6.html#algorithm-best-subset-selection",
    "href": "Courses/c_mlwr1_2024/l6/l6.html#algorithm-best-subset-selection",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "Algorithm: Best Subset Selection",
    "text": "Algorithm: Best Subset Selection\nThe algorithm is described step by step:\n\nStep 1: Description of the first step.\nStep 2: Description of the second step.\nStep 3: Description of the third step.\n\n\nPseudocode:"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1.html#regression-model",
    "href": "Courses/c_mlwr1_2024/l1/l1.html#regression-model",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "5.1 Regression Model",
    "text": "5.1 Regression Model\nThe machine learning problem can be understood as regression problem when the target variable is a continuous variable. For example, predict the house price based on different feature variables; predict the pixel values of CT scans based on MRI scans; predict the stock price based on feature variables of market. A simple scenario displayed in the figure below, a basic regression model is a linear model, \\(y_i = w_0 + w_1x_i + \\epsilon_i\\). From a geometric perspective, a linear regression model can be seen as a straight line that passes through all sample observations. In the generalization stage, the target value can be predicted from feature variable through the regression model.\n\n\n\n\n\nFigure 8: Regression Problem"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l1/l1.html#classification-model",
    "href": "Courses/c_mlwr1_2024/l1/l1.html#classification-model",
    "title": "Lecture 1: Introduction to Machine Learning",
    "section": "5.2 Classification Model",
    "text": "5.2 Classification Model\nThe problem can be viewed as a classification problem when the target variable is categorical. We often refer to this type of target variable as labels. For example, in the classification with Iris data, the species variable is the label variable, and we aim for finding a good “function” taking 4 shaping variables as input to predict the labels based on data. This function is often refer to a classifier. So, what kind of function can perform this role? Let’s take a look at a real classifier first, a “coin sorter.” Its operation is quite simple, as it classifies coins based on their different diameters. Inside the machine, there are holes of varying sizes corresponding to different diameters, and through vibration, coins will fall into the holes that match their size. In essence, it’s classifying by comparing a variable to a threshold value. The idea is quite simple, but it is just the essential idea of machine learning classifier.\n\n\n\n\nFigure 9: LHS: Classification with iris data. RHS: A real classifier, coin sorter. The working principle: Variable (diameter) V.S. Threshold value.\n\n\n\nWell, usually we have multiple feature variables in a classification problem, then how do we apply this simple working principle to design a classifier? Let’s see another example. You might not know yet, in fact, teacher becomes a classifier after an exam. Well, to pass or not to pass is a classification problem. Suppose, in a secret exam, each student answers 5 questions and each question is worth 20 points. Student passes the exam if the total points are larger or equal to 60. I have corrected all the exams; the results are summarized in Table 1, and \\(1\\) indicating the question was correctly answered and \\(0\\) indicating not. Then, who can pass the exam?\n\n\n\n\n\nI believe it is a very simple problem, for example, Super girl correctly answered 4 questions and get 80 points that is above the threshold value 60, so she passed the exam! However, spiderman only got 20 points that is lower than 60, so he can’t pass. If we clearly write down the calculation process, we actually used the following formula to calculate the totol score, then compare the total score with the critical point, 60.\n\\[\n  20\\times Q_1 + 20\\times Q_2 + 20\\times Q_3 + 20\\times Q_4 + 20\\times Q_5 \\geq 60\n\\]\n\nNow, we know what a simple classifier looks like. Essentially, it is a two-step procedure. We create a single variable through the weighted sum of all feature variables first, then compare the resulting value with a threshold value. In formal, the classifier can be represented as\n\\[\n  y = \\text{Sign}(w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_p x_p)\n\\]\nwhere \\(\\text{Sign}(x)\\) is a sign function returning 1 if \\(x&gt;0\\) and 0 if \\(x&lt;0\\). We refer coefficients \\(w_1, \\dots, w_p\\) as weights, the weighted sum of feature variables \\(w_1 x_1 + w_2 x_2 + \\dots + w_p x_p\\) as scores and \\(w_0\\), the threshold value, as bias. If the score value is equal to 0, then this observation can’t be classified by this classifier, and the thing we can do best is flip a coin to make the decision. We call all the points that satisfy equation \\(w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_p x_p = 0\\) as the decision boundary. For example, in a 2D feature space, the decision boundary \\(w_0 + w_1x_1 + w_2x_2 =0\\) is just a straight line with a slope of \\(-w_1/w_2\\) and an intercept of \\(-w_0/w_2\\), see the figure below.\n\n\n\n\nFigure 10: In this example, the 2D feature space is cut into two parts by the decision boundary (red line). For any unlabeled observations (blue dots), if it is above the decision boundary, then it will be classified as yellow , otherwise, green.\n\n\n\nThis kind of classifier is called linear classifier, since the decision boundary is presented by a linear function. It is a straight line in 2D space, a plane in 3D space, and hyper-plane in a higher dimension space. You might have already realized that in fact, a classifier is solely determined by its weights and bias, and machine learning algorithms tell us how to find the optimal weights and bias through data. There are several classical methods (algorithms) for learning a linear classifier which are perceptron algorithm, linear discriminant analysis, logistic regression, and maximum margin classifier. In this course, we will introduce all of them except maximum margin classifier.\nRemark: Just as all the rules of arithmetic start with \\(1+1\\), don’t underestimate this linear classifier. You will see that all complex classifiers are built upon them. For example, maximum margin classifier is the foundation of SVM (Support vector machine) which dominate machine learning world for 20 years, the perceptron algorithm is the starting point of artificial neural net works, and no matter how complex a neural network architecture may be, as long as it is a classifier, its final layer will inevitably be a logistic regression model.\n\nLecture 1 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_3.html",
    "href": "Courses/c_mlwr1_2024/l6/l6_3.html",
    "title": "3. Feature Selection",
    "section": "",
    "text": "Now is a good time to discuss the feature selection problem, as it can be formulated as a model selection problem and, in turn, can be viewed as a hyper-parameter tuning problem in a specific setting."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_3.html#overview",
    "href": "Courses/c_mlwr1_2024/l6/l6_3.html#overview",
    "title": "3. Feature Selection",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nFeature selection is an important step in building machine learning models. First, reducing the dimensionality of the dataset is often necessary, and there are two main approaches to achieve this: feature extraction and feature selection. Second, feature selection also helps identify the most relevant features for training a model, and it can be particularly useful in choosing the appropriate feature mapping for training nonlinear models.\nIn practice, there are many convenient methods for selecting variables. For example, statistical tests like the t-test can be used to assess whether a variable is informative. In this section, we will frame feature selection as a model selection problem and introduce subset selection methods first. Second, we will show how feature selection can be transformed into a more manageable hyper-parameter tuning problem with penalty method."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_3.html#subset-selection",
    "href": "Courses/c_mlwr1_2024/l6/l6_3.html#subset-selection",
    "title": "3. Feature Selection",
    "section": "3.2 Subset Selection",
    "text": "3.2 Subset Selection\nLet’s recall the main challenge of the nonlinear expansion idea mentioned in the previous lecture, which is feature mapping — how to correctly choose the feature mapping in order to obtain an appropriate augmented feature mapping.\n\n\n\n\n\nGo left or right? Right is not always right.\n\n\n\n\nIntuitively, deciding whether to go left or right is a model selection problem, \\[\n  y = w_0 + w_1x + w_2x^2 \\text{ V.S. } y = w_0 + w_1x + w_2x^5\n\\] but essentially it is a feature selection problem. From the perspective of feature selection, we have three feature variables, \\(\\{ x, x^2, x^5 \\}\\), to choose from, and the variables that end up in the optimal model are the selected ones. Therefore, the feature selection problem can be formulated as a model selection problem. Following this approach, we introduce three methods: best subset selection, forward stepwise selection, and backward stepwise selection.\n\n3.2.1 Best Subset Selection\nThis method involves evaluating all possible subsets of features and selecting the one that results in the best model performance.\n\n\nAlgorithm: Best Subset Selection\n\n\nInputs:\n\n\\(\\textbf{X}\\), a \\(n\\times p\\) matrix, containing all \\(p\\) feature variables.\n\\(y\\) the target variable.\n\\(\\phi\\): the metric to evaluate the model performance.\nDenote \\(\\mathcal{M}_0\\) is the model without any feature variable, i.e. \\(y = w_0 + \\epsilon\\)\n\n\nSteps:\nFor \\(k=1,2,\\dots, p\\):\n\nConsider all possible models that contain \\(k\\) feature variables.\nTrain all the models and evaluate them with an evaluation metric \\(\\phi\\).\nRecord the best model and denote it as \\(\\mathcal{M}_k\\).\n\nCompare models across all \\(\\left\\{\\mathcal{M}_k\\right\\}_{k=1}^{p}\\) and determine the overall best model.\n\nOutput: Return the index of feature variables in the overall best model.\n\nLet’s have a look at the following concrete example:\n\nAnimation-Demo of Best Subset Selection:\n\n    Replay\n\n\n\n\nRemark: While it guarantees the optimal subset, it is computationally expensive, especially when \\(p\\), the number of features is large. Indeed, evaluating all \\(2^p\\) possible subsets of feature variables becomes infeasible as the number of predictors, \\(p\\), grows large. Although one can set an upper limit on the number of feature variables included in the model, e.g. set the upper limit of the for loop as \\(k_{max} &lt; p\\), there is a potential risk of missing better models and thereby excluding important features.\n\n\n\n3.2.2 Stepwise Selection\nTo overcome the main drawback of best subset selection, stepwise selection is a heuristic method that iteratively builds or refines a model by either adding or removing predictors.\n\nForward Stepwise Selection (FSS):\nIt is an iterative method for feature selection. It starts with no feature variables in the model and adds them one at a time, selecting the one that most improves the model’s performance metric. This process continues iteratively, updating the model with the best combination of predictors, until the full model is reached, i.e., the model with all feature variables. More specifically, let’s read the algorithm below:\n\n\nAlgorithm: Forward Stepwise Selection\n\n\nInputs:\n\n\\(\\textbf{X}\\), a \\(n\\times p\\) matrix, containing all \\(p\\) feature variables.\n\\(y\\) the target variable.\n\\(\\phi\\): the metric to evaluate the model performance.\nDenote \\(\\mathcal{M}_0\\) is the model without any feature variable, i.e. \\(y = w_0 + \\epsilon\\)\n\n\nSteps:\nFor \\(k=1,2,\\dots, p\\):\n\nConsider all possible models that model \\(\\mathcal{M}_{k-1}\\) plus one more feature variable.\nTrain all the models and evaluate them with an evaluation metric \\(\\phi\\).\nRecord the best model and denote it as \\(\\mathcal{M}_k\\).\n\nCompare models across all \\(\\left\\{\\mathcal{M}_k\\right\\}_{k=1}^{p}\\) and determine the overall best model.\n\nOutput: Return the index of feature variables in the overall best model.\n\nThe concrete example:\n\n\n    Replay\n\n\n\nFrom this specific example, it can be seen that, unlike best subset selection, forward stepwise selection does not evaluate all possible models, which makes the algorithm more efficient. However, this also means that the algorithm is a greedy solution, making locally optimal decisions at each step. This trade-off sacrifices the guarantee of finding the best subset but significantly reduces computational burden.\n\n\nBackward Stepwise Selection (BSS):\nSimilar to FSS, Backward Stepwise Selection also offers another possible greedy solution. Unlike FSS, BSS starts with the full model and then iteratively removes features to select the optimal models in each step and find the overall best model across all steps. The algorithm is as follows:\n\n\nAlgorithm: Backward Stepwise Selection\n\n\nInputs:\n\n\\(\\textbf{X}\\), a \\(n\\times p\\) matrix, containing all \\(p\\) feature variables.\n\\(y\\) the target variable.\n\\(\\phi\\): the metric to evaluate the model performance.\nDenote \\(\\mathcal{M}_p\\) as the full model that containing all feature variables.\n\n\nSteps:\nFor \\(k= p-1, p-2, \\dots, 1, 0\\):\n\nConsider all possible models that model \\(\\mathcal{M}_{k+1}\\) ignoring one feature variable.\nTrain all the models and evaluate them with an evaluation metric \\(\\phi\\).\nRecord the best model and denote it as \\(\\mathcal{M}_k\\).\n\nCompare models across all \\(\\left\\{\\mathcal{M}_k\\right\\}_{k=1}^{p}\\) and determine the overall best model.\n\nOutput: Return the index of feature variables in the overall best model.\n\nThe concrete example:\n\n\n    Replay"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_3.html#regularization",
    "href": "Courses/c_mlwr1_2024/l6/l6_3.html#regularization",
    "title": "3. Feature Selection",
    "section": "3.3 Regularization",
    "text": "3.3 Regularization\nWe already have some feature selection methods, but subset selection algorithms have two major shortcomings. First, these methods either require extensive computation or risk getting stuck in local optima. Second, we often use loops to iterate over a series of models with varying complexity, which is both tedious and inefficient.\nThis leads us to two key questions:\n\nFirst, can we control model complexity with a single hyperparameter, similar to KNN?\nSecond, can this approach avoid the risk of getting stuck in local optima?\n\nThe answer is yes—we can achieve this using regularization methods.\n\n3.3.1 Conceptrual Ideas\nLet’s review the toy example in the previous lecture.\n\n\n\n\n\n\n\n\n\n\n\nIn the previous lecture, we used it to introduce the problem of overfitting. Now, let’s look at this issue from the perspective of feature selection. Do you remember the main challenge of the feature mapping idea? That’s right—choosing an appropriate feature mapping is the key challenge. In other words, here we are considering selecting some suitable variables from a set of feature variables, \\(\\{x, x^2, x^3, x^4\\}\\), to predict the \\(y\\) variable.\nNow, let’s take a look at the relationship between the two models. The Orange Model can be viewed as the full model, \\[\n  y_i=w_0+w_1x_i+w_2x_i^2+w_3x_i^3+w_4x_i^4+\\epsilon_i\n\\] The ideal model, Red Model, \\(y_i=w_0+w_1x_i+w_2x_i^2+\\epsilon_i\\), can be seen as a special case of Orange Model, or the full model with an added constraint on model parameters, i.e. \\[\n  \\textcolor[rgb]{1.00,0.00,0.00}{\\text{Model}} = \\textcolor[rgb]{1.00,0.50,0.00}{\\text{Model}} + \\text{Constraint} (w_3=w_4=0)\n\\] If the Red Model is considered as one of the candidate models in the model selection process, we can gain an insight: candidate models can be expressed as \\[\n  \\textcolor[rgb]{1.00,0.00,0.00}{\\text{Candidate Models}} = \\textcolor[rgb]{1.00,0.50,0.00}{\\text{Full Model}} + \\text{Constraint}(\\textbf{w}).\n\\] Of course, this Constraint, \\(w_3=w_4=0\\), is too specific. What we aim for is to use a single expression to represent a set of candidate models and then select the best one through an algorithm. Let’s look at another example. In the remark from Section 3.2.1, we mentioned that to reduce the computational cost in the best subset algorithm, we can set an upper limit on the number of features, \\(k_{max}\\), included in the model. It can be expressed as \\[\n  \\text{Constraint}(\\textbf{w}): \\sum_{j = 1}^4 \\textbf{1} \\{ w_j \\neq 0 \\} \\leq 3\n\\]\nThis constraint can be interpreted as the total number of non-zero parameters being less than or equal to 3. In a figurative way, it’s like saying to the cute little parameters, “Hey, our data is limited, so only three of you can have non-zero values!” From this perspective, the restriction in our formula represents the “budget” for non-zero parameter values. If you agree with me, let’s rewrite the formula of candidate models as\n\\[\n  \\textcolor[rgb]{1.00,0.00,0.00}{\\text{Candidate Models}} = \\textcolor[rgb]{1.00,0.50,0.00}{\\text{Full Model}} + \\textbf{Budget}(\\textbf{w}).\n\\] What practical significance does this formula have? It’s significant because if we can represent a set of candidate models with a single formula, we can substitute it into the loss function when estimating model parameters. This allows us to frame the model selection problem as an optimization problem.\n\nNote: The “budget” term is a function of model parameters, and it is referred to as penalty term in the formal language.\n\nIn this way, we may have the opportunity to develop a smart algorithm to find the optimal model, rather than relying on brute-force methods like best subset selection, see the figure below.\n\n\n\n\\[\n  \\begin{matrix}\n  \\text{Best subset selection:} \\\\\n  \\\\\n  \\mathcal{L}_{mse}(y_i, w_0+w_1x_i) \\\\\n  \\mathcal{L}_{mse}(y_i, w_0+w_1x_i+w_2x_i^2) \\\\\n  \\mathcal{L}_{mse}(y_i, w_0+w_1x_i+w_2x_i^2+w_3x_i^3) \\\\\n  \\mathcal{L}_{mse}(y_i, w_0+w_1x_i+w_2x_i^2+w_4x_i^4) \\\\\n  \\vdots  \\\\\n  \\end{matrix}\n\\]\n\n\\[\n  \\begin{matrix}\n  \\text{Regularization methods:}  \\\\\n    \\\\\n    \\\\\n    \\\\\n  \\mathcal{L}_{mse}(y_i, f(x_i，\\textbf{w})+\\text{Budget}(\\textbf{w})) \\\\\n    \\\\\n    \\\\\n    \\\\\n  \\end{matrix}\n\\]\n\n\nwhere \\(\\mathcal{L}_{mse}\\) is the mse-loss function and \\(f(x_i)\\) is the full model, \\[\n  f(x_i)=w_0+w_1x_i+w_2x_i^2+w_3x_i^3+w_4x_i^4+\\epsilon_i\n\\]  LHS: In best subset selection, we need to solve multiple optimization problems, one for each candidate model. RHS: However, in regularization methods, with the help of the budget term, we only need to solve a single integrated optimization problem. \n\nHow exactly does regularization methods work? Let’s discuss it further in the next subsection.\n\n\n3.3.2 Ridge Regression\nOverall, ridge regression belongs to the family of regularization methods. It represents the budget term using the \\(l_2\\)-norm, which has favorable mathematical properties, making the optimization problem solvable. To better illustrate this, let’s first revisit the previous discussion and express the best subset selection method using an optimization formula.\n\\[\n  \\begin{matrix}\n    \\min_{\\textbf{w}} & \\sum_{i=1}^n (y_i- w_0+w_1x_i+w_2x_i^2+w_3x_i^3+w_4x_i^4)^2 \\\\\n    s.t. & \\sum_{j = 1}^4 \\textbf{1} \\{ w_j \\neq 0 \\} \\leq 3\n  \\end{matrix}\n\\] i.e. consider a constraint while minimizing the MSE loss. However, this constraint lacks favorable mathematical properties, such as differentiability, making it impossible to solve the optimization problem. Therefore, we need to modify the constraint. The \\(l_2\\)-norm is a good option, \\[\n  \\sum_{j = 1}^4 w_i^2 \\leq 3\n\\] With the \\(l_2\\)-norm constraint, we can modify the problem as a ridge regression problem \\[\n  \\begin{matrix}\n    \\min_{\\textbf{w}} & \\sum_{i=1}^n (y_i- w_0+w_1x_i+w_2x_i^2+w_3x_i^3+w_4x_i^4)^2 \\\\\n    s.t. & \\sum_{j = 1}^4 w_j^2 \\leq C\n  \\end{matrix}\n\\] Here, \\(C\\) represents a budget for all the parameter values and will be treated as a hyper-parameter. When \\(C\\) is large, we have a generous budget and will consider more complex models. Conversely, when \\(C\\) is small, our choices are limited, and the resulting model will have lower complexity.\nThis formulation of the optimization problem is typically called the budget form, and we need to solve it using the method of Lagrange multipliers. The optimization results are the regression coefficients of ridge regression.\nIt also has an equivalent form, known as the penalty form\n\\[\n  \\min_{\\textbf{w}} \\left\\{ \\sum_{i=1}^n (y_i- w_0+w_1x_i+w_2x_i^2+w_3x_i^3+w_4x_i^4)^2 + \\lambda \\sum_{j = 1}^4 w_j^2 \\right\\}\n\\]\nIn this form, \\(\\lambda\\) is the given penalty weight, which corresponds to the hyper-parameter \\(C\\), but with the opposite meaning. When \\(\\lambda\\) is large, to minimize the loss function, we need to consider smaller model parameters, meaning our budget \\(C\\) is small, and as a result, we obtain a model with lower complexity. Conversely, when \\(\\lambda\\) is small, our budget \\(C\\) is large, and we end up with a model of higher complexity. Let’s see the following example.\n\nExample:\nNext, we will apply ridge regression to our toy example. Here, we choose the full model as a 4th-order polynomial regression and experiment with different penalty parameters (\\(\\lambda\\)). The fitting results and estimated regression coefficients are shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModels\n\\(\\lambda\\)\n\\(w_0\\)\n\\(w_1\\)\n\\(w_2\\)\n\\(w_3\\)\n\\(w_4\\)\n\n\n\n\nOrange\n0\n-22.438\n44.151\n-25.390\n5.681\n-0.438\n\n\nRed\n0.001\n3.624\n-2.307\n0.195\n0.051\n-0.005\n\n\nBlue\n0.01\n2.931\n-1.463\n0.027\n0.022\n0.002\n\n\nGreen\n10\n0.626\n-0.083\n-0.007\n0.000\n0.000\n\n\n\n\nThe table summarizes the results of a ridge regression experiment, illustrating the relationship between the penalty parameter \\(\\lambda\\) and model complexity. As \\(\\lambda\\) increases, the penalty for larger parameter values grows stronger, resulting in smaller coefficients for all parameters (\\(w_0, w_1, w_2, w_3, w_4\\)).\nFor example, with \\(\\lambda = 0\\) (orange model), the model has no penalty, leading to large parameter values and a highly complex model. As \\(\\lambda\\) increases to \\(0.001\\) (red model) and \\(0.01\\) (blue model ), the parameters shrink, indicating a reduction in model complexity. When \\(\\lambda = 10\\) (green model), most parameter values approach zero, yielding a very simple model.\nThis experiment demonstrates that ridge regression effectively controls model complexity through the hyper-parameter \\(\\lambda\\), where larger \\(\\lambda\\) values correspond to simpler models with lower complexity.\nWe can also observe that as \\(\\lambda\\) increases, the estimated regression coefficients continue to shrink. This is why the ridge regression model is also referred to as a shrinkage method. It is primarily used as a robust regression model to address over fitting issues.\nIf we wish to use it as a feature selection tool, an additional threshold value needs to be set. For instance, feature variables corresponding to regression coefficients smaller than \\(0.01\\) can be excluded. For the purpose of feature selection, there are other regularization methods available, such as the LASSO, which we will discuss next.\n\n\n\n3.3.3 LASSO\nThe LASSO (Least Absolute Shrinkage and Selection Operator) is a regularization method that adds an \\(l_1\\)-norm penalty to the loss function, encouraging sparsity in the model coefficients. This unique property enables LASSO to perform both shrinkage and feature selection simultaneously, making it a powerful tool for high-dimensional data analysis.\nCompared to the \\(l_2\\)-norm penalty used in ridge regression, LASSO employs the \\(l_1\\)-norm to calculate the coefficients budget, which is \\[\n  \\sum_{j = 1}^p |w_j| \\leq C\n\\] So, LASSO problem can be expressed as in the budget form, \\[\n  \\begin{matrix}\n    \\min_{\\textbf{w}} & \\sum_{i=1}^n (y_i- w_0+w_1x_i+w_2x_2+\\dots+w_px_p)^2 \\\\\n    s.t. & \\sum_{j = 1}^p |w_j| \\leq C\n  \\end{matrix}\n\\] or the penalty form \\[\n  \\min_{\\textbf{w}} \\left\\{ \\sum_{i=1}^n (y_i- w_0+w_1x_i+w_2x_2+\\dots+w_px_p)^2 + \\lambda \\sum_{j = 1}^p |w_j| \\right\\}\n\\]\nSimilar to Ridge regression, the budget parameter \\(C\\) and penalty parameter \\(\\lambda\\) here are treated as hyper parameters. They carry the same significance as the hyper parameters in Ridge regression. By solving this optimization problem, we can obtain the estimated LASSO parameters.\nIn the laboratory exercises, we will find that, unlike the shrinkage results of ridge regression, the estimates from LASSO are called sparse results, meaning that most regression coefficients are zero, while a few coefficients are non-zero. Therefore LASSO is also called Sparse method. This characteristic of LASSO makes it an important tool for feature selection.\nThe following diagram conceptually explains why LASSO produces sparse results.\n\n\n\n\n\nRidge Regression VS LASSO\n\n\n\n\nThe left and right sides represent the optimization problem for the same regression model. We can see that in this optimization problem, we have two optimization variables, and the ellipse represents the contour plot of the loss function for the regression problem, where the closer to the center, the lower the loss value. Therefore, without considering the pink figure, the dark blue point represents the optimal solution to the optimization problem, which is the least squares solution, i.e., the regression coefficients for the regular regression model.\nHowever, when we consider the constraint, we need to limit the possible points to a specific region, which is the pink area. On the left, the circle represents the feasible region for the two coefficients under the \\(l_2\\) norm constraint, while on the right, the diamond shape represents the feasible region for the two coefficients under the \\(l_1\\) norm constraint. In other words, we can only consider the optimal solution within the pink region. As a result, the optimal solution is the light blue point, which is the point closest to the minimum loss function value within the feasible region.\nFrom the above diagram, it is clear that the geometric characteristics of the two penalty functions determine the nature of their solutions. Under the \\(l_2\\) norm, the solution is closer to the y-axis, indicating that, due to the penalty term, the estimate of \\(w_1\\) undergoes shrinkage toward zero. Under the \\(l_1\\) norm, the solution lies exactly on the y-axis, meaning that, due to the penalty term, \\(w_1\\) becomes exactly zero. This explains why the \\(l_1\\) norm leads to sparse results, while the \\(l_2\\) norm leads to shrinkage results.\n\nPrevious page | Lecture 6 Homepage"
  },
  {
    "objectID": "test_area/template.html",
    "href": "test_area/template.html",
    "title": "template",
    "section": "",
    "text": "Algorithm: ALGORITHM NAME\n\n\nInputs:\n\nSteps:\n\nOutput:"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_0.html",
    "href": "Courses/c_mlwr1_2024/l6/l6_0.html",
    "title": "Lecture 6: Model Validation and Selection",
    "section": "",
    "text": "In this lecture, we focus on model validation and selection problem. Firstly, we will introduce KNN method and use it to interpret the over fitting problem in classification problems. At the same time, we also explained different types of parameters through the discussion of KNN, leading to the model selection problem. Then, we introduced various model validation methods. Finally, we discussed a specific model selection problem, namely feature selection. Here, we will introduce ridge regression and LASSO.\nOutline:\n\n6.1 Ovrfitting in Classification\n6.2 Model Validation and Selection\n6.3 Feature Selection\n\n\nLecture 6 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_1.html",
    "href": "Courses/c_mlwr1_2024/l6/l6_1.html",
    "title": "1. Overfitting in Classification",
    "section": "",
    "text": "In the previous section, we discussed the manifestations and consequences of the overfitting problem in regression. So, what about over fitting in classification problems? Let us first introduce a very intuitive classification algorithm, K Nearest Neighbors (KNN) method, and then use it to explore the over fitting issue in classification."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_1.html#k-nearest-neighbors",
    "href": "Courses/c_mlwr1_2024/l6/l6_1.html#k-nearest-neighbors",
    "title": "1. Overfitting in Classification",
    "section": "1.1 K Nearest Neighbors",
    "text": "1.1 K Nearest Neighbors\nThe k-Nearest Neighbors (KNN) algorithm is a unique classifier that stands apart from linear classifiers. It’s based on one of the most intuitive ideas in machine learning: decisions are influenced by proximity. Unlike complex parametric methods, KNN operates in a way that mirrors human decision-making in everyday life.\nBasic Idea: The fundamental concept of KNN can be likened to how we often follow trends or social cues. Imagine your neighbor buys a new type of lawn-mowing robot. Your spouse, noticing their satisfaction, might also want to buy one. KNN applies a similar principle in classification: it assigns a label to a data point based on the labels of its nearest neighbors in the feature space. This idea of “going with the flow” underpins the simplicity of KNN.\n\n\nAlgorithm: K-Nearest Neighbors Method\n\n\nInputs:\n\n\\(\\textbf{X}\\), \\(n\\times p\\) matrix, containing all \\(p\\) feature variables.\n\\(y\\), \\(n \\times 1\\) array, the target variable.\n\\(x_{new}\\), \\(p \\times 1\\) array, the feature values of a new observation.\n\\(k\\), the number of neighbors.\n\n\nSteps:\n\nCalculate the distance between \\(x_{new}\\) and each sample points in \\(\\textbf{X}\\).\nFind \\(k\\) sample points that have shortest distance to \\(x_{new}\\)\nAssign the label that is most common among the neighbors.\n\n\nOutput: The assigned label.\n\nSee the following demonstration. In this toy example, we have a binary classification problem with 2 feature variables. There are 10 observations in the training set. We apply 3-NN method to classify an arbitrary point in the 2D space according to the training samples.\n\n\n\n\n\n\n\nDemo of KNN method\n\n\n\n\n\n\n\nFormal formulation( NE ): KNN method can be easily generalized to multiple classification problems, \\(M\\) classes. For a multiple classification problem, suppose we have a training set \\(\\left\\{\\textbf{x}_i, y_i \\right\\}_{i = 1}^{N}\\), where \\(y_i\\) takes value in a set of labels \\(\\left\\{1, 2, \\dots, M \\right\\}\\). For a new observation point, \\(\\textbf{x}_{\\text{new}}\\), the posterior probability that the new point belongs to the \\(j\\)th class given the feature values can be evaluated as \\[\n  \\Pr(y=j|\\textbf{x}_{\\text{new}}) = \\frac{1}{K}\\sum_{i \\in \\mathbb{N}_k(\\textbf{x}_{\\text{new}})} \\textbf{1}\\{y_i = j\\}\n\\] where \\(\\mathbb{N}_k(\\textbf{x}_{\\text{new}})\\) denotes the index set of k nearest neighbors of \\(\\textbf{x}_{\\text{new}}\\) in the training set, the indicator function \\(\\textbf{1}\\{y_i = j\\} = 1\\) if \\(y_i = j\\) otherwise \\(0\\), and \\(j = 1,2,\\dots,M\\). After evaluating the posterior probabilities, one can make the decision accordingly.\nDiscussion:\n\nKNN method also can be applied to a regression problem. In a regression scenario, the prediction is the average value of the k nearest neighbor’s values of target variable. \\[\ny|\\textbf{x}_{\\text{new}} = \\frac{1}{K}\\sum_{i \\in \\mathbb{N}_k(\\textbf{x}_{\\text{new}})} y_i\n\\]\nKNN stands out as a special type of classifier for several reasons.\n\nMemory-Based Model: KNN is often called a memory-based model because it requires storing all the training data. Without the training data, predictions cannot be made. This contrasts with classifiers like Gaussian Discriminant Analysis (GDA), where information from the training data is condensed into model parameters, eliminating the need to retain the original dataset. While many memory-based models exist, KNN is among the simplest.\nLazy Learner: Unlike most classifiers that estimate parameters during a training phase, KNN does not perform explicit training. Once the value of \\(K\\) is chosen, the algorithm simply relies on the stored data for prediction, making it a “lazy learner.”\n\n\nDespite its simplicity, KNN is a powerful algorithm in many contexts, particularly when the dataset is small or when a straightforward decision boundary suffices."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_1.html#overfitting-in-classification-problems",
    "href": "Courses/c_mlwr1_2024/l6/l6_1.html#overfitting-in-classification-problems",
    "title": "1. Overfitting in Classification",
    "section": "1.2 Overfitting in Classification Problems",
    "text": "1.2 Overfitting in Classification Problems\nObviously, the parameter \\(K\\) plays a critical role in KNN method. The number of neighbors we consider to make the final decision significantly impacts the model’s performance. Let’s look at the example below.\nThis is a binary classification problem where the true decision boundary is a sine curve. Due to the influence of noise variables, some observations cross the boundary. By using KNN with \\(k=1\\) and \\(k=9\\), we can observe the resulting decision boundaries as shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is evident that the result with \\(k=9\\) is far better than that with \\(k=1\\). When \\(k=9\\), the true decision boundary of the sine curve can be largely identified or approximated by the KNN algorithm. However, when \\(k=1\\), we notice the emergence of many isolated “islands” on both sides of the true decision boundary. Errors occur in these “islands” because we focus excessively on individual observations in the training data, leading to overfitting.\n\nPrevious page | Lecture 6 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_2.html",
    "href": "Courses/c_mlwr1_2024/l6/l6_2.html",
    "title": "2. Model Validation and Selection",
    "section": "",
    "text": "Think of a machine learning model like a car. Cars come in broad categories, such as SUVs, Hatchbacks, Sports cars, etc., each designed for a specific purpose. Within these categories, there are further variations, for instance, an SUV might be a compact crossover, a mid-size, or a full-size SUV, and the specific features like engine size or drivetrain vary between models.\n\n\n\n\nHave you ever bought a car? My experience was that I searched online and picked the cheapest used car.\n\n\n\nSimilarly, machine learning models have different types or families, often referred to as hypotheses. Examples that we have studied so far include linear regression, polynomial regression, Gaussian Discriminant Analysis (GDA), k-Nearest Neighbors (KNN), and so on. Each family represents a broad class of models. However, within each family, the exact form of the model depends on parameters that we choose or estimate. For example, in linear regression, we need to determine the slope and intercept of the line; in polynomial regression model, we need to determine the order of polynomial terms and estimate the regression coefficients; in GDA, we need to determine the assumption of contrivance structure and estimate the weights for each feature, and so on. Among the many parameters, we can further divide them into two categories: model parameters and hyper-parameters.\n\nModel Parameters: These are the parameters that the model learns directly from the data during training. Once the model structure is decided, these parameters can be computed using algorithms. Examples include regression coefficients in linear regression, feature weights in GDA, and so on.\nHyper-parameters: These are parameters that need to be set before the model is trained. They control aspects of the model or the learning process itself. Examples include the degree of the polynomial in polynomial regression or the value of \\(k\\) in KNN. Unlike model parameters, hyper-parameters are not learned from the data directly but require careful tuning through methods like cross-validation.\n\nIn essence, building and training a machine learning model involves selecting a model type, defining its structure through hyper-parameters, and then using data to learn the model parameters. Before we begin training a model using data, we need not only to choose the type of model but also to determine the hyper-parameters that can not be set by the algorithm itself. In other words, determining the optimal values for hyper-parameters is essentially a model selection problem.\n\n\n\n\n\n\n\nThe procedure of tuning hyper-parameters\n\n\n\n\n\n\n\n\nRecall: In previous labs, we used the so-called brute-force method, the grid search method, to estimate the parameters of regression models. Do you sense a similar vibe here? In other words, without those clever algorithms, wouldn’t the coefficients of a regression model also become hyper-parameters?"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_2.html#hyper-parameters",
    "href": "Courses/c_mlwr1_2024/l6/l6_2.html#hyper-parameters",
    "title": "2. Model Validation and Selection",
    "section": "",
    "text": "Think of a machine learning model like a car. Cars come in broad categories, such as SUVs, Hatchbacks, Sports cars, etc., each designed for a specific purpose. Within these categories, there are further variations, for instance, an SUV might be a compact crossover, a mid-size, or a full-size SUV, and the specific features like engine size or drivetrain vary between models.\n\n\n\n\nHave you ever bought a car? My experience was that I searched online and picked the cheapest used car.\n\n\n\nSimilarly, machine learning models have different types or families, often referred to as hypotheses. Examples that we have studied so far include linear regression, polynomial regression, Gaussian Discriminant Analysis (GDA), k-Nearest Neighbors (KNN), and so on. Each family represents a broad class of models. However, within each family, the exact form of the model depends on parameters that we choose or estimate. For example, in linear regression, we need to determine the slope and intercept of the line; in polynomial regression model, we need to determine the order of polynomial terms and estimate the regression coefficients; in GDA, we need to determine the assumption of contrivance structure and estimate the weights for each feature, and so on. Among the many parameters, we can further divide them into two categories: model parameters and hyper-parameters.\n\nModel Parameters: These are the parameters that the model learns directly from the data during training. Once the model structure is decided, these parameters can be computed using algorithms. Examples include regression coefficients in linear regression, feature weights in GDA, and so on.\nHyper-parameters: These are parameters that need to be set before the model is trained. They control aspects of the model or the learning process itself. Examples include the degree of the polynomial in polynomial regression or the value of \\(k\\) in KNN. Unlike model parameters, hyper-parameters are not learned from the data directly but require careful tuning through methods like cross-validation.\n\nIn essence, building and training a machine learning model involves selecting a model type, defining its structure through hyper-parameters, and then using data to learn the model parameters. Before we begin training a model using data, we need not only to choose the type of model but also to determine the hyper-parameters that can not be set by the algorithm itself. In other words, determining the optimal values for hyper-parameters is essentially a model selection problem.\n\n\n\n\n\n\n\nThe procedure of tuning hyper-parameters\n\n\n\n\n\n\n\n\nRecall: In previous labs, we used the so-called brute-force method, the grid search method, to estimate the parameters of regression models. Do you sense a similar vibe here? In other words, without those clever algorithms, wouldn’t the coefficients of a regression model also become hyper-parameters?"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_2.html#model-validation-methods",
    "href": "Courses/c_mlwr1_2024/l6/l6_2.html#model-validation-methods",
    "title": "2. Model Validation and Selection",
    "section": "2.2 Model Validation Methods",
    "text": "2.2 Model Validation Methods\nFrom the above process of tuning hyper-parameters, the most critical step is Step 3, which is how to evaluate the trained model. Of course, choosing an appropriate model performance metric is important, but even more crucial is avoiding the trap of over fitting. So, how can we evaluate a model to avoid over fitting? If you recall the main characteristic of over fitting (excellent performance on the training set but poor performance on the test set), the conclusion becomes clear. The principle is:\n\nPrinciple: Avoid using the training set to evaluate the trained model.\n\nSpecifically, if a sample set is involved in model training, it should not be used for model evaluation. Based on this principle, we have the following methods for evaluating model performance: the validation set approach, cross-validation approach, and bootstrap method.\n\n2.2.1 The Validation Set Approach\nThe validation set approach is a simple method for model evaluation where the dataset is split into two parts: typically, 80% for training the model and 20% for validation to assess its performance. This allows us to test the model on unseen data and estimate its generalization ability. Recall that in the previous lab, we have actually applied this approach to find the best polynomial regression model.\nHowever, this approach has some notable drawbacks. The performance evaluation can be highly sensitive to how the data is split, as different splits may lead to varying results. Additionally, since only a portion of the data is used for training, the model might not fully leverage all available information, potentially reducing its performance.\nTo address these limitations, we can use cross-validation methods, which provide a more robust and reliable estimate of model performance by systematically rotating through different training and validation splits.\n\n\n2.2.2 Cross Validation Methods\nWhile adhering to the principle of model validation, we can adopt a more flexible approach to defining the training set and validation set. To ensure that all samples contribute to both model training and evaluation, we can dynamically adjust these two sets. There are two methods in this category, i.e. leave one out cross validation and k-fold cross validation\n\nLeave One Out Cross Validation (LOOCV) \nIn this method, the validation set is iteratively defined: each iteration holds out one sample as the test set, while the remaining samples are used to train the model. The model is then evaluated by making a prediction on the held-out sample, and the prediction error is recorded. By iteratively recording the prediction errors for all dynamic validation sets, we can aggregate these errors using an appropriate evaluation metric to compute the final cross-validation results.\nBelow is the corresponding pseudo-code and demo for this approach.\nPseudo-code of LOOCV:\n# n: training sample size\ncv_res = numeric(n) # for keeping all the cross validation errors\nfor(i in 1:n){\n  model = Algorithm(dat[-i,]) # 'dat' contains y and x\n  cv_res[i] = dat$y[i] - model(dat$x[i])\n}\nCV_performance = metric(cv_res)\nAnimation-Demo of LOOCV:\n\n\n\n    Replay\n\n\n\n\nThe main drawback of LOOCV is its computational cost, as the model must be trained as many times as there are data points, which can be inefficient for large datasets. Additionally, LOOCV may result in high variance in error estimates due to its reliance on a single data point for validation in each iteration.\n\n\n\nK-fold Cross Validation (kFCV)\nTo address these issues, kFCV offers a more efficient alternative by randomly splitting the dataset into k equally sized folds, allowing multiple samples to be used for validation in each iteration, reducing computational cost and variance. Below is the corresponding pseudo-code and demo for this approach.\nPseudo-code of KFCV:\n# n: training sample size\n# k: number of flods\n# nn = n/k: size of dynamic vliadtion set\n# Step 1: shuffle observation points\nid = sample(1:n)  \nid = matrix(id, nrow = k)\n# Step 2: doing cross validation\ncv_res = numeric(n) # for keeping all the cross validation errors\nfor(i in 1:k){\n  model = Algorithm(dat[-id[k,],]) # 'dat' contains y and x\n  cv_res[ ((i-1)*nn+1) : (i*nn) ] = dat$y[id[k,]] - model(dat$x[id[k,]])\n}\nCV_performance = metric(cv_res)\nAnimation-Demo of KFCV:\n\n\n\n    Replay\n\n\n\n\n\nQuiz: What is the relationship between LOOCV and kFCV?\n\n\n\n2.2.3 Bootstrap Method\nWhile cross-validation splits the dataset into distinct training and validation sets to evaluate model performance, bootstrap takes a different approach by focusing on resampling. Instead of partitioning the data, bootstrap generates multiple training sets by randomly sampling with replacement from the original dataset, allowing some samples to appear multiple times while others are left out. In a statistical terminology, the prepared temporary training set is refereed to bootstrap sample.\nBelow is the corresponding pseudo-code and demo for this approach.\nPseudo-code of Bootstrap:\n# n: training sample size\n# B: number of bootstrap samples (temporary training set)\nbt_res = numeric(n*B, B, n) # for keeping all the bootstrap errors\nfor(i in 1:B){\n  id = sample(1:n, n, replace = T)\n  model = Algorithm(dat[-id,]) # 'dat' contains y and x\n  cv_res[i,] = dat$y[id] - model(dat$x[id])\n}\nCV_performance = metric(cv_res)\nAnimation-Demo of Bootstrap:\n\n\n\n    Replay\n\n\n\n\nThis approach is particularly useful in small datasets where partitioning into training and validation sets could lead to a loss of valuable data for training. In addition, it is worth mentioning that the bootstrap algorithm not only provides a method for model validation but also offers an idea for creating nonlinear models. The famous random forest model is based on the bootstrap algorithm. We will revisit this topic in the second part of this course."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_2.html#standard-procedure",
    "href": "Courses/c_mlwr1_2024/l6/l6_2.html#standard-procedure",
    "title": "2. Model Validation and Selection",
    "section": "2.3 Standard Procedure",
    "text": "2.3 Standard Procedure\nThe standard procedure in machine learning involves four main steps:\n\nSplit the data into three sets: training, validation, and testing.\nTune hyper-parameters using the training and validation sets.\nTrain the final model using all available data from the training and validation sets.\nEvaluate the model on the testing set to estimate its generalization performance.\n\nDiscussion:\n\nThe boundary between the training and validation sets can be vague, as it depends on the specific validation method used (e.g., k-fold, bootstrap, etc.).\n\nThe testing set is primarily used to estimate the model’s performance on unseen data. If you’re only interested in selecting the best model and don’t care about performance on a completely new data set, the testing set can be ignored.\n\n\nPrevious page | Lecture 6 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_home.html",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_home.html",
    "title": "Lab 4: Exercises about Model Validation and Selection",
    "section": "",
    "text": "we implement and practice k-nearest neighbors method and subset selection methods with Breast cancer data and simulated data."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-1.1",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-1.1",
    "title": "Lab 4: Exercises about Model Validation and Selection",
    "section": "Task 1.1:",
    "text": "Task 1.1:\nRead the help document of the function knn in library class first. This exercise aims to analyze how the choice of \\(k\\) affects the KNN method’s performance on training and test datasets and identify any patterns in overfitting or underfitting as \\(k\\) varies. Follow the instructions below to complete your experiment, and then draw your conclusions based on the experimental results.\n# Step 1: Define the candidates of hyper-parameter k\nK = seq(1,300,2)\nnum_k = length(K)\nres_tr = res_te = numeric(num_k) # to keep performance of KNN in traing set and test set\n\n# Step 2: For each k, store the model performance both in training set and test set\nfor(i in 1:num_k){\n  # Predict the target variable for each case in training set by KNN with K[i]. \n  # Store the accuracy in `res_tr`\n  # Predict the target variable for each case in test set by KNN with K[i].\n  # Store the accuracy in `res_te`\n}\n\n# Step 3: Visualize the results"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-2.1-practive-leaps-package",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-2.1-practive-leaps-package",
    "title": "Lab 4: Exercises about Model Validation and Selection",
    "section": "Task 2.1: Practive leaps package",
    "text": "Task 2.1: Practive leaps package\nRead the help document of function regsubsets. Apply this function to do best subset selection on the simulated data, dat. Print out the names of variable selected in the candidate model with 8 feature variables and their model coefficients.\nTips: Two options to get the regression coefficients:\n\nYou can find the included variables of the candidate model with 8 feature variables by the following code.\n\nsummary(m)$which # m: the output of `regsubsets`\n\nUse function coef to extract the selected variables. You need to specify two arguments, the output of regsubsets function and the id of target candidate model."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-2.2-prediction-with-candidate-models",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-2.2-prediction-with-candidate-models",
    "title": "Lab 4: Exercises about Model Validation and Selection",
    "section": "Task 2.2: Prediction with candidate models",
    "text": "Task 2.2: Prediction with candidate models\nSet random seed as 2024. Randomly split the data set into training and testing sets (80% VS 20%). Use the same method as task 2.1 to find all the candidate models. Apply the candidate model with 8 feature variables to the testing set and estimate the RMSE.\nTips: Function model.matrix can be used for preparing the data matrix for prediction, e.g.,\nx_test = model.matrix(y~., dat_te)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-2.3-select-the-final-optimal-model",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-2.3-select-the-final-optimal-model",
    "title": "Lab 4: Exercises about Model Validation and Selection",
    "section": "Task 2.3: Select the final optimal model",
    "text": "Task 2.3: Select the final optimal model\nWith the same data spiting, find the optimal model by training-against-validation method. Estimate the model performance with the testing set."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-2.4-optinal-practice-glmnet-package",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-2.4-optinal-practice-glmnet-package",
    "title": "Lab 4: Exercises about Model Validation and Selection",
    "section": "Task 2.4 (optinal): Practice glmnet package",
    "text": "Task 2.4 (optinal): Practice glmnet package\nRead the help document of cv.glmnet function in package glmnet first. Apply LASSO method to select the feature variables in dataset dat.\nNote: If you are short on time, you can skip this task for now. We will focus on practicing this package in the lab next week.\n\nLecture 6 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-1.2-practice-loocv",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-1.2-practice-loocv",
    "title": "Lab 4: Exercises about Model Validation and Selection",
    "section": "Task 1.2: Practice LOOCV",
    "text": "Task 1.2: Practice LOOCV\nRead the help document of the function knn.cv in library class first. Apply LOOCV to select the best hyper-parameter \\(k\\) and test the model performance in the testing set.\nTips: You can apply function knn.cv to perform LOOCV to select the best hyper-parameter. The suggested instruction:\n# Step 1: Define the candidates of hyper-parameter k\nK = seq(1,300,2)\nnum_k = length(K)\ncv_acc = numeric(num_k)\n# Step 2: \nfor(i in 1:num_k){\n  # apply function `knn.cv` to calculate cross validation accuracy,\n  # and store it in `cv_acc`.\n}\n# Step 3: Find the model with highest cross validation accuracy and estimate its performance in testing set."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-1.3-practice-5-fold-cross-validation",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-1.3-practice-5-fold-cross-validation",
    "title": "Lab 4: Exercises about Model Validation and Selection",
    "section": "Task 1.3: Practice 5-fold Cross Validation",
    "text": "Task 1.3: Practice 5-fold Cross Validation\nTune the hyper-parameter by 5-fold cross validation. Estimate the model performance of the optimal model with testing set.\nNote: Function knn.cv only provide LOOCV option. So, you need to implement the 5-fold CV by your own code."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-1.1-practice-knn-method",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_home.html#task-1.1-practice-knn-method",
    "title": "Lab 4: Exercises about Model Validation and Selection",
    "section": "Task 1.1: Practice KNN method",
    "text": "Task 1.1: Practice KNN method\nRead the help document of the function knn in library class first. This exercise aims to analyze how the choice of \\(k\\) affects the KNN method’s performance on training and test datasets and identify any patterns in overfitting or underfitting as \\(k\\) varies. Follow the instructions below to complete your experiment, and then draw your conclusions based on the experimental results.\n# Step 1: Define the candidates of hyper-parameter k\nK = seq(1,300,2)\nnum_k = length(K)\nres_tr = res_te = numeric(num_k) # to keep performance of KNN in traing set and test set\n\n# Step 2: For each k, store the model performance both in training set and test set\nfor(i in 1:num_k){\n  # Predict the target variable for each case in training set by KNN with K[i]. \n  # Store the accuracy in `res_tr`\n  # Predict the target variable for each case in test set by KNN with K[i].\n  # Store the accuracy in `res_te`\n}\n\n# Step 3: Visualize the results"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html",
    "title": "Solutions to Exercises in Lab 4",
    "section": "",
    "text": "we implement and practice k-nearest neighbors method and subset selection methods with Breast cancer data and simulated data."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html#task-1.1-practice-knn-method",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html#task-1.1-practice-knn-method",
    "title": "Solutions to Exercises in Lab 4",
    "section": "Task 1.1: Practice KNN method",
    "text": "Task 1.1: Practice KNN method\nRead the help document of the function knn in library class first. This exercise aims to analyze how the choice of \\(k\\) affects the KNN method’s performance on training and test datasets and identify any patterns in overfitting or underfitting as \\(k\\) varies. Follow the instructions below to complete your experiment, and then draw your conclusions based on the experimental results.\n Solutions:\n Step 1:\n\nK = seq(1,300,2)\nnum_k = length(K)\nres_tr = res_te = numeric(num_k)\n\n Step 2:\n\nlibrary(class)\nfor(i in 1:num_k){\n  y_tr_pre = knn(dat_tr[,-1], dat_tr[,-1], dat_tr[,1], K[i])\n  res_tr[i] = mean(y_tr_pre == dat_tr[,1])\n  y_te_pre = knn(dat_tr[,-1], dat_te[,-1], dat_tr[,1], K[i])\n  res_te[i] =mean(y_te_pre == dat_te[,1])  \n}\n\n Step 3:\n\nplot(1:num_k, res_tr, type = \"b\", col = \"red\", pch = 20, cex = 0.5,\n     ylab = \"Accuracy\", xlab = \"k\", ylim = range(c(res_tr, res_te)))\npoints(1:num_k, res_te, type = \"b\", col = \"blue\", pch = 20, cex = 0.5)"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html#task-1.2-practice-loocv",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html#task-1.2-practice-loocv",
    "title": "Solutions to Exercises in Lab 4",
    "section": "Task 1.2: Practice LOOCV",
    "text": "Task 1.2: Practice LOOCV\nRead the help document of the function knn.cv in library class first. Apply LOOCV to select the best hyper-parameter \\(k\\) and test the model performance in the testing set.\nTips: You can apply function knn.cv to perform LOOCV to select the best hyper-parameter. The suggested instruction:\n Step 1:\n\n# define all the candidate values\nK = seq(1,300,2)\nnum_k = length(K)\n# creat a vector 'cv_acc' to save all the cross validation results for all candidate models\ncv_acc = numeric(num_k) \n\n Step 2:\n\nfor(i in 1:num_k){\n  # loop over all candidate models\n  y_pre = knn.cv(train = dat_tr[,-1], cl = dat_tr[,1], k = K[i]) # get the prediction by leave one out cross validation\n  cv_acc[i] = mean(dat_tr[,1] == y_pre) # calculate the cv accuracy for the ith candidate model\n}\nplot(1:num_k, cv_acc, type=\"b\", pch = 20, cex = 0.5, xlab = \"k\")\n\n\n\n\n\n\n\n\n Step 3:\n\nopt_k = which(cv_acc==max(cv_acc)) # find the optimal K\nK[opt_k]\n\n[1] 27\n\n# Estimate the model performance with testing data\ny_pre = knn(train = dat_tr[,-1], test = dat_te[,-1], cl = dat_tr[,1], k = K[opt_k])\nmean(dat_te[,1] == y_pre)\n\n[1] 0.877193"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html#task-1.3-practice-5-fold-cross-validation",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html#task-1.3-practice-5-fold-cross-validation",
    "title": "Solutions to Exercises in Lab 4",
    "section": "Task 1.3: Practice 5-fold Cross Validation",
    "text": "Task 1.3: Practice 5-fold Cross Validation\nTune the hyper-parameter by 5-fold cross validation. Estimate the model performance of the optimal model with testing set.\nNote: Function knn.cv only provide LOOCV option. So, you need to implement the 5-fold CV by your own code.\n Solutions:\n\nK = seq(1,300,2)\nnum_k = length(K)\nset.seed(8312)\ncv_k_acc = numeric(num_k)\nID = matrix(sample(1:445), nrow = 5) # randomly split the sample into k folds\n# each row of 'ID' contains all the id of observations in the corresponding fold\n\n Solutions: Since each row in ID contains the index of one fold, we can write a for loop to implement 5-fold validation.\n\nfor(i in 1:num_k){\n  temp_res = numeric(5)\n  for(j in 1:5){\n    y_pre = knn(train = dat_tr[-ID[j,],-1], # pick up the feature variables of observations not in the jth fold\n                test = dat_tr[ID[j,],-1], # predict on the observations in the jth fold\n                cl = dat_tr[-ID[j,],1], # target variable of observations not in the jth fold\n                k = K[i])\n    temp_res[j] = mean(y_pre == dat_tr[ID[j,],1]) # accuracy of the current model for the jth fold\n  }\n  cv_k_acc[i] = mean(temp_res) # overall cv accuracy for the jth fold\n}\nplot(1:num_k, cv_k_acc, type=\"b\", pch = 20, cex = 0.5)\n\n\n\n\n\n\n\n\n Solutions: Find the optimal model selected by 5-fold CV and estimate the model performance.\n\nopt_k = which(cv_k_acc==max(cv_k_acc)) # find the optimal K\nK[opt_k]\n\n[1] 17\n\n# Estimate the model performance with testing data\ny_pre = knn(train = dat_tr[,-1], test = dat_te[,-1], cl = dat_tr[,1], k = K[opt_k])\nmean(dat_te[,1] == y_pre)\n\n[1] 0.8684211"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html#task-2.1-practive-leaps-package",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html#task-2.1-practive-leaps-package",
    "title": "Solutions to Exercises in Lab 4",
    "section": "Task 2.1: Practive leaps package",
    "text": "Task 2.1: Practive leaps package\nRead the help document of function regsubsets. Apply this function to do best subset selection on the simulated data, dat. Print out the names of variable selected in the candidate model with 8 feature variables and their model coefficients.\nTips: Two options to get the regression coefficients:\n\nYou can find the included variables of the candidate model with 8 feature variables by the following code.\n\nsummary(m)$which # m: the output of `regsubsets`\n\nUse function coef to extract the selected variables. You need to specify two arguments, the output of regsubsets function and the id of target candidate model.\n\n\nlibrary(leaps)\nm1 = regsubsets(y~., dat, nvmax = 10) # if you want to do forward/backward selection, then you need to add 'method' in the function\n# summary(m1) \n\n\nsummary(m1)$which\n\n   (Intercept)    X1   X2    X3    X4    X5    X6    X7    X8    X9   X10   X11\n1         TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2         TRUE FALSE TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n3         TRUE  TRUE TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n4         TRUE  TRUE TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n5         TRUE  TRUE TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n6         TRUE  TRUE TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n7         TRUE  TRUE TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n8         TRUE  TRUE TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n9         TRUE  TRUE TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n10        TRUE  TRUE TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n     X12   X13   X14   X15   X16   X17   X18   X19   X20\n1  FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2  FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n3  FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n4  FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n5  FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n6  FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n7   TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n8   TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n9   TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n10  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n\n\n\n# solution 1\nres = summary(m1)$which[8,-1]\ntemp_dat = dat[, res]\ntemp_dat$y = dat$y\nm_temp = lm( y~., temp_dat )\nm_temp$coefficients\n\n(Intercept)          X1          X2          X3          X4          X5 \n-0.04070300  1.98697488 -3.11149188  0.96235490  2.34621090  1.42697443 \n        X12         X18         X19 \n 0.06994639 -0.08102189  0.05768604 \n\n\n\n# solution 2\ncoef(m1, id=8) # find all the variables and coefficients in the optimal model with 8 features \n\n(Intercept)          X1          X2          X3          X4          X5 \n-0.04070300  1.98697488 -3.11149188  0.96235490  2.34621090  1.42697443 \n        X12         X18         X19 \n 0.06994639 -0.08102189  0.05768604"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html#task-2.2-prediction-with-candidate-models",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html#task-2.2-prediction-with-candidate-models",
    "title": "Solutions to Exercises in Lab 4",
    "section": "Task 2.2: Prediction with candidate models",
    "text": "Task 2.2: Prediction with candidate models\nSet random seed as 2024. Randomly split the data set into training and testing sets (80% VS 20%). Use the same method as task 2.1 to find all the candidate models. Apply the candidate model with 8 feature variables to the testing set and estimate the RMSE.\nTips: Function model.matrix can be used for preparing the data matrix for prediction, e.g.,\nx_test = model.matrix(y~., dat_te)\n\nset.seed(2024)\nid = sample(1:dim(dat)[1], dim(dat)[1]*0.8)\ndat_tr = dat[id,]\ndat_te = dat[-id,]\n\nm1 = regsubsets(y~., dat_tr, nvmax = 10)\nx_test = model.matrix(y~., dat_te) # function for preparing prediction matrix for the testing set\ncoe = coef(m1, id = 7)\npred = x_test[, names(coe)]%*%coe # prediction. you also can implement this step by for loop\nrmse = sqrt(mean((dat_te$y - pred)^2))\nrmse\n\n[1] 1.205906"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html#task-2.3-select-the-final-optimal-model",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html#task-2.3-select-the-final-optimal-model",
    "title": "Solutions to Exercises in Lab 4",
    "section": "Task 2.3: Select the final optimal model",
    "text": "Task 2.3: Select the final optimal model\nWith the same data spiting, find the optimal model by training-against-validation method. Estimate the model performance with the testing set.\n\nid_train = sample(id, length(id)*0.8)\ndat_training = dat_tr[id_train, ] \ndat_validating = dat_tr[-id_train, ] # further split training set as training and validation set\n\n# find the best model \nm1 = regsubsets(y~., dat_training, nvmax = 10) \nx_val = model.matrix(y~., dat_validating)\n\nres = numeric(10)\nfor(i in 1:10){\n  coe = coef(m1, id = i)\n  pred = x_val[, names(coe)]%*%coe\n  res[i] = sqrt(mean((dat_validating$y - pred)^2))\n}\nplot(res)\n\n\n\n\n\n\n\n# we choose the model with 10 feature variables\n# train the model with 10 feature variables and evaluate the model performance with the testing set.\nopt = which(res == min(res))\nopt\n\n[1] 5\n\ncoef(m1, id = opt)\n\n(Intercept)          X1          X2          X3          X4          X5 \n-0.07504814  1.91831167 -3.13194720  0.92864807  2.31970426  1.44105002 \n\nres = summary(m1)$which[8,-1]\ntemp_dat = dat_tr[, res]\ntemp_dat$y = dat_tr$y\n\nfinal_m = lm(y~., data = temp_dat)\npred = predict(final_m, dat_te)\nmean((dat_te$y - pred)^2) # it the estimate model performance\n\n[1] 1.433956"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html#task-2.4-optinal-practice-glmnet-package",
    "href": "Courses/c_mlwr1_2024/l6/l6_lab_solutions.html#task-2.4-optinal-practice-glmnet-package",
    "title": "Solutions to Exercises in Lab 4",
    "section": "Task 2.4 (optinal): Practice glmnet package",
    "text": "Task 2.4 (optinal): Practice glmnet package\nRead the help document of cv.glmnet function in package glmnet first. Apply LASSO method to select the feature variables in dataset dat.\nNote: If you are short on time, you can skip this task for now. We will focus on practicing this package in the lab next week.\n Solutions:\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\nset.seed(2024)\nmodel = cv.glmnet( x = as.matrix(dat_tr[,-21]), y = dat_tr[,21],\n                   nfolds = 10, alpha = 1, type.measure = \"mse\")\nplot(model)\n\n\n\n\n\n\n\n\n This figure illustrates the cross-validation results. The x-axis represents the logarithm of the penalty parameter, the y-axis shows the cross-validation MSE, and the top of the image indicates the number of variables included in the corresponding model. The first dashed line marks the model with the smallest cross-validation MSE, while the second dashed line indicates the model whose performance is within one standard deviation of the minimum MSE. In other words, the performance of the model marked by the second dashed line is very close to that of the model with the minimum MSE. However, in practice, we tend to prefer simpler models, which is why the model marked by the second dashed line is often selected.\n\nmodel$lambda.min\n\n[1] 0.03378175\n\nmodel$lambda.1se\n\n[1] 0.1363775\n\nwhich(coef(model, s = \"lambda.min\")!=0)\n\n [1]  1  2  3  4  5  6  7  8  9 10 13 16 17 18 19 20\n\nwhich(coef(model, s = \"lambda.1se\")!=0)\n\n[1] 1 2 3 4 5 6\n\n\n Above, the values of the penalty parameter and the corresponding indices of the selected variables for both the mse-min and mse-1se models are displayed.\n\nLecture 6 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7.html",
    "href": "Courses/c_mlwr1_2024/l7/l7.html",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "",
    "text": "In this lecture, we will learn about a very important classifier in machine learning, the logistic regression classifier. First, we will discuss intuitively how classification can be performed using a linear regression model and point out its critical weakness. Then, we will introduce logistic regression to address this weakness, discussing its model construction and how to use it as a classifier. Finally, we will discuss the training of the logistic regression model. By understanding its loss function, we will easily grasp how to use regularization methods in logistic regression."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7.html#intuitive-idea",
    "href": "Courses/c_mlwr1_2024/l7/l7.html#intuitive-idea",
    "title": "Lecture 7: Logistic Regression",
    "section": "2.1 Intuitive Idea",
    "text": "2.1 Intuitive Idea"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7.html#decision-boundary",
    "href": "Courses/c_mlwr1_2024/l7/l7.html#decision-boundary",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "2.4 Decision Boundary",
    "text": "2.4 Decision Boundary\nIn the field of statistics, logistic regression is typically considered a type of generalized linear regression model. Generalized linear models (GLMs) form a large family of models, which includes common distributions for the response variable, such as Poisson regression, multinomial regression, beta regression, and so on. Logistic regression is also often referred to as a nonlinear regression model because, in the end, it projects the feature variables onto a nonlinear surface, as shown in the figure below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n LHS: Multivariate linear regression model. RHS: Logistic regression model, the surface is \\(\\phi(w_0 + w_1X_1+ w_2X2)\\), where \\(\\phi\\) is the logistic function. \n\n\nQuestion: If logistic regression is considered a nonlinear statistical model, is it a nonlinear classifier?\n\n\nR example:\nLet’s address the previous example with outliers. To solve this problem, we can kill three birds with one stone: learn an R example, validate the robustness of logistic regression, and finally, experimentally obtain the answer to the above question. The data set can be downloaded here.\nI show you the dataset for the problem we discussed before and visualize it in a scatter plot.\n\nhead(dat)\n\n     x1    x2 y\n1 1.419 2.280 1\n2 0.553 3.838 1\n3 1.704 2.422 1\n4 1.331 3.096 1\n5 1.660 1.397 1\n6 2.434 1.712 1\n\n\n\ncolor_obs = ifelse(dat$y==1, \"blue\", \"red\")\npar(mar = rep(1,4))\nplot(dat$x1, dat$x2, col = color_obs, pch = 20, \n     cex = 2, axes = F, xlab=\"\", ylab = \"\")\n\n\n\n\n\n\n\n\nLet’s first demonstrate how to obtain the classification boundary for the linear regression model. By the way, the answer to the previous quiz is the linear regression model. That is, if we set the family argument to Gaussian, we will get a linear regression model. In other words, its output is the same as the output from the lm function. Now, let’s use it to calculate the model parameters.\n\nm_linReg = glm(y~., data = dat, family = \"gaussian\")\nsummary(m_linReg)\n\n\nCall:\nglm(formula = y ~ ., family = \"gaussian\", data = dat)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.08317    0.06386  16.962  &lt; 2e-16 ***\nx1          -0.03156    0.01510  -2.091   0.0392 *  \nx2          -0.10966    0.02241  -4.894 3.94e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1108737)\n\n    Null deviance: 25.000  on 99  degrees of freedom\nResidual deviance: 10.755  on 97  degrees of freedom\nAIC: 68.805\n\nNumber of Fisher Scoring iterations: 2\n\n\nNext, let’s render the classification boundary of the classifier based on this model. Similar to how we rendered the classification boundary for k-NN previously, we will first generate a grid for the 2D feature space. Then, we will use our model to classify each grid point and present the decision boundary.\n\nx1 = seq(-1,16,0.1)\nx2 = seq(0,14,0.1) \nd = expand.grid(x1 = x1, x2 = x2)\nclass(d)\n\n[1] \"data.frame\"\n\n\n\nscores = predict(m_linReg, d)\ncolor_test = ifelse(scores &gt; 0.5, \"#D6E8FF\", \"#FFD6D6\")\n\n\npar(mar = rep(1,4))\nplot(dat$x1, dat$x2, col = color_obs, pch = 20, \n     cex = 2, axes = F, xlab=\"\", ylab = \"\")\npoints(d$x1, d$x2, col = color_test, pch = 20, cex = 0.5)\npoints(dat$x1, dat$x2, col = color_obs, pch = 20, cex = 2)\n\n\n\n\n\n\n\n\nAs we observed earlier, the classifier based on the linear regression model is very sensitive to outliers. Even in such a simple classification problem, it makes errors in certain areas. Now, let’s take a look at the performance of the classifier based on the logistic regression model.\n\nm_LogReg = glm(y~., data = dat, family = binomial())\n\n\nres_pred = predict(m_LogReg, d, type = \"response\")\nrange(res_pred)\n\n[1] 2.220446e-16 1.000000e+00\n\n\nHere, we use the predict function to predict the label for each grid point. Note that in the function above, we set the type argument to ‘response’, so we will obtain the posterior probability for each grid point based on its coordinates. Then, we can classify the points using the standard cutoff of 0.5. Finally, we use this grid to present the classification boundary of the classifier based on the logistic regression model.\n\ncolor_test = ifelse(res_pred &gt; 0.5, \"#D6E8FF\", \"#FFD6D6\")\n\n\npar(mar = rep(1,4))\nplot(dat$x1, dat$x2, col = color_obs, pch = 20, \n     cex = 2, axes = F, xlab=\"\", ylab = \"\")\npoints(d$x1, d$x2, col = color_test, pch = 20, cex = 0.5)\npoints(dat$x1, dat$x2, col = color_obs, pch = 20, cex = 2)\n\n\n\n\n\n\n\n\nAs we can see, using logistic regression as a classifier allows us to correct the impact of outliers. Additionally, we can observe that the classification boundary of the logistic regression classifier is also a straight line, meaning it is a linear classifier. In fact, this conclusion is not difficult to reach. From the principle of the classifier, the classification boundary of the logistic regression classifier is: \\[\n  \\Pr(Y = 1|\\textbf{x}) = \\phi(\\text{scores}) = \\frac{1}{1 + e^{-(w_0 + w_1X_1 + \\dots + w_pX_p)}} = \\frac{1}{2}\n\\] If we simplify this formula, we can derive the classification boundary of the logistic regression classifier \\[\n  w_0 + w_1X_1 + \\dots + w_pX_p = 0\n\\] So, the classifier based on logistic regression is a linear classifier."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7.html#likelihood-function-and-cross-entropy-loss",
    "href": "Courses/c_mlwr1_2024/l7/l7.html#likelihood-function-and-cross-entropy-loss",
    "title": "Lecture 7: Logistic Regression",
    "section": "3.1 Likelihood function and Cross-entropy Loss",
    "text": "3.1 Likelihood function and Cross-entropy Loss\nSuppose we have a set of training observations, \\(\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i = 1}^n\\). The distribution of the target variable is Binary distribution, i.e.  \\[\n  \\Pr( y_i, \\pi(\\textbf{x}_i, \\textbf{w}) ) = \\pi(\\textbf{x}_i, \\textbf{w}) ^{y_i} (1-\\pi(\\textbf{x}_i, \\textbf{w}))^{1 - y_i}  \n\\] where \\(y_i = 1 \\text{ or } 0\\). Since we have independent observations, the joint likelihood of the training sample is \\[\n  L(\\textbf{w}; \\textbf{y}, \\textbf{X} ) = \\prod_{i = 1}^{n} \\pi(\\textbf{x}_i, \\textbf{w}) ^{y_i} (1-\\pi(\\textbf{x}_i, \\textbf{w}))^{1 - y_i}\n\\] The log-likelihood function is \\[\n  \\ell(\\textbf{w}; \\textbf{y}, \\textbf{X}) = \\sum_{i = 1}^n \\left\\{ y_i\\log( \\pi(\\textbf{x}_i, \\textbf{w}) ) + (1-y_i)\\log(1-\\pi(\\textbf{x}_i, \\textbf{w})) \\right\\}\n\\]"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7.html#grident-descent-algorithm",
    "href": "Courses/c_mlwr1_2024/l7/l7.html#grident-descent-algorithm",
    "title": "Lecture 7: Logistic Regression",
    "section": "3.2 Grident Descent Algorithm",
    "text": "3.2 Grident Descent Algorithm\n\n\nLecture 7 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7.html#penalized-logistic-regression",
    "href": "Courses/c_mlwr1_2024/l7/l7.html#penalized-logistic-regression",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "3.2 Penalized Logistic Regression",
    "text": "3.2 Penalized Logistic Regression\nIn the previous lecture, we discussed the shrinkage and sparse versions of the regression model. Through these, we can both avoid the risk of overfitting and indirectly obtain feature selection results. For classification problems, we have similar tools available, that is penalized logistic regression.\nLet’s first recall the idea of penalized regression. We define a set of candidate models by adding the calculation of the budget for the model parameter values, i.e., \\[\n  \\textcolor[rgb]{1.00,0.00,0.00}{\\text{Candidate Models}} = \\textcolor[rgb]{1.00,0.50,0.00}{\\text{Full Model}} + \\textbf{Budget}(\\textbf{w}).\n\\] With the general form of all candidate models, the penalized regression problem can be formulated as \\[\n  \\min_{\\textbf{w}} \\left\\{ \\mathcal{L}_{mse}\\left( \\textbf{w}; \\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N \\right) + \\lambda\\textbf{Budget}(\\textbf{w})  \\right\\}\n\\] where the mse loss is just the sum squared residuals, and the budget term can be \\(L_1\\) norm, i.e. LASSO, or \\(L_2\\) norm, i.e. ridge regression.\nNow, let’s return to the logistic regression model. The clever among you might have already realized that the difference between penalized logistic regression and the previous penalized regression is simply the choice of the loss function. If we replace the MSE loss in the above formula with the cross-entropy loss, we obtain the optimization problem for penalized logistic regression, and the optimal solution is the penalized logistic regression model parameters.\nSimilarly, if we choose the \\(L_2\\) norm, we will get a shrinkage solution, whereas the \\(L_1\\) norm will provide us with a sparse solution and serve as an important tool for feature selection in classification problems. In addition, we will encounter many variations of penalty terms, such as the Elastic net penalty, \\[\n  \\alpha \\times \\sum_{j = 1}^p |w_j|  + (1- \\alpha) \\times \\sum_{j = 1}^p w_j^2\n\\] where \\(\\alpha\\) is an extra hyper-parameter taking value in \\([0,1]\\). From the above formula, it is easy to see that the calculation of the parameter value budget in elastic net is intermediate between ridge regression and LASSO. If the parameter \\(\\alpha\\) is set to 1, the elastic net degenerates into the LASSO penalty. Conversely, if \\(\\alpha\\) is set to 0, we get the \\(L_2\\) penalty, which corresponds to ridge regression. When \\(\\alpha\\) takes any value between 0 and 1, we obtain the elastic net. In other words, the elastic net is a convex combination of the \\(L_1\\) penalty and the \\(L_2\\) penalty. This setup makes the corresponding candidate models more flexible. Of course, the trade-off is that we need to consider an additional hyperparameter. Alright, let’s stop here for now. We will explain the implementation of penalized logistic regression in more detail in the upcoming labs."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7.html#exploration",
    "href": "Courses/c_mlwr1_2024/l7/l7.html#exploration",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "2.1 Exploration",
    "text": "2.1 Exploration\nLet’s go back to our previous discussion: what is the main issue with using linear regression models for classification problems? First, we need to clarify one point: what is the essence of the linear regression model? Recall the discussion of maximum likelihood estimation of linear regression model. The essence of the linear regression model is to predict the expected value of the target variable using a linear combination of the feature variables, i.e. \\[\n  \\text{E}(y | \\textbf{x}) = w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p\n\\] This is quite normal for a continuous target variable. However, when we try to extend this model to classification problems, we encounter new issues.\nEssentially, we are trying to model the expected value of target variable as the weighted sum of feature variables, when we apply linear regression model to a classification problem. However, in classification problems, the target variable is a categorical variable. For categorical variables, the assumption of a normal distribution is completely unreasonable. We need to use discrete distributions to characterize the distribution of these variables, for example, using binary distribution for a binary classification problem, i.e. \\(y \\sim \\text{Ber}(\\pi)\\), \\[\n  y = \\left\\{\n    \\begin{matrix}\n      1  & \\text{Positive case} \\\\\n      0 & \\text{Negative case}\n    \\end{matrix}\n  \\right.\n\\] What is the expected value of a binary distributed random variable? It is the probability of \\(y = 1\\), i.e. \\(\\text{E}(y) = \\pi\\). So, we are using the following equation \\[\n  \\pi|\\textbf{x} = \\text{E}(y|\\textbf{x}) = w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p\n\\] If we analyze the range of values on both sides of the equation, it is not difficult to identify the critical flaw in using linear regression to handle classification problems. The left side of the equation represents a probability value, with a range of \\([0, 1]\\), while the right side can take any real number. Clearly, fitting a probability with an arbitrary real number is unreasonable and this is where the problem lies."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7.html#model-construction",
    "href": "Courses/c_mlwr1_2024/l7/l7.html#model-construction",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "2.2 Model Construction",
    "text": "2.2 Model Construction\nTo address the issue mentioned above, mathematicians proposed an idea: using a function to transform values that span the entire real number axis into a fraction. This transformation function is the well-known logistic function, \\[\n  \\phi(t) = \\frac{1}{1+e^{-t}}\n\\] The graph of the logistic function is shown in the figure below. We can see that through its transformation, the output for any input \\(x\\) lies between the two red dashed lines, meaning it is a fraction.\n\n\n\n\n\n\n\n\n\n\n\nWith the help of the logistic function, we can start building our logistic regression model.\nFor a binary classification problem, we assume that the target variable follows a binary distribution, where the probability of a positive case is calculated as the logistic transformed weighted sum of the feature variables plus a constant. In a formal mathematical language, logistic regression model presents a probability model, \\(y\\sim \\text{Ber}\\left( \\pi(\\textbf{x}; \\textbf{w}) \\right)\\), where \\[\n  \\pi(\\textbf{x}; \\textbf{w}) = \\phi(w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p) = \\frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p)}}\n\\] As can be seen from the above model, the logistic regression model essentially provides us with a mechanism for calculating the posterior probability of the target variable, i.e. \\[\n  \\Pr\\left( Y = 1 | \\textbf{x} \\right) =  \\frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p)}}\n\\] In other words, once we have obtained all the parameters \\(\\textbf{w}\\), we can evaluate the posterior probability using the values of the feature variables. If you still remember the classification rule of the GDA classifier, \\[\n  \\widehat{y} = \\arg \\max_{y} \\Pr(y| \\textbf{x})\n\\] then it is natural that the logistic regression model can be used as a classifier. Simply speaking, if the posterior probability evaluated by logistic regression, \\(\\Pr\\left( Y = 1 | \\textbf{x}_{new} \\right) &gt; 0.5\\), then we should classify this new case as positive, otherwise, negative group."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7.html#logistic-regression-in-r",
    "href": "Courses/c_mlwr1_2024/l7/l7.html#logistic-regression-in-r",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "2.3 Logistic Regression in R",
    "text": "2.3 Logistic Regression in R\nIn R, we can apply glm function to estimate logistic regression model, and the usage is very simple.\n# usage of function `glm` for estiamte logistic regression\nmodel = glm(MODEL_EXPRESSION, family = binomial(), DATA)\nSimilar to using lm to estimate linear regression, we first need to specify the model expression, and the rules are the same. However, the difference is that we need to verify another argument, family, as Binomial. This argument mainly determines the type of the target variable \\(y\\), that is, it specifies its distribution. “Binomial” means that the target variable follows a binomial distribution, which implies that \\(y\\) has a binary distribution. More demonstration by examples will be presented in the next subsection.\n\nQuiz: Can you guess what kind of model will be returned if you specify family as gaussian?"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7.html#cross-entropy-loss-and-likelihood-function",
    "href": "Courses/c_mlwr1_2024/l7/l7.html#cross-entropy-loss-and-likelihood-function",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "3.1 Cross-entropy Loss and Likelihood function",
    "text": "3.1 Cross-entropy Loss and Likelihood function\nAbove, we conceptually explained the logistic regression model and its classifier and demonstrated its implementation in R. Next, we will address a more theoretical question: how to train our model, or in other words, how to estimate the model parameters. This question can be approached from both machine learning and statistical modeling perspectives, yielding consistent conclusions.\n\nStrategy: From the machine learning perspective, it is relatively easy to formulate the optimization problem for the logistic regression model. However, different from MSE loss, to fully understand the cross-entropy loss function, we would need to learn some additional concepts from information theory. Since we have already studied likelihood theory, deriving the objective function (i.e., the likelihood function) from this perspective is comparatively straightforward. Therefore, to simplify the learning process, I will introduce the cross-entropy loss from the machine learning perspective and then interpret it through the lens of likelihood theory. In the near future, you can revisit this concept from the information theory perspective when you study neural networks.\n\n\n3.1.1 Cross-entropy Loss\nLet us recall the brute-force method we used when training a regression model. First, we define a loss function, specifically the MSE loss, which we use to evaluate the model’s performance.\n\n\n\nIn this formula: LHS: It represents the relationship between the model’s loss and its parameters, i.e., the loss function. Its value is determined by two factors: the model parameters and the data fed into the model. Since the data is fixed and unchanging, the quality of the model depends on the choice of parameters. RHS: It specifies how the loss is calculated. Since the model \\(f\\) is a regression model, its loss can be directly measured by the prediction error, namely the Mean Squared Error (MSE).\n\n\nAfter defining the loss function, we can select the optimal model based on the performance corresponding to different sets of model parameters. In the absence of an efficient algorithm, brute-force computation is the simplest solution. However, for a well-defined optimization problem, smart mathematicians would never resort to brute-force computation so easily. This has led to the development of various algorithms for training models, e.g. gradient descent algorithm.\nAlright, let’s return to the logistic regression model. How do we determine its model parameters? Similarly, we can design a loss function for the model and formulate it as an optimization problem. However, for a classification problem, we cannot directly calculate the model error and take the average, as we do in regression problems. Instead, the most commonly used loss function for classification problems is the cross-entropy loss: \\[\n  \\mathcal{L}\\left(\\textbf{w};\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N  \\right) = -\\frac{1}{N}\\sum_{i = 1}^N \\left\\{ y_i\\log( \\pi(\\textbf{x}_i, \\textbf{w}) ) + (1-y_i)\\log(1-\\pi(\\textbf{x}_i, \\textbf{w})) \\right\\}\n\\] Similar to training regression problems, the model parameters of logistic regression can be obtained by optimizing the cross-entropy loss, i.e., \\[\n   \\hat{\\textbf{w}} = \\arg\\max_{\\textbf{w}} \\mathcal{L}(\\textbf{w};\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N)\n\\]\n\nNote: The cross-entropy loss is undeniably famous. You will encounter it again when you study neural network models and deep learning in the future.\n\nUnlike regression problems, optimizing the cross-entropy loss does not have an analytical solution. This means that we must use numerical algorithms to find the optimal solution. Typically, we use second-order optimization algorithms to estimate the parameters of logistic regression, such as the Newton-Raphson algorithm you practiced in Lab 1. In broader fields, like optimizing deep neural network models, the gradient descent algorithm is commonly used. We will only touch on this briefly here, and I will discuss it in more detail in the future.\n\nThink: Why can’t we design the loss function using prediction error like in regression problems? What would happen if we did?\n\n\n\n3.1.2 Maximum Likelihood Estimation\nCross-entropy loss is not as easy to understand as MSE loss; you need to learn some information theory to fully grasp it. But don’t worry, here we will approach it from the perspective of statistical theory, specifically from the concept of maximum likelihood estimation (MLE), which you have already studied. In the end, you will find that the likelihood function of MLE and the cross-entropy loss are equivalent.\nSuppose we have a set of training observations, \\(\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i = 1}^N\\). The distribution of the target variable is Binary distribution, i.e.  \\[\n  \\Pr\\left( y_i, \\pi(\\textbf{x}_i, \\textbf{w}) \\right) = \\pi(\\textbf{x}_i, \\textbf{w}) ^{y_i} (1-\\pi(\\textbf{x}_i, \\textbf{w}))^{1 - y_i}  \n\\] where \\(y_i = 1 \\text{ or } 0\\). Since we have independent observations, the joint likelihood of the training sample is \\[\n  L\\left(\\textbf{w}; ;\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N \\right) = \\prod_{i = 1}^{n} \\pi(\\textbf{x}_i, \\textbf{w}) ^{y_i} (1-\\pi(\\textbf{x}_i, \\textbf{w}))^{1 - y_i}\n\\] The log-likelihood function is \\[\n  \\ell\\left(\\textbf{w}; \\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N\\right) = \\sum_{i = 1}^N \\left\\{ y_i\\log( \\pi(\\textbf{x}_i, \\textbf{w}) ) + (1-y_i)\\log(1-\\pi(\\textbf{x}_i, \\textbf{w})) \\right\\}\n\\] The MLE of \\(\\textbf{w}\\) is \\[\n  \\hat{\\textbf{w}}_{\\text{MLE}} = \\arg\\max_{\\textbf{w}} \\ell\\left(\\textbf{w};\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N\\right)\n\\]\nNow we can compare the likelihood function and the cross-entropy loss function. Upon comparison, you will find that they differ only by a negative sign. Therefore, maximizing the likelihood function is equivalent to minimizing the loss function; they are interchangeable. So, if you want to understand cross-entropy loss, start by approaching it from the perspective of likelihood analysis."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/Discussion of Lectures 6.html#lecture-6-model-validation-and-selection",
    "href": "Courses/c_mlwr1_2024/Discussion of Lectures 6.html#lecture-6-model-validation-and-selection",
    "title": "Discussion of Lecture 6",
    "section": "Lecture 6: Model Validation and Selection",
    "text": "Lecture 6: Model Validation and Selection\n\n\nWhat is KNN? What is special about KNN? Can you apply KNN for regression problem?\nWhat is difference between model parameters and hyper-parameters?\nHow do you deteremine the values of hyper-parameters?\nLOOCV V.S. KFCV V.S. Bootstrap\nWhy applying stepwise selection methods has the risk of local optimal solution trap?\nWhat is the difference between ridge regression and LASSO?\nWhat is the hyper-parameter in regularization methods? What is the its role?"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_0.html",
    "href": "Courses/c_mlwr1_2024/l7/l7_0.html",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "",
    "text": "In this lecture, we will learn about a very important classifier in machine learning, the logistic regression classifier. First, we will discuss intuitively how classification can be performed using a linear regression model and point out its critical weakness. Then, we will introduce logistic regression to address this weakness, discussing its model construction and how to use it as a classifier. Finally, we will discuss the training of the logistic regression model. By understanding its loss function, we will easily grasp how to use regularization methods in logistic regression.\nOutline:\n\n7.1 Regression Model for Classification\n7.2 Logistic Regression Classifier\n7.3 Cross-Entropy Loss and Penalized Logistic Regression\n\n\nLecture 7 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_1.html",
    "href": "Courses/c_mlwr1_2024/l7/l7_1.html",
    "title": "1 Regression Model for Classification",
    "section": "",
    "text": "“Regression model for classification” sounds a bit strange. Do these two types of problems have something in common? How can we use a linear regression model to build a classifier?\n\n1.1 Motivation\nAs we discussed earlier, if there is only one feature variable, how can we perform classification? Let’s revisit this simple problem to gain some insights. Based on our previous discussion, the key to this problem lies in determining the classification boundary, that is, finding a method to identify the cutoff. For example, when discussing Fisher’s idea, we mentioned that for two normal distributions with equal variance, the optimal classification boundary is the midpoint between the two population means. Now, let’s approach this from a different perspective.\nThe difficulty of this problem lies in the fact that different feature variables will have different criteria. For example, we might use body size to distinguish between ethnic groups, flower shape measurements to classify flower species, or image-extracted features to diagnose cases. However, there is no universal cutoff to serve as a standard for all of them. Therefore, we need to transform any variable onto a dimension that has a universal cutoff. So, which dimension is that magical dimension, and what should we do to find it?\nWe can use the target variable to find the magical dimension to perform the transformation. First, we encode the target variable to make the categorical variable numerical—for example, assigning 1 to the positive group and 0 to the negative group. Then, we estimate a linear regression model for the target variable based on the feature variables. Through the regression model, all observation points are projected onto that magical dimension, where 0.5 serves as the universal cutoff. See the demo below:\n\n\n    Replay\n\n\n\n\nRemark: You might find this problem a bit tedious and think, “Why not just pick a random point in the region between the two populations?” However, note that this approach is overly subjective. What we need is a universal method to calculate this cutoff, such as Fisher’s idea or the method discussed here. In the future, we will introduce a third approach. You can also think about whether you have any good ideas.\n\n\n\n1.2 Classifier based on Linear Regression Model\nThis idea can be naturally extended to the case of multiple feature variables. Suppose we have a set of feature variables, \\(X_1,X_2,\\dots,X_p\\), and the encoded target variable \\(Y\\). The trained multivariate linear regression model is \\[\n  Y = w_0 + w_1X_1 + w_2X_2 + \\dots + w_pX_p\n\\] With this model, the \\(p\\) feature variable values of any observation are recalculated and used for the final decision. We call the fitted \\(Y\\), \\(w_0 + w_1X_1 + w_2X_2 + \\dots + w_pX_p\\), Score. So, if \\(\\text{Score} &gt; 0.5\\) then it will be assigned as positive, and if \\(\\text{Score} &lt; 0.5\\) then it will be assigned as negative. So the decision boundary will be \\(\\text{Score} = 0.5\\), i.e.  \\[\n  w_0 + w_1X_1 + w_2X_2 + \\dots + w_pX_p = 0.5\n\\] Therefore, the classifier based on linear regression model is a linear decision boundary, the regression coefficients are the weights and \\(0.5-w_0\\) is the cutoff, or bias term. See the demo below.\n\n\n\n\n\n\n\n  There are two feature variables and the target variable has been encoded. That plane in blue-red gradient color represents a linear regression model with two variables, projecting the information from the two feature variables onto a third dimension. The black plane, parallel to the feature variables’ plane, represents our universal cutoff. If you rotate the 3D plot, you will see that the intersection line between the cutoff plane and the regression model (the blue-purple plane) is the classification boundary in the 2D feature space. \n\n\n\n1.3 Main Issues\nIn fact, linear regression models have many advantages. For example, the optimal solution under the MSE loss function has an analytical solution, meaning we have a formula to calculate the parameters of the regression model, which makes linear regression very efficient. However, in machine learning, it is almost never used as a classifier. Why is that? The main reason is that it has high requirements for the data distribution, and any significant outliers will affect its performance. For example, in the image below: on LHS is the classification boundary determined by the linear regression model. It looks fine with no issues. However, if we add some outliers to the red group, see the RHS, the decision boundary determined by the linear regression model is severely affected. Even with this simple problem, the linear regression model can make mistakes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo, how can we solve this problem? In the next section, we will answer these questions and introduce the important classifier in machine learning: logistic regression.\n\nQuiz: Can you use the basic statistical knowledge you’ve learned to explain why a classifier based on a linear regression model is sensitive to outliers?\n\n\nPrevious page | Lecture 7 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_2.html",
    "href": "Courses/c_mlwr1_2024/l7/l7_2.html",
    "title": "2. Logistic Regression Classifier",
    "section": "",
    "text": "Do you know about logistic regression? It is an important tool in statistical analysis. In the future, I will write a note introducing it from a statistical perspective. Similarly, it is also one of the most important classifiers in machine learning. For now, let’s introduce logistic regression from a machine learning perspective.\n\n\n\n\nNot long ago, I saw this post online, and I felt the original poster didn’t quite get to the point, so I left a comment below. After studying, come back and take a look. If you understand it, it means you have fully grasped logistic regression.\n\n\n\n\n\nLet’s go back to our previous discussion: what is the main issue with using linear regression models for classification problems? First, we need to clarify one point: what is the essence of the linear regression model? Recall the discussion of maximum likelihood estimation of linear regression model. The essence of the linear regression model is to predict the expected value of the target variable using a linear combination of the feature variables, i.e. \\[\n  \\text{E}(y | \\textbf{x}) = w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p\n\\] This is quite normal for a continuous target variable. However, when we try to extend this model to classification problems, we encounter new issues.\nEssentially, we are trying to model the expected value of target variable as the weighted sum of feature variables, when we apply linear regression model to a classification problem. However, in classification problems, the target variable is a categorical variable. For categorical variables, the assumption of a normal distribution is completely unreasonable. We need to use discrete distributions to characterize the distribution of these variables, for example, using binary distribution for a binary classification problem, i.e. \\(y \\sim \\text{Ber}(\\pi)\\), \\[\n  y = \\left\\{\n    \\begin{matrix}\n      1  & \\text{Positive case} \\\\\n      0 & \\text{Negative case}\n    \\end{matrix}\n  \\right.\n\\] What is the expected value of a binary distributed random variable? It is the probability of \\(y = 1\\), i.e. \\(\\text{E}(y) = \\pi\\). So, we are using the following equation \\[\n  \\pi|\\textbf{x} = \\text{E}(y|\\textbf{x}) = w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p\n\\] If we analyze the range of values on both sides of the equation, it is not difficult to identify the critical flaw in using linear regression to handle classification problems. The left side of the equation represents a probability value, with a range of \\([0, 1]\\), while the right side can take any real number. Clearly, fitting a probability with an arbitrary real number is unreasonable and this is where the problem lies.\n\n\n\nTo address the issue mentioned above, mathematicians proposed an idea: using a function to transform values that span the entire real number axis into a fraction. This transformation function is the well-known logistic function, \\[\n  \\phi(t) = \\frac{1}{1+e^{-t}}\n\\] The graph of the logistic function is shown in the figure below. We can see that through its transformation, the output for any input \\(x\\) lies between the two red dashed lines, meaning it is a fraction.\n\n\n\n\n\n\n\n\n\n\n\nWith the help of the logistic function, we can start building our logistic regression model.\nFor a binary classification problem, we assume that the target variable follows a binary distribution, where the probability of a positive case is calculated as the logistic transformed weighted sum of the feature variables plus a constant. In a formal mathematical language, logistic regression model presents a probability model, \\(y\\sim \\text{Ber}\\left( \\pi(\\textbf{x}; \\textbf{w}) \\right)\\), where \\[\n  \\pi(\\textbf{x}; \\textbf{w}) = \\phi(w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p) = \\frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p)}}\n\\] As can be seen from the above model, the logistic regression model essentially provides us with a mechanism for calculating the posterior probability of the target variable, i.e. \\[\n  \\Pr\\left( Y = 1 | \\textbf{x} \\right) =  \\frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p)}}\n\\] In other words, once we have obtained all the parameters \\(\\textbf{w}\\), we can evaluate the posterior probability using the values of the feature variables. If you still remember the classification rule of the GDA classifier, \\[\n  \\widehat{y} = \\arg \\max_{y} \\Pr(y| \\textbf{x})\n\\] then it is natural that the logistic regression model can be used as a classifier. Simply speaking, if the posterior probability evaluated by logistic regression, \\(\\Pr\\left( Y = 1 | \\textbf{x}_{new} \\right) &gt; 0.5\\), then we should classify this new case as positive, otherwise, negative group.\n\n\n\nIn R, we can apply glm function to estimate logistic regression model, and the usage is very simple.\n# usage of function `glm` for estiamte logistic regression\nmodel = glm(MODEL_EXPRESSION, family = binomial(), DATA)\nSimilar to using lm to estimate linear regression, we first need to specify the model expression, and the rules are the same. However, the difference is that we need to verify another argument, family, as Binomial. This argument mainly determines the type of the target variable \\(y\\), that is, it specifies its distribution. “Binomial” means that the target variable follows a binomial distribution, which implies that \\(y\\) has a binary distribution. More demonstration by examples will be presented in the next subsection.\n\nQuiz: Can you guess what kind of model will be returned if you specify family as gaussian?\n\n\n\n\nIn the field of statistics, logistic regression is typically considered a type of generalized linear regression model. Generalized linear models (GLMs) form a large family of models, which includes common distributions for the response variable, such as Poisson regression, multinomial regression, beta regression, and so on. Logistic regression is also often referred to as a nonlinear regression model because, in the end, it projects the feature variables onto a nonlinear surface, as shown in the figure below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n LHS: Multivariate linear regression model. RHS: Logistic regression model, the surface is \\(\\phi(w_0 + w_1X_1+ w_2X2)\\), where \\(\\phi\\) is the logistic function. \n\n\nQuestion: If logistic regression is considered a nonlinear statistical model, is it a nonlinear classifier?\n\n\n\nLet’s address the previous example with outliers. To solve this problem, we can kill three birds with one stone: learn an R example, validate the robustness of logistic regression, and finally, experimentally obtain the answer to the above question.\nI show you the dataset for the problem we discussed before and visualize it in a scatter plot.\n\nhead(dat)\n\n     x1    x2 y\n1 1.419 2.280 1\n2 0.553 3.838 1\n3 1.704 2.422 1\n4 1.331 3.096 1\n5 1.660 1.397 1\n6 2.434 1.712 1\n\n\n\ncolor_obs = ifelse(dat$y==1, \"blue\", \"red\")\npar(mar = rep(1,4))\nplot(dat$x1, dat$x2, col = color_obs, pch = 20, \n     cex = 2, axes = F, xlab=\"\", ylab = \"\")\n\n\n\n\n\n\n\n\nLet’s first demonstrate how to obtain the classification boundary for the linear regression model. By the way, the answer to the previous quiz is the linear regression model. That is, if we set the family argument to Gaussian, we will get a linear regression model. In other words, its output is the same as the output from the lm function. Now, let’s use it to calculate the model parameters.\n\nm_linReg = glm(y~., data = dat, family = \"gaussian\")\nsummary(m_linReg)\n\n\nCall:\nglm(formula = y ~ ., family = \"gaussian\", data = dat)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.08317    0.06386  16.962  &lt; 2e-16 ***\nx1          -0.03156    0.01510  -2.091   0.0392 *  \nx2          -0.10966    0.02241  -4.894 3.94e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1108737)\n\n    Null deviance: 25.000  on 99  degrees of freedom\nResidual deviance: 10.755  on 97  degrees of freedom\nAIC: 68.805\n\nNumber of Fisher Scoring iterations: 2\n\n\nNext, let’s render the classification boundary of the classifier based on this model. Similar to how we rendered the classification boundary for k-NN previously, we will first generate a grid for the 2D feature space. Then, we will use our model to classify each grid point and present the decision boundary.\n\nx1 = seq(-1,16,0.1)\nx2 = seq(0,14,0.1) \nd = expand.grid(x1 = x1, x2 = x2)\nclass(d)\n\n[1] \"data.frame\"\n\n\n\nscores = predict(m_linReg, d)\ncolor_test = ifelse(scores &gt; 0.5, \"#D6E8FF\", \"#FFD6D6\")\n\n\npar(mar = rep(1,4))\nplot(dat$x1, dat$x2, col = color_obs, pch = 20, \n     cex = 2, axes = F, xlab=\"\", ylab = \"\")\npoints(d$x1, d$x2, col = color_test, pch = 20, cex = 0.5)\npoints(dat$x1, dat$x2, col = color_obs, pch = 20, cex = 2)\n\n\n\n\n\n\n\n\nAs we observed earlier, the classifier based on the linear regression model is very sensitive to outliers. Even in such a simple classification problem, it makes errors in certain areas. Now, let’s take a look at the performance of the classifier based on the logistic regression model.\n\nm_LogReg = glm(y~., data = dat, family = binomial())\n\n\nres_pred = predict(m_LogReg, d, type = \"response\")\nrange(res_pred)\n\n[1] 2.220446e-16 1.000000e+00\n\n\nHere, we use the predict function to predict the label for each grid point. Note that in the function above, we set the type argument to ‘response’, so we will obtain the posterior probability for each grid point based on its coordinates. Then, we can classify the points using the standard cutoff of 0.5. Finally, we use this grid to present the classification boundary of the classifier based on the logistic regression model.\n\ncolor_test = ifelse(res_pred &gt; 0.5, \"#D6E8FF\", \"#FFD6D6\")\n\n\npar(mar = rep(1,4))\nplot(dat$x1, dat$x2, col = color_obs, pch = 20, \n     cex = 2, axes = F, xlab=\"\", ylab = \"\")\npoints(d$x1, d$x2, col = color_test, pch = 20, cex = 0.5)\npoints(dat$x1, dat$x2, col = color_obs, pch = 20, cex = 2)\n\n\n\n\n\n\n\n\nAs we can see, using logistic regression as a classifier allows us to correct the impact of outliers. Additionally, we can observe that the classification boundary of the logistic regression classifier is also a straight line, meaning it is a linear classifier. In fact, this conclusion is not difficult to reach. From the principle of the classifier, the classification boundary of the logistic regression classifier is: \\[\n  \\Pr(Y = 1|\\textbf{x}) = \\phi(\\text{scores}) = \\frac{1}{1 + e^{-(w_0 + w_1X_1 + \\dots + w_pX_p)}} = \\frac{1}{2}\n\\] If we simplify this formula, we can derive the classification boundary of the logistic regression classifier \\[\n  w_0 + w_1X_1 + \\dots + w_pX_p = 0\n\\] So, the classifier based on logistic regression is a linear classifier.\n\nPrevious page | Lecture 7 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_2.html#exploration",
    "href": "Courses/c_mlwr1_2024/l7/l7_2.html#exploration",
    "title": "2. Logistic Regression Classifier",
    "section": "",
    "text": "Let’s go back to our previous discussion: what is the main issue with using linear regression models for classification problems? First, we need to clarify one point: what is the essence of the linear regression model? Recall the discussion of maximum likelihood estimation of linear regression model. The essence of the linear regression model is to predict the expected value of the target variable using a linear combination of the feature variables, i.e. \\[\n  \\text{E}(y | \\textbf{x}) = w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p\n\\] This is quite normal for a continuous target variable. However, when we try to extend this model to classification problems, we encounter new issues.\nEssentially, we are trying to model the expected value of target variable as the weighted sum of feature variables, when we apply linear regression model to a classification problem. However, in classification problems, the target variable is a categorical variable. For categorical variables, the assumption of a normal distribution is completely unreasonable. We need to use discrete distributions to characterize the distribution of these variables, for example, using binary distribution for a binary classification problem, i.e. \\(y \\sim \\text{Ber}(\\pi)\\), \\[\n  y = \\left\\{\n    \\begin{matrix}\n      1  & \\text{Positive case} \\\\\n      0 & \\text{Negative case}\n    \\end{matrix}\n  \\right.\n\\] What is the expected value of a binary distributed random variable? It is the probability of \\(y = 1\\), i.e. \\(\\text{E}(y) = \\pi\\). So, we are using the following equation \\[\n  \\pi|\\textbf{x} = \\text{E}(y|\\textbf{x}) = w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p\n\\] If we analyze the range of values on both sides of the equation, it is not difficult to identify the critical flaw in using linear regression to handle classification problems. The left side of the equation represents a probability value, with a range of \\([0, 1]\\), while the right side can take any real number. Clearly, fitting a probability with an arbitrary real number is unreasonable and this is where the problem lies."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_2.html#model-construction",
    "href": "Courses/c_mlwr1_2024/l7/l7_2.html#model-construction",
    "title": "2. Logistic Regression Classifier",
    "section": "",
    "text": "To address the issue mentioned above, mathematicians proposed an idea: using a function to transform values that span the entire real number axis into a fraction. This transformation function is the well-known logistic function, \\[\n  \\phi(t) = \\frac{1}{1+e^{-t}}\n\\] The graph of the logistic function is shown in the figure below. We can see that through its transformation, the output for any input \\(x\\) lies between the two red dashed lines, meaning it is a fraction.\n\n\n\n\n\n\n\n\n\n\n\nWith the help of the logistic function, we can start building our logistic regression model.\nFor a binary classification problem, we assume that the target variable follows a binary distribution, where the probability of a positive case is calculated as the logistic transformed weighted sum of the feature variables plus a constant. In a formal mathematical language, logistic regression model presents a probability model, \\(y\\sim \\text{Ber}\\left( \\pi(\\textbf{x}; \\textbf{w}) \\right)\\), where \\[\n  \\pi(\\textbf{x}; \\textbf{w}) = \\phi(w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p) = \\frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p)}}\n\\] As can be seen from the above model, the logistic regression model essentially provides us with a mechanism for calculating the posterior probability of the target variable, i.e. \\[\n  \\Pr\\left( Y = 1 | \\textbf{x} \\right) =  \\frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2 + \\dots + w_px_p)}}\n\\] In other words, once we have obtained all the parameters \\(\\textbf{w}\\), we can evaluate the posterior probability using the values of the feature variables. If you still remember the classification rule of the GDA classifier, \\[\n  \\widehat{y} = \\arg \\max_{y} \\Pr(y| \\textbf{x})\n\\] then it is natural that the logistic regression model can be used as a classifier. Simply speaking, if the posterior probability evaluated by logistic regression, \\(\\Pr\\left( Y = 1 | \\textbf{x}_{new} \\right) &gt; 0.5\\), then we should classify this new case as positive, otherwise, negative group."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_2.html#logistic-regression-in-r",
    "href": "Courses/c_mlwr1_2024/l7/l7_2.html#logistic-regression-in-r",
    "title": "2. Logistic Regression Classifier",
    "section": "",
    "text": "In R, we can apply glm function to estimate logistic regression model, and the usage is very simple.\n# usage of function `glm` for estiamte logistic regression\nmodel = glm(MODEL_EXPRESSION, family = binomial(), DATA)\nSimilar to using lm to estimate linear regression, we first need to specify the model expression, and the rules are the same. However, the difference is that we need to verify another argument, family, as Binomial. This argument mainly determines the type of the target variable \\(y\\), that is, it specifies its distribution. “Binomial” means that the target variable follows a binomial distribution, which implies that \\(y\\) has a binary distribution. More demonstration by examples will be presented in the next subsection.\n\nQuiz: Can you guess what kind of model will be returned if you specify family as gaussian?"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_2.html#decision-boundary",
    "href": "Courses/c_mlwr1_2024/l7/l7_2.html#decision-boundary",
    "title": "2. Logistic Regression Classifier",
    "section": "",
    "text": "In the field of statistics, logistic regression is typically considered a type of generalized linear regression model. Generalized linear models (GLMs) form a large family of models, which includes common distributions for the response variable, such as Poisson regression, multinomial regression, beta regression, and so on. Logistic regression is also often referred to as a nonlinear regression model because, in the end, it projects the feature variables onto a nonlinear surface, as shown in the figure below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n LHS: Multivariate linear regression model. RHS: Logistic regression model, the surface is \\(\\phi(w_0 + w_1X_1+ w_2X2)\\), where \\(\\phi\\) is the logistic function. \n\n\nQuestion: If logistic regression is considered a nonlinear statistical model, is it a nonlinear classifier?\n\n\n\nLet’s address the previous example with outliers. To solve this problem, we can kill three birds with one stone: learn an R example, validate the robustness of logistic regression, and finally, experimentally obtain the answer to the above question.\nI show you the dataset for the problem we discussed before and visualize it in a scatter plot.\n\nhead(dat)\n\n     x1    x2 y\n1 1.419 2.280 1\n2 0.553 3.838 1\n3 1.704 2.422 1\n4 1.331 3.096 1\n5 1.660 1.397 1\n6 2.434 1.712 1\n\n\n\ncolor_obs = ifelse(dat$y==1, \"blue\", \"red\")\npar(mar = rep(1,4))\nplot(dat$x1, dat$x2, col = color_obs, pch = 20, \n     cex = 2, axes = F, xlab=\"\", ylab = \"\")\n\n\n\n\n\n\n\n\nLet’s first demonstrate how to obtain the classification boundary for the linear regression model. By the way, the answer to the previous quiz is the linear regression model. That is, if we set the family argument to Gaussian, we will get a linear regression model. In other words, its output is the same as the output from the lm function. Now, let’s use it to calculate the model parameters.\n\nm_linReg = glm(y~., data = dat, family = \"gaussian\")\nsummary(m_linReg)\n\n\nCall:\nglm(formula = y ~ ., family = \"gaussian\", data = dat)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.08317    0.06386  16.962  &lt; 2e-16 ***\nx1          -0.03156    0.01510  -2.091   0.0392 *  \nx2          -0.10966    0.02241  -4.894 3.94e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1108737)\n\n    Null deviance: 25.000  on 99  degrees of freedom\nResidual deviance: 10.755  on 97  degrees of freedom\nAIC: 68.805\n\nNumber of Fisher Scoring iterations: 2\n\n\nNext, let’s render the classification boundary of the classifier based on this model. Similar to how we rendered the classification boundary for k-NN previously, we will first generate a grid for the 2D feature space. Then, we will use our model to classify each grid point and present the decision boundary.\n\nx1 = seq(-1,16,0.1)\nx2 = seq(0,14,0.1) \nd = expand.grid(x1 = x1, x2 = x2)\nclass(d)\n\n[1] \"data.frame\"\n\n\n\nscores = predict(m_linReg, d)\ncolor_test = ifelse(scores &gt; 0.5, \"#D6E8FF\", \"#FFD6D6\")\n\n\npar(mar = rep(1,4))\nplot(dat$x1, dat$x2, col = color_obs, pch = 20, \n     cex = 2, axes = F, xlab=\"\", ylab = \"\")\npoints(d$x1, d$x2, col = color_test, pch = 20, cex = 0.5)\npoints(dat$x1, dat$x2, col = color_obs, pch = 20, cex = 2)\n\n\n\n\n\n\n\n\nAs we observed earlier, the classifier based on the linear regression model is very sensitive to outliers. Even in such a simple classification problem, it makes errors in certain areas. Now, let’s take a look at the performance of the classifier based on the logistic regression model.\n\nm_LogReg = glm(y~., data = dat, family = binomial())\n\n\nres_pred = predict(m_LogReg, d, type = \"response\")\nrange(res_pred)\n\n[1] 2.220446e-16 1.000000e+00\n\n\nHere, we use the predict function to predict the label for each grid point. Note that in the function above, we set the type argument to ‘response’, so we will obtain the posterior probability for each grid point based on its coordinates. Then, we can classify the points using the standard cutoff of 0.5. Finally, we use this grid to present the classification boundary of the classifier based on the logistic regression model.\n\ncolor_test = ifelse(res_pred &gt; 0.5, \"#D6E8FF\", \"#FFD6D6\")\n\n\npar(mar = rep(1,4))\nplot(dat$x1, dat$x2, col = color_obs, pch = 20, \n     cex = 2, axes = F, xlab=\"\", ylab = \"\")\npoints(d$x1, d$x2, col = color_test, pch = 20, cex = 0.5)\npoints(dat$x1, dat$x2, col = color_obs, pch = 20, cex = 2)\n\n\n\n\n\n\n\n\nAs we can see, using logistic regression as a classifier allows us to correct the impact of outliers. Additionally, we can observe that the classification boundary of the logistic regression classifier is also a straight line, meaning it is a linear classifier. In fact, this conclusion is not difficult to reach. From the principle of the classifier, the classification boundary of the logistic regression classifier is: \\[\n  \\Pr(Y = 1|\\textbf{x}) = \\phi(\\text{scores}) = \\frac{1}{1 + e^{-(w_0 + w_1X_1 + \\dots + w_pX_p)}} = \\frac{1}{2}\n\\] If we simplify this formula, we can derive the classification boundary of the logistic regression classifier \\[\n  w_0 + w_1X_1 + \\dots + w_pX_p = 0\n\\] So, the classifier based on logistic regression is a linear classifier.\n\nPrevious page | Lecture 7 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_3.html",
    "href": "Courses/c_mlwr1_2024/l7/l7_3.html",
    "title": "3. Cross-Entropy Loss and Penalized Logistic Regression",
    "section": "",
    "text": "Above, we conceptually explained the logistic regression model and its classifier and demonstrated its implementation in R. Next, we will address a more theoretical question: how to train our model, or in other words, how to estimate the model parameters. This question can be approached from both machine learning and statistical modeling perspectives, yielding consistent conclusions.\n\nStrategy: From the machine learning perspective, it is relatively easy to formulate the optimization problem for the logistic regression model. However, different from MSE loss, to fully understand the cross-entropy loss function, we would need to learn some additional concepts from information theory. Since we have already studied likelihood theory, deriving the objective function (i.e., the likelihood function) from this perspective is comparatively straightforward. Therefore, to simplify the learning process, I will introduce the cross-entropy loss from the machine learning perspective and then interpret it through the lens of likelihood theory. In the near future, you can revisit this concept from the information theory perspective when you study neural networks.\n\n\n\nLet us recall the brute-force method we used when training a regression model. First, we define a loss function, specifically the MSE loss, which we use to evaluate the model’s performance.\n\n\n\nIn this formula: LHS: It represents the relationship between the model’s loss and its parameters, i.e., the loss function. Its value is determined by two factors: the model parameters and the data fed into the model. Since the data is fixed and unchanging, the quality of the model depends on the choice of parameters. RHS: It specifies how the loss is calculated. Since the model \\(f\\) is a regression model, its loss can be directly measured by the prediction error, namely the Mean Squared Error (MSE).\n\n\nAfter defining the loss function, we can select the optimal model based on the performance corresponding to different sets of model parameters. In the absence of an efficient algorithm, brute-force computation is the simplest solution. However, for a well-defined optimization problem, smart mathematicians would never resort to brute-force computation so easily. This has led to the development of various algorithms for training models, e.g. gradient descent algorithm.\nAlright, let’s return to the logistic regression model. How do we determine its model parameters? Similarly, we can design a loss function for the model and formulate it as an optimization problem. However, for a classification problem, we cannot directly calculate the model error and take the average, as we do in regression problems. Instead, the most commonly used loss function for classification problems is the cross-entropy loss: \\[\n  \\mathcal{L}\\left(\\textbf{w};\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N  \\right) = -\\frac{1}{N}\\sum_{i = 1}^N \\left\\{ y_i\\log( \\pi(\\textbf{x}_i, \\textbf{w}) ) + (1-y_i)\\log(1-\\pi(\\textbf{x}_i, \\textbf{w})) \\right\\}\n\\] Similar to training regression problems, the model parameters of logistic regression can be obtained by optimizing the cross-entropy loss, i.e., \\[\n   \\hat{\\textbf{w}} = \\arg\\max_{\\textbf{w}} \\mathcal{L}(\\textbf{w};\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N)\n\\]\n\nNote: The cross-entropy loss is undeniably famous. You will encounter it again when you study neural network models and deep learning in the future.\n\nUnlike regression problems, optimizing the cross-entropy loss does not have an analytical solution. This means that we must use numerical algorithms to find the optimal solution. Typically, we use second-order optimization algorithms to estimate the parameters of logistic regression, such as the Newton-Raphson algorithm you practiced in Lab 1. In broader fields, like optimizing deep neural network models, the gradient descent algorithm is commonly used. We will only touch on this briefly here, and I will discuss it in more detail in the future.\n\nThink: Why can’t we design the loss function using prediction error like in regression problems? What would happen if we did?\n\n\n\n\nCross-entropy loss is not as easy to understand as MSE loss; you need to learn some information theory to fully grasp it. But don’t worry, here we will approach it from the perspective of statistical theory, specifically from the concept of maximum likelihood estimation (MLE), which you have already studied. In the end, you will find that the likelihood function of MLE and the cross-entropy loss are equivalent.\nSuppose we have a set of training observations, \\(\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i = 1}^N\\). The distribution of the target variable is Binary distribution, i.e.  \\[\n  \\Pr\\left( y_i, \\pi(\\textbf{x}_i, \\textbf{w}) \\right) = \\pi(\\textbf{x}_i, \\textbf{w}) ^{y_i} (1-\\pi(\\textbf{x}_i, \\textbf{w}))^{1 - y_i}  \n\\] where \\(y_i = 1 \\text{ or } 0\\). Since we have independent observations, the joint likelihood of the training sample is \\[\n  L\\left(\\textbf{w}; ;\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N \\right) = \\prod_{i = 1}^{n} \\pi(\\textbf{x}_i, \\textbf{w}) ^{y_i} (1-\\pi(\\textbf{x}_i, \\textbf{w}))^{1 - y_i}\n\\] The log-likelihood function is \\[\n  \\ell\\left(\\textbf{w}; \\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N\\right) = \\sum_{i = 1}^N \\left\\{ y_i\\log( \\pi(\\textbf{x}_i, \\textbf{w}) ) + (1-y_i)\\log(1-\\pi(\\textbf{x}_i, \\textbf{w})) \\right\\}\n\\] The MLE of \\(\\textbf{w}\\) is \\[\n  \\hat{\\textbf{w}}_{\\text{MLE}} = \\arg\\max_{\\textbf{w}} \\ell\\left(\\textbf{w};\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N\\right)\n\\]\nNow we can compare the likelihood function and the cross-entropy loss function. Upon comparison, you will find that they differ only by a negative sign. Therefore, maximizing the likelihood function is equivalent to minimizing the loss function; they are interchangeable. So, if you want to understand cross-entropy loss, start by approaching it from the perspective of likelihood analysis."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_3.html#cross-entropy-loss-and-likelihood-function",
    "href": "Courses/c_mlwr1_2024/l7/l7_3.html#cross-entropy-loss-and-likelihood-function",
    "title": "3. Cross-Entropy Loss and Penalized Logistic Regression",
    "section": "",
    "text": "Above, we conceptually explained the logistic regression model and its classifier and demonstrated its implementation in R. Next, we will address a more theoretical question: how to train our model, or in other words, how to estimate the model parameters. This question can be approached from both machine learning and statistical modeling perspectives, yielding consistent conclusions.\n\nStrategy: From the machine learning perspective, it is relatively easy to formulate the optimization problem for the logistic regression model. However, different from MSE loss, to fully understand the cross-entropy loss function, we would need to learn some additional concepts from information theory. Since we have already studied likelihood theory, deriving the objective function (i.e., the likelihood function) from this perspective is comparatively straightforward. Therefore, to simplify the learning process, I will introduce the cross-entropy loss from the machine learning perspective and then interpret it through the lens of likelihood theory. In the near future, you can revisit this concept from the information theory perspective when you study neural networks.\n\n\n\nLet us recall the brute-force method we used when training a regression model. First, we define a loss function, specifically the MSE loss, which we use to evaluate the model’s performance.\n\n\n\nIn this formula: LHS: It represents the relationship between the model’s loss and its parameters, i.e., the loss function. Its value is determined by two factors: the model parameters and the data fed into the model. Since the data is fixed and unchanging, the quality of the model depends on the choice of parameters. RHS: It specifies how the loss is calculated. Since the model \\(f\\) is a regression model, its loss can be directly measured by the prediction error, namely the Mean Squared Error (MSE).\n\n\nAfter defining the loss function, we can select the optimal model based on the performance corresponding to different sets of model parameters. In the absence of an efficient algorithm, brute-force computation is the simplest solution. However, for a well-defined optimization problem, smart mathematicians would never resort to brute-force computation so easily. This has led to the development of various algorithms for training models, e.g. gradient descent algorithm.\nAlright, let’s return to the logistic regression model. How do we determine its model parameters? Similarly, we can design a loss function for the model and formulate it as an optimization problem. However, for a classification problem, we cannot directly calculate the model error and take the average, as we do in regression problems. Instead, the most commonly used loss function for classification problems is the cross-entropy loss: \\[\n  \\mathcal{L}\\left(\\textbf{w};\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N  \\right) = -\\frac{1}{N}\\sum_{i = 1}^N \\left\\{ y_i\\log( \\pi(\\textbf{x}_i, \\textbf{w}) ) + (1-y_i)\\log(1-\\pi(\\textbf{x}_i, \\textbf{w})) \\right\\}\n\\] Similar to training regression problems, the model parameters of logistic regression can be obtained by optimizing the cross-entropy loss, i.e., \\[\n   \\hat{\\textbf{w}} = \\arg\\max_{\\textbf{w}} \\mathcal{L}(\\textbf{w};\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N)\n\\]\n\nNote: The cross-entropy loss is undeniably famous. You will encounter it again when you study neural network models and deep learning in the future.\n\nUnlike regression problems, optimizing the cross-entropy loss does not have an analytical solution. This means that we must use numerical algorithms to find the optimal solution. Typically, we use second-order optimization algorithms to estimate the parameters of logistic regression, such as the Newton-Raphson algorithm you practiced in Lab 1. In broader fields, like optimizing deep neural network models, the gradient descent algorithm is commonly used. We will only touch on this briefly here, and I will discuss it in more detail in the future.\n\nThink: Why can’t we design the loss function using prediction error like in regression problems? What would happen if we did?\n\n\n\n\nCross-entropy loss is not as easy to understand as MSE loss; you need to learn some information theory to fully grasp it. But don’t worry, here we will approach it from the perspective of statistical theory, specifically from the concept of maximum likelihood estimation (MLE), which you have already studied. In the end, you will find that the likelihood function of MLE and the cross-entropy loss are equivalent.\nSuppose we have a set of training observations, \\(\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i = 1}^N\\). The distribution of the target variable is Binary distribution, i.e.  \\[\n  \\Pr\\left( y_i, \\pi(\\textbf{x}_i, \\textbf{w}) \\right) = \\pi(\\textbf{x}_i, \\textbf{w}) ^{y_i} (1-\\pi(\\textbf{x}_i, \\textbf{w}))^{1 - y_i}  \n\\] where \\(y_i = 1 \\text{ or } 0\\). Since we have independent observations, the joint likelihood of the training sample is \\[\n  L\\left(\\textbf{w}; ;\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N \\right) = \\prod_{i = 1}^{n} \\pi(\\textbf{x}_i, \\textbf{w}) ^{y_i} (1-\\pi(\\textbf{x}_i, \\textbf{w}))^{1 - y_i}\n\\] The log-likelihood function is \\[\n  \\ell\\left(\\textbf{w}; \\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N\\right) = \\sum_{i = 1}^N \\left\\{ y_i\\log( \\pi(\\textbf{x}_i, \\textbf{w}) ) + (1-y_i)\\log(1-\\pi(\\textbf{x}_i, \\textbf{w})) \\right\\}\n\\] The MLE of \\(\\textbf{w}\\) is \\[\n  \\hat{\\textbf{w}}_{\\text{MLE}} = \\arg\\max_{\\textbf{w}} \\ell\\left(\\textbf{w};\\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N\\right)\n\\]\nNow we can compare the likelihood function and the cross-entropy loss function. Upon comparison, you will find that they differ only by a negative sign. Therefore, maximizing the likelihood function is equivalent to minimizing the loss function; they are interchangeable. So, if you want to understand cross-entropy loss, start by approaching it from the perspective of likelihood analysis."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_3.html#penalized-logistic-regression",
    "href": "Courses/c_mlwr1_2024/l7/l7_3.html#penalized-logistic-regression",
    "title": "3. Cross-Entropy Loss and Penalized Logistic Regression",
    "section": "3.2 Penalized Logistic Regression",
    "text": "3.2 Penalized Logistic Regression\nIn the previous lecture, we discussed the shrinkage and sparse versions of the regression model. Through these, we can both avoid the risk of overfitting and indirectly obtain feature selection results. For classification problems, we have similar tools available, that is penalized logistic regression.\nLet’s first recall the idea of penalized regression. We define a set of candidate models by adding the calculation of the budget for the model parameter values, i.e., \\[\n  \\textcolor[rgb]{1.00,0.00,0.00}{\\text{Candidate Models}} = \\textcolor[rgb]{1.00,0.50,0.00}{\\text{Full Model}} + \\textbf{Budget}(\\textbf{w}).\n\\] With the general form of all candidate models, the penalized regression problem can be formulated as \\[\n  \\min_{\\textbf{w}} \\left\\{ \\mathcal{L}_{mse}\\left( \\textbf{w}; \\left\\{ y_i, \\textbf{x}_i \\right\\}_{i=1}^N \\right) + \\lambda\\textbf{Budget}(\\textbf{w})  \\right\\}\n\\] where the mse loss is just the sum squared residuals, and the budget term can be \\(L_1\\) norm, i.e. LASSO, or \\(L_2\\) norm, i.e. ridge regression.\nNow, let’s return to the logistic regression model. The clever among you might have already realized that the difference between penalized logistic regression and the previous penalized regression is simply the choice of the loss function. If we replace the MSE loss in the above formula with the cross-entropy loss, we obtain the optimization problem for penalized logistic regression, and the optimal solution is the penalized logistic regression model parameters.\nSimilarly, if we choose the \\(L_2\\) norm, we will get a shrinkage solution, whereas the \\(L_1\\) norm will provide us with a sparse solution and serve as an important tool for feature selection in classification problems. In addition, we will encounter many variations of penalty terms, such as the Elastic net penalty, \\[\n  \\alpha \\times \\sum_{j = 1}^p |w_j|  + (1- \\alpha) \\times \\sum_{j = 1}^p w_j^2\n\\] where \\(\\alpha\\) is an extra hyper-parameter taking value in \\([0,1]\\). From the above formula, it is easy to see that the calculation of the parameter value budget in elastic net is intermediate between ridge regression and LASSO. If the parameter \\(\\alpha\\) is set to 1, the elastic net degenerates into the LASSO penalty. Conversely, if \\(\\alpha\\) is set to 0, we get the \\(L_2\\) penalty, which corresponds to ridge regression. When \\(\\alpha\\) takes any value between 0 and 1, we obtain the elastic net. In other words, the elastic net is a convex combination of the \\(L_1\\) penalty and the \\(L_2\\) penalty. This setup makes the corresponding candidate models more flexible. Of course, the trade-off is that we need to consider an additional hyperparameter. Alright, let’s stop here for now. We will explain the implementation of penalized logistic regression in more detail in the upcoming labs.\n\nPrevious page | Lecture 7 Homepage | Next page"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_4.html",
    "href": "Courses/c_mlwr1_2024/l7/l7_4.html",
    "title": "Course Conclusion",
    "section": "",
    "text": "Alright, we can finally take a break. In this course, we have introduced some important concepts of machine learning through simple models. I hope the gap between basic statistics and basic machine learning isn’t too wide. I also hope that you will revisit and digest these materials, laying a solid foundation for further learning in machine learning. Thank you for your attention, and I look forward to meeting you again here in the near future.\n\nWishing you all the best!\nXijia Liu, December 6, 2024, Umeå\n\n\nPrevious page | Lecture 7 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_home.html#lecture-notes",
    "href": "Courses/c_mlwr1_2024/l7/l7_home.html#lecture-notes",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "Lecture notes:",
    "text": "Lecture notes:\n\nRead the integrated notes: here\nRead the pagination notes: here\nDownload the PDF notes here."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_home.html#reading-guidelines-you-can-read-the-book",
    "href": "Courses/c_mlwr1_2024/l7/l7_home.html#reading-guidelines-you-can-read-the-book",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "Reading Guidelines: You can read the book…",
    "text": "Reading Guidelines: You can read the book…\nFor lecture 7, it is recommended that you read the following sections in the textbook.\n\nAbout Logistic regression classifier, read subsection 4.3"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_home.html#discussion",
    "href": "Courses/c_mlwr1_2024/l7/l7_home.html#discussion",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "Discussion",
    "text": "Discussion\nSlides for discussion: click here"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_home.html#lab",
    "href": "Courses/c_mlwr1_2024/l7/l7_home.html#lab",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "Lab:",
    "text": "Lab:\nLaboratory: Entrance\nSolutions: click here\nR file: TBA \n\nCourse Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7.html#motivation",
    "href": "Courses/c_mlwr1_2024/l7/l7.html#motivation",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "1.1 Motivation",
    "text": "1.1 Motivation\nAs we discussed earlier, if there is only one feature variable, how can we perform classification? Let’s revisit this simple problem to gain some insights. Based on our previous discussion, the key to this problem lies in determining the classification boundary, that is, finding a method to identify the cutoff. For example, when discussing Fisher’s idea, we mentioned that for two normal distributions with equal variance, the optimal classification boundary is the midpoint between the two population means. Now, let’s approach this from a different perspective.\nThe difficulty of this problem lies in the fact that different feature variables will have different criteria. For example, we might use body size to distinguish between ethnic groups, flower shape measurements to classify flower species, or image-extracted features to diagnose cases. However, there is no universal cutoff to serve as a standard for all of them. Therefore, we need to transform any variable onto a dimension that has a universal cutoff. So, which dimension is that magical dimension, and what should we do to find it?\nWe can use the target variable to find the magical dimension to perform the transformation. First, we encode the target variable to make the categorical variable numerical—for example, assigning 1 to the positive group and 0 to the negative group. Then, we estimate a linear regression model for the target variable based on the feature variables. Through the regression model, all observation points are projected onto that magical dimension, where 0.5 serves as the universal cutoff. See the demo below:\n\n\n    Replay\n\n\n\n\nRemark: You might find this problem a bit tedious and think, “Why not just pick a random point in the region between the two populations?” However, note that this approach is overly subjective. What we need is a universal method to calculate this cutoff, such as Fisher’s idea or the method discussed here. In the future, we will introduce a third approach. You can also think about whether you have any good ideas."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7.html#classifier-based-on-linear-regression-model",
    "href": "Courses/c_mlwr1_2024/l7/l7.html#classifier-based-on-linear-regression-model",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "1.2 Classifier based on Linear Regression Model",
    "text": "1.2 Classifier based on Linear Regression Model\nThis idea can be naturally extended to the case of multiple feature variables. Suppose we have a set of feature variables, \\(X_1,X_2,\\dots,X_p\\), and the encoded target variable \\(Y\\). The trained multivariate linear regression model is \\[\n  Y = w_0 + w_1X_1 + w_2X_2 + \\dots + w_pX_p\n\\] With this model, the \\(p\\) feature variable values of any observation are recalculated and used for the final decision. We call the fitted \\(Y\\), \\(w_0 + w_1X_1 + w_2X_2 + \\dots + w_pX_p\\), Score. So, if \\(\\text{Score} &gt; 0.5\\) then it will be assigned as positive, and if \\(\\text{Score} &lt; 0.5\\) then it will be assigned as negative. So the decision boundary will be \\(\\text{Score} = 0.5\\), i.e.  \\[\n  w_0 + w_1X_1 + w_2X_2 + \\dots + w_pX_p = 0.5\n\\] Therefore, the classifier based on linear regression model is a linear decision boundary, the regression coefficients are the weights and \\(0.5-w_0\\) is the cutoff, or bias term. See the demo below.\n\n\n\n\n\n\n\n  There are two feature variables and the target variable has been encoded. That plane in blue-red gradient color represents a linear regression model with two variables, projecting the information from the two feature variables onto a third dimension. The black plane, parallel to the feature variables’ plane, represents our universal cutoff. If you rotate the 3D plot, you will see that the intersection line between the cutoff plane and the regression model (the blue-purple plane) is the classification boundary in the 2D feature space."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7.html#main-issues",
    "href": "Courses/c_mlwr1_2024/l7/l7.html#main-issues",
    "title": "Lecture 7: Logistic Regression Classifier",
    "section": "1.3 Main Issues",
    "text": "1.3 Main Issues\nIn fact, linear regression models have many advantages. For example, the optimal solution under the MSE loss function has an analytical solution, meaning we have a formula to calculate the parameters of the regression model, which makes linear regression very efficient. However, in machine learning, it is almost never used as a classifier. Why is that? The main reason is that it has high requirements for the data distribution, and any significant outliers will affect its performance. For example, in the image below: on LHS is the classification boundary determined by the linear regression model. It looks fine with no issues. However, if we add some outliers to the red group, see the RHS, the decision boundary determined by the linear regression model is severely affected. Even with this simple problem, the linear regression model can make mistakes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo, how can we solve this problem? In the next section, we will answer these questions and introduce the important classifier in machine learning: logistic regression.\n\nQuiz: Can you use the basic statistical knowledge you’ve learned to explain why a classifier based on a linear regression model is sensitive to outliers?"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_lab_home.html",
    "href": "Courses/c_mlwr1_2024/l7/l7_lab_home.html",
    "title": "Lab 5: Logistic Regression Classifier",
    "section": "",
    "text": "We implement and practice LASSO and logistic regression classifier with diamonds dataset (download here) and breast-cancer dataset.\n\nTask 1: About Penalized Regression (LASSO)\nWe use the diamonds dataset that contains data for 53,940 diamonds. Each diamond includes 10 variables such as price, cut, color, and others. Train a regression model with a LASSO penalty to predict the price from different feature variables.\n\nThe first column is useless, we should remove it.\nSet random seed as 2023, then randomly split dataset training and testing set (80/20).\nApply cv.glmnet function to tune the hyper-parameter through a 10-fold cross-validation.\nThis is a good example showing how to include categorical feature variables when one implements LASSO regression by the glmnet package. The argument x in cv.glmnet function must be a matrix. Tips: We can use the function model.matrix. (We used this function in the previous lab)\nCalculate the square root of MSE of the model with the testing set.\n\n\n\nTask 2: About Logistic Regression\nWe use breast-cancer dataset to practice the Logistic regression and do some experiments. Train a logistic regression model with the training set to predict the diagnosis results from feature variables radius, texture, and smoothness. Evaluate the accuracy of the resulting model on the testing set.\n\nApply glm function to train the logistic regression model described above. Note: The variable Diagnosis is of character type, however, glm function only accepts a numerical or factor type target variable.\nEvaluate the resulting model with the testing set. The function predict can be applied to predict the label of new observations with the resulting model. Different from a regression model, however, you need to specify another argument type to correctly get the prediction. You can choose response, then the predict function will return the posterior probability. Calculate the accuracy and kappa statistic.\nManually calculate the posterior probability for an observation in the training set.\nWrite down the decision boundary of the resulting model.\n\n\n\nTask 3: About Penalized Logistic Regression\nWe use breast-cancer dataset to practice penalized logistic regression. Using all the 30 feature variables to train a penalized logistic regression model to predict the diagnosis results.\n\nApply cv.glmnet function with the training dataset. We do 10-fold cross-validation.\nPrint the coefficients of the final models with minimum and 1se misclassification errors\nEvaluate the resulting model with the testing set. Calculate the accuracy and kappa statistic.\n\n\nLecture 7 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l7/l7_lab_solutions.html",
    "href": "Courses/c_mlwr1_2024/l7/l7_lab_solutions.html",
    "title": "Solutions to Exercises in Lab 5",
    "section": "",
    "text": "We implement and practice LASSO and logistic regression classifier with diamonds dataset (download here) and breast-cancer dataset.\n\nTask 1: About Penalized Regression (LASSO)\nWe use the diamonds dataset that contains data for 53,940 diamonds. Each diamond includes 10 variables such as price, cut, color, and others. Train a regression model with a LASSO penalty to predict the price from different feature variables.\n\nThe first column is useless, we should remove it.\n\n\ndat = read.table(file = 'diamonds.csv', sep = \",\", header = T)\nhead(dat)\n\n  X carat       cut color clarity depth table price    x    y    z\n1 1  0.23     Ideal     E     SI2  61.5    55   326 3.95 3.98 2.43\n2 2  0.21   Premium     E     SI1  59.8    61   326 3.89 3.84 2.31\n3 3  0.23      Good     E     VS1  56.9    65   327 4.05 4.07 2.31\n4 4  0.29   Premium     I     VS2  62.4    58   334 4.20 4.23 2.63\n5 5  0.31      Good     J     SI2  63.3    58   335 4.34 4.35 2.75\n6 6  0.24 Very Good     J    VVS2  62.8    57   336 3.94 3.96 2.48\n\ndat = dat[,-1]\n\n\nSet random seed as 2023, then randomly split dataset training and testing set (80/20).\n\n\nset.seed(2023)\nid = sample(1:dim(dat)[1], dim(dat)[1]*0.8)\ndat_tr = dat[id, ]\ndat_te = dat[-id, ]\n\n\nApply cv.glmnet function to tune the hyper-parameter through a 10-fold cross-validation. This is a good example showing how to include categorical feature variables when one implements LASSO regression by the glmnet package. Tips: We can use the function model.matrix. (We used this function in the previous lab)\n\n\n# A good example of handling categorical feature variables\ndat_tr_x = model.matrix(price~.-1, dat_tr)\n\nThe argument x in cv.glmnet function must be a matrix.\n\nclass(dat_tr_x)\n\n[1] \"matrix\" \"array\" \n\n\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\nm = cv.glmnet(x = dat_tr_x, \n              y = dat_tr$price, \n              family = \"gaussian\", \n              alpha = 1, # if alpha = 1, we use lasso penalty, 0 for ridge regression, number between 0 and 1 for elstic net\n              nfolds = 10)\nplot(m)\n\n\n\n\n\n\n\n\n\nCalculate the square root of MSE of the model with the testing set.\n\n\ndat_te_x = model.matrix(price~.-1, dat_te)\npre_y = predict(m, newx = dat_te_x, s = \"lambda.min\") # again, 'newx' is prepared with function 'model.matrix'\nsqrt((mean((pre_y - dat_te$price)^2))) # model performance, square root MSE.\n\n[1] 1131.613\n\n\n\n\nTask 2: About Logistic Regression\nWe use breast-cancer dataset to practice the Logistic regression and do some experiments. Train a logistic regression model with the training set to predict the diagnosis results from feature variables radius, texture, and smoothness. Evaluate the accuracy of the resulting model on the testing set.\n\n# Import data\nrm(list = ls()) # clean the workspace, remove all the objects in the r environment \ndat_tr = read.table(\"BreastCancerTrain.txt\", header = T, sep = \",\")\ndat_te = read.table(\"BreastCancerTest.txt\", header = T, sep = \",\")\n\n\nApply glm function to train the logistic regression model described above. Note: The variable Diagnosis is of character type, however, glm function only accepts a numerical or factor type target variable.\n\n\n# here, we need to change the data type of variable 'Diagnosis'.\n# One option is changing it to numeric type\ndat_tr[,1] = ifelse(dat_tr$Diagnosis == \"M\", 1, 0)\ndat_te[,1] = ifelse(dat_te$Diagnosis == \"M\", 1, 0)\n\n Alternative option, change the type of target variables as factor\ndat_tr[,1] = as.factor(dat_tr[,1])\ndat_te[,1] = as.factor(dat_te[,1])\n\nm = glm(Diagnosis~radius+texture+smoothness, dat_tr, family = binomial())\n\n\nEvaluate the resulting model with the testing set. The function predict can be applied to predict the label of new observations with the resulting model. Different from a regression model, however, you need to specify another argument type to correctly get the prediction. You can choose response, then the predict function will return the posterior probability. Calculate the accuracy and kappa statistic.\n\n\nlibrary(caret)\ny_pre = predict(m, newdata = dat_te[,-1], type = \"response\") # here you need to set argument 'type' as 'response', then the posterior probability will be returned. \ny_pre = as.numeric(y_pre&gt;0.5) # choose 0.5 as a cutoff\n\nmean(y_pre == dat_te$Diagnosis)\n\n[1] 0.9122807\n\nconfusionMatrix(as.factor(y_pre), as.factor(dat_te$Diagnosis), positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 70  5\n         1  5 34\n                                          \n               Accuracy : 0.9123          \n                 95% CI : (0.8446, 0.9571)\n    No Information Rate : 0.6579          \n    P-Value [Acc &gt; NIR] : 2.23e-10        \n                                          \n                  Kappa : 0.8051          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.8718          \n            Specificity : 0.9333          \n         Pos Pred Value : 0.8718          \n         Neg Pred Value : 0.9333          \n             Prevalence : 0.3421          \n         Detection Rate : 0.2982          \n   Detection Prevalence : 0.3421          \n      Balanced Accuracy : 0.9026          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\nManually calculate the posterior probability for an observation in the training set.\n\n To manually calculate the posterior probability, we need to define two functions, one for calculating the score values and one for calculate the probability.\n\nm$coefficients # get the model coefficients\n\n(Intercept)      radius     texture  smoothness \n-43.2524211   1.4078512   0.4230013 147.4029846 \n\nscore_function = function(x, coeff){\n  x = c(1,x) \n  score = sum(x*coeff) # w_0 + w_1x_1 + w_2x_2 + ... + w_px_p\n  return(score)\n}\nlogit_function = function(s){1/(1+exp(-s))} # define logit function for calculating posterior probability\n\n Here, we show the results for the 105th observation in the dataset.\n\nid = 105\n# First, calculate the posterior probability by function `predict`\nprob = predict(m, newdata = dat_te[id, ], type = \"response\")\nprob\n\n      105 \n0.9997994 \n\n# Second, manually calculate the posteriro probability\nx = as.matrix(dat_te[id, c(2,3,6)]) # `dat_te` is a data frame, we need to transform it to an array \nscore = score_function(x, m$coefficients)\nprob = logit_function(score)\nprob\n\n[1] 0.9997994\n\n\n\nWrite down the decision boundary of the resulting model.  Check the model coefficients first\n\n\nm$coefficients\n\n(Intercept)      radius     texture  smoothness \n-43.2524211   1.4078512   0.4230013 147.4029846 \n\n\n The decision boundary is \\[\n  1.41\\times radius + 0.42 \\times texture + 147.4 \\times smoothness = 43.252\n\\] i.e. if the score value is above \\(43.252\\), then we predict it as positive case (malignant). \n\n\nTask 3: About Penalized Logistic Regression\nWe use breast-cancer dataset to practice penalized logistic regression. Using all the 30 feature variables to train a penalized logistic regression model to predict the diagnosis results.\n\n# Import data\nrm(list = ls()) # clean the workspace, remove all the objects in the r environment \ndat_tr = read.table(\"BreastCancerTrain.txt\", header = T, sep = \",\")\ndat_te = read.table(\"BreastCancerTest.txt\", header = T, sep = \",\")\n\n\nApply cv.glmnet function with the training dataset. We do 10-fold cross-validation.\n\n\nm = cv.glmnet(x = as.matrix(dat_tr[,-1]), \n              y = dat_tr$Diagnosis, \n              data = dat_tr, \n              family = \"binomial\",\n              alpha = 1,\n              type.measure = 'class', # if you set it as 'class', then the cross validation is based on accuracy, otherwise an other statistic will be used.\n              nfolds = 10)\nplot(m)\n\n\n\n\n\n\n\n\n\nPrint the coefficients of the final models with minimum and 1se misclassification errors\n\n\ncoef(m, s=\"lambda.min\")\n\n31 x 1 sparse Matrix of class \"dgCMatrix\"\n                               s1\n(Intercept)           -29.2856879\nradius                  .        \ntexture                 .        \nperimeter               .        \narea                    .        \nsmoothness              .        \ncompactness            -0.7484243\nconcavity               .        \nconcave.points         46.7025092\nsymmetry                .        \nfractal.dimension     -31.2602347\nradius.sd               7.3236847\ntexture.sd             -0.3241931\nperimeter.sd            .        \narea.sd                 .        \nsmoothness.sd         129.6340403\ncompactness.sd        -48.4007891\nconcavity.sd            .        \nconcave.points.sd       .        \nsymmetry.sd             .        \nfractal.dimension.sd  -96.7969337\nradius.max              0.7350046\ntexture.max             0.3013601\nperimeter.max           .        \narea.max                .        \nsmoothness.max          3.4507500\ncompactness.max         .        \nconcavity.max           6.3141801\nconcave.points.max     18.1460402\nsymmetry.max            9.3793100\nfractal.dimension.max   .        \n\ncoef(m, s=\"lambda.1se\")\n\n31 x 1 sparse Matrix of class \"dgCMatrix\"\n                               s1\n(Intercept)           -25.3871148\nradius                  .        \ntexture                 .        \nperimeter               .        \narea                    .        \nsmoothness              .        \ncompactness             .        \nconcavity               .        \nconcave.points         38.5959990\nsymmetry                .        \nfractal.dimension     -23.7109255\nradius.sd               3.1344563\ntexture.sd              .        \nperimeter.sd            .        \narea.sd                 .        \nsmoothness.sd          22.9577651\ncompactness.sd         -7.6073052\nconcavity.sd            .        \nconcave.points.sd       .        \nsymmetry.sd             .        \nfractal.dimension.sd  -34.1606945\nradius.max              0.6937458\ntexture.max             0.2429477\nperimeter.max           .        \narea.max                .        \nsmoothness.max          8.6824182\ncompactness.max         .        \nconcavity.max           2.6250709\nconcave.points.max     15.7788687\nsymmetry.max            6.9674362\nfractal.dimension.max   .        \n\n\n\nEvaluate the resulting model with the testing set. Calculate the accuracy and kappa statistic.\n\n\ny_pre = predict(m, as.matrix(dat_te[,-1]), s = \"lambda.1se\", type = \"response\")\ny_pre = as.numeric(y_pre&gt;0.5)\n\n\ny_pre = ifelse(y_pre == 1, 'M', 'B')\nmean(y_pre == dat_te$Diagnosis)\n\n[1] 0.9561404\n\nconfusionMatrix(as.factor(y_pre), as.factor(dat_te$Diagnosis), positive = \"M\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  B  M\n         B 73  3\n         M  2 36\n                                          \n               Accuracy : 0.9561          \n                 95% CI : (0.9006, 0.9856)\n    No Information Rate : 0.6579          \n    P-Value [Acc &gt; NIR] : 1.136e-14       \n                                          \n                  Kappa : 0.902           \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.9231          \n            Specificity : 0.9733          \n         Pos Pred Value : 0.9474          \n         Neg Pred Value : 0.9605          \n             Prevalence : 0.3421          \n         Detection Rate : 0.3158          \n   Detection Prevalence : 0.3333          \n      Balanced Accuracy : 0.9482          \n                                          \n       'Positive' Class : M               \n                                          \n\n\n\nLecture 7 Homepage"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/Discussion of Lectures 7.html#lecture-7-logistic-regression-classifier",
    "href": "Courses/c_mlwr1_2024/Discussion of Lectures 7.html#lecture-7-logistic-regression-classifier",
    "title": "Discussion of Lecture 7",
    "section": "",
    "text": "What is the main idea of classifiers based on regular linear regression model?\nWhat is the limitation of classifiers based on regression model?\nWhat are the similarities and differences between linear regression and logistic regression?\nHow to build a classifier based on logistic regression models?\nIs the classifier based on logistic regression a linear classifier or nonlinear classifier?\nHow to determine the decision boundary?\nCross-entropy loss function and likelihood function."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/test.html",
    "href": "Courses/c_mlwr1_2024/test.html",
    "title": "Test",
    "section": "",
    "text": "Slides Demo\n\n\n\nUse the left and right arrow keys to navigate the slides."
  },
  {
    "objectID": "slides_demos/knn.html#demo-of-k-nearest-neighbors-method",
    "href": "slides_demos/knn.html#demo-of-k-nearest-neighbors-method",
    "title": "My Yggdrasil",
    "section": "Demo of K-nearest neighbors method",
    "text": "Demo of K-nearest neighbors method"
  },
  {
    "objectID": "slides_demos/knn.html#we-have-a-binary-classification-problem",
    "href": "slides_demos/knn.html#we-have-a-binary-classification-problem",
    "title": "My Yggdrasil",
    "section": "We have a binary classification problem",
    "text": "We have a binary classification problem"
  },
  {
    "objectID": "slides_demos/knn.html#section",
    "href": "slides_demos/knn.html#section",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Now, let’s assign a label to the purple star point"
  },
  {
    "objectID": "slides_demos/knn.html#section-1",
    "href": "slides_demos/knn.html#section-1",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Step 1: find the 3 nearest neighbors"
  },
  {
    "objectID": "slides_demos/knn.html#section-2",
    "href": "slides_demos/knn.html#section-2",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Step 2: preditc it as the color of majority"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html",
    "title": "My Yggdrasil",
    "section": "",
    "text": "A binary classification problem\n\n\n\n\nAssign a label to the purple star point\n\n\n\n\nStep 1: find the 3 nearest neighbors\n\n\n\n\nStep 2: preditc it as the color of majority\n\n\n\n\nAnother example\n\n\n\n\nStep 1: find the 3 nearest neighbors\n\n\n\n\nStep 2: preditc it as the color of majority\n\n\n\n\nIf apply KNN to all possible points, then…\n\n\n\n\nWe get the decision Boundary"
  },
  {
    "objectID": "Courses/mlwr2_2025/mlwr2_index.html#course-design",
    "href": "Courses/mlwr2_2025/mlwr2_index.html#course-design",
    "title": "Machine Learning with R, Part 2",
    "section": "Course Design",
    "text": "Course Design\nIn the second part of this course, for better readability, I will introduce embedded slides like the one below. Therefore, when you see windows with light green borders, please don’t mistake them for simple images and overlook the important information. Next, I will try to use slides to illustrate the design of my course.\n\n\n\n\n\n\n\nSlides for course design"
  },
  {
    "objectID": "Courses/mlwr2_2025/mlwr2_index.html#textbook",
    "href": "Courses/mlwr2_2025/mlwr2_index.html#textbook",
    "title": "Machine Learning with R, Part 2",
    "section": "TextBook",
    "text": "TextBook\nWe use ‘An Introduction to statistical learning’ as our textbook. The website of the book: https://www.statlearning.com. On this website, you can not only get an electronic copy of this book, but also find a lot of useful information."
  },
  {
    "objectID": "Courses/mlwr2_2025/mlwr2_index.html#teaching-methods",
    "href": "Courses/mlwr2_2025/mlwr2_index.html#teaching-methods",
    "title": "Machine Learning with R, Part 2",
    "section": "Teaching Methods",
    "text": "Teaching Methods\nAs an online course, we naturally choose the flipped classroom teaching method. That is, students first read and study the materials and textbooks we provided, and then conduct laboratory lessons after discussions in a one hour recap session."
  },
  {
    "objectID": "Courses/mlwr2_2025/mlwr2_index.html#examination-methods",
    "href": "Courses/mlwr2_2025/mlwr2_index.html#examination-methods",
    "title": "Machine Learning with R, Part 2",
    "section": "Examination Methods",
    "text": "Examination Methods\nWe use a combination of project study and oral interviews to assess students’ mastery of course knowledge."
  },
  {
    "objectID": "Courses/mlwr2_2025/mlwr2_index.html#list-of-lectures",
    "href": "Courses/mlwr2_2025/mlwr2_index.html#list-of-lectures",
    "title": "Machine Learning with R, Part 2",
    "section": "List of Lectures",
    "text": "List of Lectures\n\nLecture 1. Feature Extraction and PCA\nLecture 2. Another Perspective on PCA\nLecture 3. Artificial Neural Networks and Deep Learning"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1.html#definition-of-pca",
    "href": "Courses/mlwr2_2025/l1/l1.html#definition-of-pca",
    "title": "Lecture 1: Feature Extraction",
    "section": "2.1 Definition of PCA",
    "text": "2.1 Definition of PCA"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1.html#formulation-of-pca-problem",
    "href": "Courses/mlwr2_2025/l1/l1.html#formulation-of-pca-problem",
    "title": "Lecture 1: Feature Extraction",
    "section": "2.2 Formulation of PCA Problem",
    "text": "2.2 Formulation of PCA Problem"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1.html#pca-algorithm",
    "href": "Courses/mlwr2_2025/l1/l1.html#pca-algorithm",
    "title": "Lecture 1: Feature Extraction and PCA",
    "section": "3.1 PCA Algorithm",
    "text": "3.1 PCA Algorithm\nIn the previous section, we discussed what PCA is and the optimization problem it involves. This optimization problem is actually quite easy to solve because it is equivalent to an eigenvalue decomposition problem. Here, we will skip the mathematical details and directly present the algorithm, explaining how to obtain PC weights and use them to compute the new data matrix. Finally, we will demonstrate this with a small example.\n\n\nAlgorithm: Principal Components Analysis\n\n\nInputs:\n\n\\(\\textbf{X}\\), \\(n\\times p\\) matrix, containing all \\(p\\) feature variables.\n\n\nSteps:\n\nCalculate the covariance matrix, \\(S_{\\textbf{X}}\\)\nPerform eigenvalue decomposition on the covariance matrix, \\(S_{\\textbf{X}} = \\textbf{W} \\boldsymbol{\\Lambda} \\textbf{W}^{\\top}\\). \\(\\mathbf{W}\\) is a \\(p \\times p\\) matrix, where each column, \\(\\textbf{w}_j\\), is an eigenvector, and we call it weights matrix. \\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix, where each diagonal element \\(\\lambda_j\\) is the eigenvalue corresponding to its eigenvector \\(\\textbf{w}_j\\).\nCalculate the new data matrix: \\(\\mathbf{Z} = \\mathbf{XW}\\)\n\n\nOutput: The new data matrix, \\(\\mathbf{Z}\\), the weights matrix, \\(\\mathbf{W}\\), and the matrix of eigenvalues, \\(\\boldsymbol{\\Lambda}\\).\n\nFirst, it is not difficult to guess that the PC weights we are looking for are exactly the column vectors in \\(\\mathbf{W}\\). Each column vector (eigenvector) \\(\\textbf{w}_j\\) is a p-dimensional vector, which contains the weights needed for each original variable when computing the new variables.\nSecond, an observation no mater in the original data matrix, \\(\\textbf{x}_i\\), i.e. each row in \\(\\textbf{X}\\), or a new observation, \\(\\textbf{x}_{new}\\), we can calculate the new variable (extracted feature) as \\(\\textbf{x}_i^{\\top}\\textbf{w}_j\\) or \\(\\textbf{x}_{new}^{\\top}\\textbf{w}_j\\). For the entire original data matrix, we can compute all possible extracted feature variables by matrix multiplication at once, i.e. \\(\\mathbf{Z} = \\mathbf{XW}\\).\nFinally, the eigenvalue matrix, \\(\\boldsymbol{\\Lambda}\\), contains the variance of all extracted features, which represents the amount of information they carry.\n\nRemark: In many textbooks, authors emphasize that the mean of all variables should be removed before performing PCA. This step is not essential, but it can lead to slight differences in the calculation of eigenvectors depending on the algorithm used. We will not go into too much detail on this here."
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1.html#a-simple-example",
    "href": "Courses/mlwr2_2025/l1/l1.html#a-simple-example",
    "title": "Lecture 1: Feature Extraction and PCA",
    "section": "3.2 A Simple Example",
    "text": "3.2 A Simple Example\nIn this section, we use a simple example to demonstrate how to implement PCA with a simple program in R. First, we import the data, record some basic information, and visualize the data\n\ndat = iris #import data\nX = as.matrix(dat[,-5]) #take the 4 feature variables and save them in a matrix\nn = dim(X)[1] #record the sample size\np = dim(X)[2] #record the number of variables\npairs(X, col = dat$Species) #visualize the data in a pairwise scatter plot\n\n\n\n\n\n\n\n\nNext we do PCA on data matrix X. First, we calculate the covariance matrix.\n\nS = cov(X) # calculate the covariance matrix\n\nSecond, do eigenvalue decomposition on covariance matrix S and save the results in res. In R, we can apply function eigen to perform eigenvalue decomposition.\n\nres = eigen(S)\nstr(res)\n\nList of 2\n $ values : num [1:4] 4.2282 0.2427 0.0782 0.0238\n $ vectors: num [1:4, 1:4] 0.3614 -0.0845 0.8567 0.3583 0.6566 ...\n - attr(*, \"class\")= chr \"eigen\"\n\n\nAs shown above, the results of the eigenvalue decomposition contain all the information we need. slot values contains all the eigenvalues, and vectors contains all the eigenvectors, that is the optimal weights matrix.\n\nW = res$vectors # define the weights matrix W\nw_1 = W[,1] # the first column of W is the first PC weights.\nw_1\n\n[1]  0.36138659 -0.08452251  0.85667061  0.35828920\n\n\nAbove, we define the weights matrix \\(\\textbf{W}\\), and the first column, \\(\\textbf{w}_i\\), contains the optimal weights for calculating the first extracted variables. Next, we extract the first PC of the first flower.\n\nX[1, , drop = F]\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]          5.1         3.5          1.4         0.2\n\nX[1, , drop = F]%*%w_1 # or, you can try sum(X[1,]*w_1) which is more like a weighted sum.\n\n        [,1]\n[1,] 2.81824\n\n\nSo, \\(2.81824\\) is just value of first PC for the first flower. We can extract the first PC for all the flowers together, then we get the first new variable.\n\nz_1 = X%*%w_1\n\nYou can print out z_1 in your own computer and you will see it is a \\(150 \\times 1\\) vector. We can even use matrix multiplication more efficiently to extract all possible principal components (PCs) at once.\n\nZ = X%*%W\ndim(Z)\n\n[1] 150   4\n\n\nYou can see that the new data matrix Z has the same dimension as the original data matrix X. But what is the advantage of the new data matrix? We can find the answer by comparing the covariance matrices of the two data matrices.\n\nround(cov(X),2)\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         0.69       -0.04         1.27        0.52\nSepal.Width         -0.04        0.19        -0.33       -0.12\nPetal.Length         1.27       -0.33         3.12        1.30\nPetal.Width          0.52       -0.12         1.30        0.58\n\nround(cov(Z),2)\n\n     [,1] [,2] [,3] [,4]\n[1,] 4.23 0.00 0.00 0.00\n[2,] 0.00 0.24 0.00 0.00\n[3,] 0.00 0.00 0.08 0.00\n[4,] 0.00 0.00 0.00 0.02\n\n\nIt is easy to see that, compared to the original data matrix, the covariance matrix of the new data matrix is much simpler. The new variables are not only uncorrelated, but their variances gradually decrease.\nThis means that the amount of information contained in the variables of the new data matrix gradually decreases. The first new variable contains the most information, while the last new variable carries very little. Based on this, we can easily decide which variables to discard from the new matrix. This is the key essence of the PCA algorithm.\n\nround(res$values,2)\n\n[1] 4.23 0.24 0.08 0.02\n\n\nFinally, let’s take a look at the eigenvalues obtained from the eigenvalue decomposition. Yes, the eigenvalues represent the variances of the new variables, which indicate the amount of information they contain."
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1.html#hwd-data-example",
    "href": "Courses/mlwr2_2025/l1/l1.html#hwd-data-example",
    "title": "Lecture 1: Feature Extraction",
    "section": "3.1 HWD Data Example",
    "text": "3.1 HWD Data Example\nHere, we will apply PCA to a larger dataset—the Handwritten Digit (HWD) dataset, a benchmark dataset in machine learning. This example will illustrate and help us grasp a deeper aspect of PCA. This profound idea not only aids in better understanding feature extraction but will also assist us later in quickly comprehending the essence of artificial neural networks and deep learning.\n\n3.1.1 HWD Dataset\nFirst, let’s introduce HWD dataset. The Handwritten Digit dataset consists of around 7291 images of digits ranging from 0 to 9.\n\ndat = read.table(gzfile(\"zip.train.gz\"))\ndat = as.matrix(dat)\ndim(dat)\n\n[1] 7291  257\n\ndat[1, 1:10]\n\n    V1     V2     V3     V4     V5     V6     V7     V8     V9    V10 \n 6.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.631  0.862 \n\n\nAs you can see, the HWD dataset has been imported and named dat. This dataset contains 257 variables and 7291 cases (images). The first variable is the number presented by the image, for example, the first row in dat presents number 6. Each image is represented as a 16 by 16 pixel grid, flattened into a vector of 256 values (one for each pixel). Therefore, each row in the dataset corresponds to one such vector, effectively capturing the pixel intensity of a single digit. By reshaping the 256-dimensional vector back into a 16x16 matrix, the original digit image can be reconstructed and visualized, for example\n\nX = dat[which(dat[,1] == 4), -1]\ny = dat[which(dat[,1] == 4), 1]\ndim(X)\n\n[1] 652 256\n\n\n\ntemp = matrix(X[8,256:1],16,16,byrow=T)[,16:1]\npar(pin = c(4, 4))\nimage(t(temp), col=cus_col(256), frame = T, axes = F)\ngrid(nx = nrow(temp), ny = ncol(temp), col = \"black\", lty = \"dotted\")\n\n\n\n\n\n\n\n\n\nn = dim(X)[1]\nset.seed(8312)\nID = sample(1:n, 24)\npar(mfrow = c(4,6), mar = rep(1,4))\nfor(i in ID){\n  temp= matrix(X[i,256:1],16,16,byrow=T)[,16:1]\n  image(t(temp), col=cus_col(256), frame = T, axes = F)\n  title(paste(\"ID: \", i))\n}\n\n\n\n\n\n\n\n\n\n\n3.1.2 PCA on HWD Data and Analysis\n\nmean_image = colMeans(X)\ntemp = matrix(mean_image[256:1],16,16,byrow=T)[,16:1]\npar(pin = c(4, 4))\nimage(t(temp), col=cus_col(256), frame = T, axes = F)\n\n\n\n\n\n\n\n\n\nS = cov(X)\npca_res = eigen(S)\nQ = pca_res$vectors\ndim(Q)\n\n[1] 256 256\n\n\n\nsum((X[1,]-mean_image)*Q[,1])\n\n[1] 3.19266\n\n\n\nn = dim(X)[1]\nZ = (X - matrix(rep(mean_image, n),n,256,byrow = T))%*%Q\ndim(Z)\n\n[1] 652 256\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2,3), mar=2*c(1,1,1,1)) \nfor(i in 1:6){\n  temp = matrix(Q[256:1,i],16,16,byrow=T)[,16:1]\n  image(t(temp), col=cus_col(256), frame = T, axes = F)\n  title(paste0(\"Weights for PC\", i))\n}"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1.html#image-reconstuction-view",
    "href": "Courses/mlwr2_2025/l1/l1.html#image-reconstuction-view",
    "title": "Lecture 1: Feature Extraction",
    "section": "3.2 Image Reconstuction View",
    "text": "3.2 Image Reconstuction View\n\npar(mfrow = c(2,3), mar=2*c(1,1,1,1))\nfor(i in 251:256){\n  temp = matrix(Q[256:1,i],16,16,byrow=T)[,16:1]\n  image(t(temp), col=cus_col(256), frame = T, axes = F)\n}"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1.html#features-v.s.-individuals",
    "href": "Courses/mlwr2_2025/l1/l1.html#features-v.s.-individuals",
    "title": "Lecture 1: Feature Extraction",
    "section": "4.1 Features V.S. Individuals",
    "text": "4.1 Features V.S. Individuals"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1.html#feature-extraction-in-machine-learning",
    "href": "Courses/mlwr2_2025/l1/l1.html#feature-extraction-in-machine-learning",
    "title": "Lecture 1: Feature Extraction",
    "section": "4.2 Feature Extraction in Machine Learning",
    "text": "4.2 Feature Extraction in Machine Learning"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1.html",
    "href": "Courses/mlwr2_2025/l1/l1.html",
    "title": "Lecture 1: Feature Extraction and PCA",
    "section": "",
    "text": "In the first part of the course, we discussed one approach to solving the problem of data dimensionality reduction: feature selection. By using different feature selection methods, we can control the size of the feature space, thereby obtaining relatively simpler models to overcome the overfitting issue. In this lecture, we will introduce another type of methods of data dimensionality reduction, which is also an essential concept in machine learning: feature extraction. Feature extraction can be understood as a general term for methods that construct new variables from the original ones.\nBy using the original feature variables to create a relatively smaller set of new feature variables, we can control the size of the feature space. At the same time, a good machine learning model depends on two factors: efficient algorithms and a well-designed feature space. It is not hard to understand that if we have an absolutely ideal feature space—one that is linearly separable—then simply applying a basic algorithm can yield a perfect predictive model. Therefore, feature extraction plays a crucial role in machine learning applications."
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_temp.html#definition-of-pca",
    "href": "Courses/mlwr2_2025/l1/l1_temp.html#definition-of-pca",
    "title": "Lecture 1: Feature Extraction",
    "section": "2.1 Definition of PCA",
    "text": "2.1 Definition of PCA"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_temp.html#formulation-of-pca-problem",
    "href": "Courses/mlwr2_2025/l1/l1_temp.html#formulation-of-pca-problem",
    "title": "Lecture 1: Feature Extraction",
    "section": "2.2 Formulation of PCA Problem",
    "text": "2.2 Formulation of PCA Problem"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_temp.html#pca-algorithm",
    "href": "Courses/mlwr2_2025/l1/l1_temp.html#pca-algorithm",
    "title": "Lecture 1: Feature Extraction",
    "section": "2.3 PCA Algorithm",
    "text": "2.3 PCA Algorithm"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_temp.html#a-simple-example",
    "href": "Courses/mlwr2_2025/l1/l1_temp.html#a-simple-example",
    "title": "Lecture 1: Feature Extraction",
    "section": "2.4 A Simple Example",
    "text": "2.4 A Simple Example"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_temp.html#hwd-data-example",
    "href": "Courses/mlwr2_2025/l1/l1_temp.html#hwd-data-example",
    "title": "Lecture 1: Feature Extraction",
    "section": "3.1 HWD Data Example",
    "text": "3.1 HWD Data Example\nHere, we will apply PCA to a larger dataset—the Handwritten Digit (HWD) dataset, a benchmark dataset in machine learning. This example will illustrate and help us grasp a deeper aspect of PCA. This profound idea not only aids in better understanding feature extraction but will also assist us later in quickly comprehending the essence of artificial neural networks and deep learning.\n\n3.1.1 HWD Dataset\nFirst, let’s introduce HWD dataset. The Handwritten Digit dataset consists of around 7291 images of digits ranging from 0 to 9.\n\ndat = read.table(gzfile(\"zip.train.gz\"))\ndat = as.matrix(dat)\ndim(dat)\n\n[1] 7291  257\n\ndat[1, 1:10]\n\n    V1     V2     V3     V4     V5     V6     V7     V8     V9    V10 \n 6.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.631  0.862 \n\n\nAs you can see, the HWD dataset has been imported and named dat. This dataset contains 257 variables and 7291 cases (images). The first variable is the number presented by the image, for example, the first row in dat presents number 6. Each image is represented as a 16 by 16 pixel grid, flattened into a vector of 256 values (one for each pixel). Therefore, each row in the dataset corresponds to one such vector, effectively capturing the pixel intensity of a single digit. By reshaping the 256-dimensional vector back into a 16x16 matrix, the original digit image can be reconstructed and visualized, for example\n\nX = dat[which(dat[,1] == 4), -1]\ny = dat[which(dat[,1] == 4), 1]\ndim(X)\n\n[1] 652 256\n\n\n\ntemp = matrix(X[8,256:1],16,16,byrow=T)[,16:1]\npar(pin = c(4, 4))\nimage(t(temp), col=cus_col(256), frame = T, axes = F)\ngrid(nx = nrow(temp), ny = ncol(temp), col = \"black\", lty = \"dotted\")\n\n\n\n\n\n\n\n\n\nn = dim(X)[1]\nset.seed(8312)\nID = sample(1:n, 24)\npar(mfrow = c(4,6), mar = rep(1,4))\nfor(i in ID){\n  temp= matrix(X[i,256:1],16,16,byrow=T)[,16:1]\n  image(t(temp), col=cus_col(256), frame = T, axes = F)\n  title(paste(\"ID: \", i))\n}\n\n\n\n\n\n\n\n\n\n\n3.1.2 PCA on HWD Data and Analysis\n\nmean_image = colMeans(X)\ntemp = matrix(mean_image[256:1],16,16,byrow=T)[,16:1]\npar(pin = c(4, 4))\nimage(t(temp), col=cus_col(256), frame = T, axes = F)\n\n\n\n\n\n\n\n\n\nS = cov(X)\npca_res = eigen(S)\nQ = pca_res$vectors\ndim(Q)\n\n[1] 256 256\n\n\n\nsum((X[1,]-mean_image)*Q[,1])\n\n[1] 3.19266\n\n\n\nn = dim(X)[1]\nZ = (X - matrix(rep(mean_image, n),n,256,byrow = T))%*%Q\ndim(Z)\n\n[1] 652 256\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2,3), mar=2*c(1,1,1,1)) \nfor(i in 1:6){\n  temp = matrix(Q[256:1,i],16,16,byrow=T)[,16:1]\n  image(t(temp), col=cus_col(256), frame = T, axes = F)\n  title(paste0(\"Weights for PC\", i))\n}"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_temp.html#image-reconstuction-view",
    "href": "Courses/mlwr2_2025/l1/l1_temp.html#image-reconstuction-view",
    "title": "Lecture 1: Feature Extraction",
    "section": "3.2 Image Reconstuction View",
    "text": "3.2 Image Reconstuction View\n\n\n\n\n\n\npar(mfrow = c(2,3), mar=2*c(1,1,1,1))\nfor(i in 251:256){\n  temp = matrix(Q[256:1,i],16,16,byrow=T)[,16:1]\n  image(t(temp), col=cus_col(256), frame = T, axes = F)\n}"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_temp.html#features-v.s.-individuals",
    "href": "Courses/mlwr2_2025/l1/l1_temp.html#features-v.s.-individuals",
    "title": "Lecture 1: Feature Extraction",
    "section": "4.1 Features V.S. Individuals",
    "text": "4.1 Features V.S. Individuals"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_temp.html#feature-extraction-in-machine-learning",
    "href": "Courses/mlwr2_2025/l1/l1_temp.html#feature-extraction-in-machine-learning",
    "title": "Lecture 1: Feature Extraction",
    "section": "4.2 Feature Extraction in Machine Learning",
    "text": "4.2 Feature Extraction in Machine Learning"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3.html",
    "href": "Courses/mlwr2_2025/l3/l3.html",
    "title": "Lecture 3: ANN and Deep Learning",
    "section": "",
    "text": "Next, let’s review what we mean by a machine learning model first.\n\n\n\n\n\n\n\nModel/Machine\n\n\n\n\n\n\n\nThey are right. Essentially, any machine learning model can be understood as a transformation \\(f\\) that converts feature variables into predictions. The type of model is determined by \\(f\\), while its specific characteristics are controlled by parameters. In other words, a successful machine learning model consists of the right model class \\(f\\) plus the appropriate parameters \\(w\\)."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2.html",
    "href": "Courses/mlwr2_2025/l2/l2.html",
    "title": "Lecture 2: Another Perspective on PCA",
    "section": "",
    "text": "In the previous lecture, we introduced the concept of feature extraction, leading to a data-driven feature extraction method—PCA. In this lecture, we will take a deeper look at this algorithm from a different perspective."
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1.html#what-is-pca",
    "href": "Courses/mlwr2_2025/l1/l1.html#what-is-pca",
    "title": "Lecture 1: Feature Extraction and PCA",
    "section": "2.1 What is PCA?",
    "text": "2.1 What is PCA?\n\n\n\n\nWhat is PCA? I took this picture when I was waiting for traffic lights close to Stroa Coop at Tomtebo in Umeå in January 2021. What perfect timing! Being stopped by traffic lights isn’t always a bad thing.\n\n\n\nPCA is a linear feature extraction tool. Let’s first provide the definition:\n\nPCA is a  linear  numerical method for creating a relatively smaller set of mutually  orthogonal   new variables  from the original dataset and the most  information  can be preserved in the new dataset.\n\nNext, let me explain each highlighted key words in the definition.\nNew variables and Linear: The term “new variable” is easy to understand. PCA is a type of feature extraction method, and the results of feature extraction are essentially new variables. However, this feature extraction, or the map \\(g()\\), is not arbitrary; we constrain it to be linear. In other words, all the new variables must satisfy the following equation:\n\\[\n  g_{\\textbf{w}}(\\textbf{x}) = w_1x_1 + w_2x_2 + \\dots + w_px_p\n\\] where \\(\\textbf{x}\\) expresses all the \\(p\\) original feature variables, and \\(\\textbf{w}\\) contains all the \\(p\\) coefficients. A straightforward example is the average. The average is a widely used method of summarizing information in real-life situations. It is the time to awkwardly show the generation gap. For example, in old-style music competitions, a singer would typically receive scores from several judges after their performance. Then, the host would say, “Remove the highest score, remove the lowest score, and the final score is……” The final score is just the average. More precisely, it is the truncated mean.\n\n\n\n\n\n\n\nHere, I want to emphasize something for you. To make it more memorable, let me start with a sad memory.\n\nSad memories: In middle school, I was very good at mathematics and physics, and also terrible at English and literature. The end of every exam was always the most awkward moment for me. Say I got 100 scores both for mathematics and physics, but 0 scores both for English and literature. My teacher simply informed the average score as the overall evaluation of my study to my parents. I guess she used the following formula \\[\n  \\frac{1}{4} \\text{Math} + \\frac{1}{4} \\text{Physics}+ \\frac{1}{4} \\text{English} + \\frac{1}{4} \\text{Literature}\n\\] Obviously, I was hopeless. However, my smart mother courageously stepped up, she simply adjusted the coefficients of the feature extraction function, \\[\n  \\frac{1}{2} \\text{Math} + \\frac{1}{2} \\text{Physics}+ 0 \\cdot \\text{English}+ 0 \\cdot \\text{Literature}\n\\] and told me that you are actually great!\n\nA good choice of coefficients not only can save a young people but also leads to a informative new variable for different purposes. This sad story highlights the role of coefficients in the feature extraction function. In one word, different coefficients lead to different information.\n\n\n\n\n\n\n\nI would like to use the above picture to close the discussion about this keyword. Essentially, the extracted feature is just the weighted sum of original features, while the weighted sum is called linear combination in linear algebra and the geometry meaning of linear combination is projection. I strongly suggest you read about mathematical projection if you are not familiar with this concept.\nThis hand shadow game is a good example. If you don’t know it, just looking at the two hands won’t immediately tell you what the performer wants to show the audience. But once the light casts a shadow, the image of a dog becomes clear. The basic idea of feature extraction is similar—by using the right feature extraction function, useful information can be presented in a way that is easy for a computer to recognize. In one words, the new variable can be viewed as a shadow of the object (\\(\\textbf{x}\\)) from a proper direction (\\(\\textbf{w}\\)).\nInformation : Based on the discussion above, it’s not difficult to see that the goal of the PCA algorithm is to find a set of suitable coefficients to achieve feature extraction. But what does “suitable” coefficients mean? Are there specific criteria for this? To understand the answer to this question, we need to take a closer look at the key term “information”.\nIn statistics and information theory, we have many measures of information. In PCA, however, we use a simple and familiar measure to quantify how much information a variable contains, that is “Variance”. Let’s start with an interesting example.\nI will present two events—think about which one you would be more eager to share with your family or friends:\nEvent 1: This morning, the sun rose in the east.\nEvent 2: NASA has just admitted to observing UFOs over the past 10 years.\nI think you already have the answer. No rush, let me and my wife simulate this scenario, as shown in the picture below.\n\n\n\n\n\n\n\nOn the left, my wife immediately saw through my trick of avoiding cooking at home. On the right, she was genuinely shocked by the news—although she still figured out my trick five minutes later. For us, the amount of information in a message depends on how surprising it is. Essentially, information amount is roughly equal to the degree of surprise.\nAnother example, I tell two students, “You can pass this exam.” The first student has prepared well and is confident in their answers, while the second student didn’t do well and feels uncertain. Clearly, the amount of information my message carries is completely different for them. Therefore, the degree of surprise in a message depends on the uncertainty of the event it describes. In statistics, uncertainty is usually measured by variance.\nLet me use one last example to convince you. Suppose we have an epidemiological dataset about cervical cancer, which includes age, gender, BMI, and various health indicators. Now, which variable do we absolutely not need? Think about what the variance of this variable would be.\nOrthogonal : At this point, we’ve basically understood the core idea of how PCA extracts new variables. However, there’s one more thing to clarify: for a dataset containing many variables, we usually extract a set of new variables. The PCA algorithm has a specific requirement for the relationship between these extracted variables, which is orthogonality. Simply put, orthogonality means that there is no linear relationship between the extracted variables, meaning their covariance is zero. We’ll see this more clearly in a concrete example later."
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1.html#pca-problem",
    "href": "Courses/mlwr2_2025/l1/l1.html#pca-problem",
    "title": "Lecture 1: Feature Extraction and PCA",
    "section": "2.2 PCA Problem",
    "text": "2.2 PCA Problem\nWith all this groundwork laid out, it becomes much easier to understand how PCA extracts variables from the original dataset. Simply put, PCA first aims to find a set of coefficients to calculate new variables, and this set of coefficients is designed to maximize the variance of the extracted new variables. Suppose, we have \\(p\\) variables in a dataset. \\[\n  \\max_{\\textbf{w}} \\left\\{ \\text{Var} \\left( \\underbrace{w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\dots + w_p \\cdot x_p}_{\\text{extracted feature}} \\right) \\right\\}\n\\]\nBy solving this optimization problem, we obtain an optimizer \\(\\textbf{w}_1\\) that is a set of coefficients for computing the first new variable. We call \\(\\textbf{w}_1\\) the first Principal Component weights (PC weithgs), and the variable calculated using these coefficients is commonly known as the first principal component. Of course, these are just statistical terms. In machine learning, this is simply a feature extraction function obtained through an algorithm under certain constraints.\nAs mentioned earlier, we usually need to extract a series of new variables from the original dataset to replace the old ones, achieving dimensionality reduction. Finding the second set of coefficients is not much different from the previous problem—we still aim to maximize the variance of the resulting variable. However, the key difference is that we need to add a constraint to ensure that we do not obtain the same first PC weights again. This constraint is what we call orthogonality before.\n\\[\n  \\max_{\\textbf{w}: \\textbf{w} \\perp \\textbf{w}_1} \\left\\{ \\text{Var} \\left( \\underbrace{w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\dots + w_p \\cdot x_p}_{\\text{extracted feature}} \\right) \\right\\}\n\\] Some linear algebra knowledge is needed to fully understand \\(\\textbf{w} \\perp \\textbf{w}_1\\), but you can ignore the details for now and just remember two key points:\n\nThis condition prevents us from obtaining the first set of PC weights again.\n\nThe second principal component (new variable) obtained this way will be linearly uncorrelated with the first principal component.\n\nOf course, if needed, we continue searching for the third set of PC weights. This time, we need to add two orthogonality constraints to ensure it remains uncorrelated with both the first and second principal components, that is \\(\\textbf{w} \\perp \\textbf{w}_1\\) and \\(\\textbf{w} \\perp \\textbf{w}_2\\). By following this approach, we can continue finding more PC weights. In fact, we can obtain up to \\(p\\) new variables—yes, the same number as in the original dataset.\nYou might be wondering: How does this achieve dimensionality reduction? Let’s explore this in the next section with a concrete example."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#a-binary-classification-problem",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#a-binary-classification-problem",
    "title": "My Yggdrasil",
    "section": "A binary classification problem",
    "text": "A binary classification problem"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#assign-a-label-to-the-purple-star-point",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#assign-a-label-to-the-purple-star-point",
    "title": "My Yggdrasil",
    "section": "Assign a label to the purple star point",
    "text": "Assign a label to the purple star point"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#step-1-find-the-3-nearest-neighbors",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#step-1-find-the-3-nearest-neighbors",
    "title": "My Yggdrasil",
    "section": "Step 1: find the 3 nearest neighbors",
    "text": "Step 1: find the 3 nearest neighbors"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#step-2-preditc-it-as-the-color-of-majority",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#step-2-preditc-it-as-the-color-of-majority",
    "title": "My Yggdrasil",
    "section": "Step 2: preditc it as the color of majority",
    "text": "Step 2: preditc it as the color of majority"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#another-example",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#another-example",
    "title": "My Yggdrasil",
    "section": "Another example",
    "text": "Another example"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#step-1-find-the-3-nearest-neighbors-1",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#step-1-find-the-3-nearest-neighbors-1",
    "title": "My Yggdrasil",
    "section": "Step 1: find the 3 nearest neighbors",
    "text": "Step 1: find the 3 nearest neighbors"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#step-2-preditc-it-as-the-color-of-majority-1",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#step-2-preditc-it-as-the-color-of-majority-1",
    "title": "My Yggdrasil",
    "section": "Step 2: preditc it as the color of majority",
    "text": "Step 2: preditc it as the color of majority"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#if-apply-knn-to-all-possible-points-then",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#if-apply-knn-to-all-possible-points-then",
    "title": "My Yggdrasil",
    "section": "If apply KNN to all possible points, then…",
    "text": "If apply KNN to all possible points, then…"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#we-get-the-decision-boundary",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#we-get-the-decision-boundary",
    "title": "My Yggdrasil",
    "section": "We get the decision Boundary",
    "text": "We get the decision Boundary"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_tuning.html#section",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_tuning.html#section",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Step 0: Determine the hypothetical model and its hyper-parameters"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_tuning.html#section-1",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_tuning.html#section-1",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Step 1: Prepare a set of candidate values for hyper-parameters"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_tuning.html#section-2",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_tuning.html#section-2",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Step 2: Set up all the candidate models"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_tuning.html#section-3",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_tuning.html#section-3",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Step 2: Feed the data and train the models"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_tuning.html#section-4",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_tuning.html#section-4",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Step 3: Record the model performance"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_tuning.html#section-5",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_tuning.html#section-5",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Step 4: Model with the best performance as the final choice"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section",
    "title": "My Yggdrasil",
    "section": "",
    "text": "A binary classification problem"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-1",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-1",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Assign a label to the purple star point"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-2",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-2",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Step 1: find the 3 nearest neighbors"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-3",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-3",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Step 2: preditc it as the color of majority"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-4",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-4",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Another example"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-5",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-5",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Step 1: find the 3 nearest neighbors"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-6",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-6",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Step 2: preditc it as the color of majority"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-7",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-7",
    "title": "My Yggdrasil",
    "section": "",
    "text": "If apply KNN to all possible points, then…"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-8",
    "href": "Courses/c_mlwr1_2024/l6/l6_slides_knn.html#section-8",
    "title": "My Yggdrasil",
    "section": "",
    "text": "We get the decision Boundary"
  },
  {
    "objectID": "Courses/c_mlwr1_2024/Discussion of Lectures 7.html",
    "href": "Courses/c_mlwr1_2024/Discussion of Lectures 7.html",
    "title": "Discussion of Lecture 7",
    "section": "",
    "text": "What is the main idea of classifiers based on regular linear regression model?\nWhat is the limitation of classifiers based on regression model?\nWhat are the similarities and differences between linear regression and logistic regression?\nHow to build a classifier based on logistic regression models?\nIs the classifier based on logistic regression a linear classifier or nonlinear classifier?\nHow to determine the decision boundary?\nCross-entropy loss function and likelihood function."
  },
  {
    "objectID": "Courses/c_mlwr1_2024/Discussion of Lectures 5.html",
    "href": "Courses/c_mlwr1_2024/Discussion of Lectures 5.html",
    "title": "Discussion of Lectures 5",
    "section": "",
    "text": "Do you remember the names of prediction and prediction errors in the regression analysis?\nMSE V.S. Likelihood function V.S. Loss function\nCan you describe the basic idea of nonlinear extension in simple words?\nWhat are the main challenges of this idea?\nCan you explain what is overfitting problem?\nWhat are the main reasons?\nHow do you interpret overfitting problem from variance-bias trade off point of view?"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#section",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#section",
    "title": "My Yggdrasil",
    "section": "",
    "text": "At home, my wife and I often use this recipe to make hummus.\nMotivating question: my wife has a secret recipe for a new flavor called ’Umemus’, and wants to secretly show me. Any solution?"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#section-1",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#section-1",
    "title": "My Yggdrasil",
    "section": "",
    "text": "At home, my wife and I often use this recipe to make hummus.\nMy wife has a secret recipe for a new flavor called ’Umemus’, and wants to secretly show me.\nAny solution?"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#motivating-question",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#motivating-question",
    "title": "My Yggdrasil",
    "section": "Motivating Question",
    "text": "Motivating Question\n\n\n\n\n\nAt home, my wife and I often use this recipe (RHS) to make hummus.\n\n\n\n\nOne day, my wife got a secret recipe for a new flavor called ’Ummus’. She wants me to try it tonight and needs me to prepare the ingredients in advance.\n\n\n\n\nHowever, using any communication tool comes with security risks. So how can she safely pass this secret recipe to me?\n Any solution? Think about it!"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#slide-title",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#slide-title",
    "title": "My Yggdrasil",
    "section": "Slide Title",
    "text": "Slide Title\n\n\nItems List\n\nItem 1 {.fragment}\nItem 2 {.fragment}\nItem 3 {.fragment}"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#our-solution",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#our-solution",
    "title": "My Yggdrasil",
    "section": "Our solution",
    "text": "Our solution\n\n\nActually, this is how we store the recipe at home. First, we list all possible ingredients in a fixed order."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#our-solution-1",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#our-solution-1",
    "title": "My Yggdrasil",
    "section": "Our solution",
    "text": "Our solution\n\n\nThen, we fill in the required amounts in the corresponding cells. For example, the classic flavor requires these numbers."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#our-solution-2",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#our-solution-2",
    "title": "My Yggdrasil",
    "section": "Our solution",
    "text": "Our solution\n\n\nUsing the same method, we can record the amounts for two other flavors as well."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#our-solution-3",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#our-solution-3",
    "title": "My Yggdrasil",
    "section": "Our solution",
    "text": "Our solution\n\n\nFor the ingredients that are not needed, we simply fill in with 0."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#our-solution-4",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe.html#our-solution-4",
    "title": "My Yggdrasil",
    "section": "Our solution",
    "text": "Our solution\n\n\nThis way, as long as my wife gives me this sequence of numbers 1, 0, 3, 2, 1, 0, 1, 1/4, 0, 1, she can pass the ‘Hummus’ recipe to me and no one understands it.\nOh no, it looks like I’ve already leaked our secret recipe…"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe_1.html#section",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe_1.html#section",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Wait, this is essentially a dataset! Here, we have three cases, one target variable (flavor), and 10 feature variables."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe_1.html#section-1",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe_1.html#section-1",
    "title": "My Yggdrasil",
    "section": "",
    "text": "This is exactly the machine learning philosophy we mentioned in the first part. When ordering food, making decisions without tasting the flavor (the target variable, the expensive information) is risky. However, we can make predictions based on our experience (the model). Of course, not all ingredients (feature variables) are equally useful."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe_1.html#section-2",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe_1.html#section-2",
    "title": "My Yggdrasil",
    "section": "",
    "text": "For example, olive oil and garlic are clearly important pieces of information for identifying the classic flavor."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe_1.html#section-3",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe_1.html#section-3",
    "title": "My Yggdrasil",
    "section": "",
    "text": "For example…"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe_1.html#section-4",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe_1.html#section-4",
    "title": "My Yggdrasil",
    "section": "",
    "text": "For example… In other words, as long as we have good feature variables, training a good classifier becomes a piece of cake."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe_1.html#section-5",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe_1.html#section-5",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Obviously, chickpeas and lemon provide no useful information. This aligns with the principle we discussed in the previous lecture. These two variables have no variation in the dataset, so they don’t contribute any information."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe_1.html",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe_1.html",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Wait, this is essentially a dataset! Here, we have three cases, one target variable (flavor), and 10 feature variables."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe_2.html#section",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe_2.html#section",
    "title": "My Yggdrasil",
    "section": "",
    "text": "I’m sure everyone knows how to use a recipe. Otherwise, let me show you,"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe_2.html#section-1",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe_2.html#section-1",
    "title": "My Yggdrasil",
    "section": "",
    "text": "If you tend to mess things up—like I do—then it’s best to lay out all the possible ingredients first. Then, simply follow the numbers in the recipe, measuring each amount accordingly and adding it to the bowl."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe_2.html#section-2",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe_2.html#section-2",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Then mix, mix, and mix thoroughly…"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_recipe_2.html#section-3",
    "href": "Courses/mlwr2_2025/l2/l2_slides_recipe_2.html#section-3",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Congratulations! You’ve got the classic flavor!"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2.html#hwd-dataset",
    "href": "Courses/mlwr2_2025/l2/l2.html#hwd-dataset",
    "title": "Lecture 2: Another Perspective on PCA",
    "section": "2.1 HWD Dataset",
    "text": "2.1 HWD Dataset\nFirst, let’s introduce HWD dataset. The Handwritten Digit dataset consists of around 7291 images of digits ranging from 0 to 9.\n\ndat = read.table(gzfile(\"zip.train.gz\"))\ndat = as.matrix(dat)\ndim(dat)\n\n[1] 7291  257\n\ndat[1, 1:10]\n\n    V1     V2     V3     V4     V5     V6     V7     V8     V9    V10 \n 6.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.631  0.862 \n\n\nAs you can see, the HWD dataset has been imported and named dat. This dataset contains 257 variables and 7291 cases (images). The first variable is the number presented by the image, for example, the first row in dat presents number 6. Each image is represented as a 16 by 16 pixel grid, flattened into a vector of 256 values (one for each pixel). Therefore, each row in the dataset corresponds to one such vector, effectively capturing the pixel intensity of a single digit. Next, to achieve better results, we will only focus on all the images that represent the number 4.\n\nX = dat[which(dat[,1] == 4), -1]\ny = dat[which(dat[,1] == 4), 1]\ndim(X)\n\n[1] 652 256\n\n\nBy reshaping the 256-dimensional vector back into a 16x16 matrix, the original digit image can be reconstructed and visualized。 As you can see, on the left is the number 4 rendered using the 8th row of X. On the right are the images of the number 4 corresponding to the first 24 rows of X. I don’t know what you think, but I feel the 8th image of the handwritten number 4 looks pretty good, while the others are a bit hard to comment on."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2.html#pca-on-hwd-data-and-analysis",
    "href": "Courses/mlwr2_2025/l2/l2.html#pca-on-hwd-data-and-analysis",
    "title": "Lecture 2: Another Perspective on PCA",
    "section": "2.2 PCA on HWD Data and Analysis",
    "text": "2.2 PCA on HWD Data and Analysis\nNow, we perform PCA method on the data matrix X of images of my favorite number. As the same as what we did with iris data last time, we will calculate the covariance matrix first and then perform eigenvalues decomposition on it.\n\nS = cov(X)\npca_res = eigen(S)\nW = pca_res$vectors\nhead(pca_res$values, 10)\n\n [1] 13.536233  9.838261  8.111957  5.918451  4.211639  3.767007  3.504070\n [8]  3.248869  2.781496  2.459582\n\n\nAbove, all the PC weights are stored in matrix W, with each column representing a set of PC weights. Additionally, we can see that the information corresponding to the first ten sets of PC weights is automatically arranged in descending order. Let’s calculate the value of the first PC (extracted feature) of the first image X[1,]\n\nsum(X[1,]*W[,1])\n\n[1] 1.575447\n\n\nYou can calculate all new variables for all images using a two-layered for loop, for example:\nZ = X # the new data matrix\nfor (i in 1:nrow(X)) {\n  for (j in 1:ncol(X)) {\n    Z[i, j] = sum(X[i, ]*W[, j])\n  }\n}\nIf you are familiar with matrix computation, the new dataset can be obtained in a single command:\n\nZ = X%*%W\ndim(Z)\n\n[1] 652 256\n\nZ[1:5,1:5]\n\n          [,1]     [,2]       [,3]       [,4]       [,5]\n[1,]  1.575447 3.078215  6.2773087  1.9985827  1.0902283\n[2,] -6.735149 4.544327 -0.4849858 -0.5599420  0.2861956\n[3,] -6.639667 2.193575  2.2740280 -0.4521205 -4.2699895\n[4,] -2.831801 2.746806 -0.7435662  3.5391657 -1.1454160\n[5,] -1.601569 7.037529  2.5278122  3.7237227 -0.5490663\n\n\nSo how do we know if our PCA was successful? We can compare the covariance of the original dataset and the new dataset, as shown in the figure below. On the left, we have the heatmap of the covariance matrix for the original dataset. We can see that the original dataset’s covariance matrix has a complex structure—not only do the variances (information content) of different variables show no obvious differences, but all variables exhibit intricate correlations, making it difficult to make selections. On the right, we see the covariance matrix of the new dataset. Its structure is clear, and the amount of information contained decreases rapidly, making it much easier to choose variables and achieve dimensionality reduction.\n\n\n\n\n\nNow, let’s focus our attention on matrix W.\n\ndim(W)\n\n[1] 256 256\n\n\nAs expected, each set of PC weights contains 256 values, which is exactly the number of pixel values in an image. This means that each set of PC weights can also be viewed as an image of the same size as the original images of numbers. So, let’s try placing the first set of PC weights into a \\(16\\times 16\\) matrix and display it as an image to see what happens.\n\n\n\n\n\n\n\n\n\n\n\nThis seems to resemble some abstract form of the digit 4, or perhaps it emphasizes the vertical stroke on the right side of a 4. Interesting, isn’t it? Let’s continue applying the same process and display the first six sets of PC weights.\n\n\n\n\n\n\n\n\n\n\n\nIt seems that each set of PC weights represents an abstract version of the digit 4, emphasizing a specific part of the number. Fascinating!\nNow, let’s incorporate this information into our newly obtained dataset.\n\n\n\n\n\nThis is our new dataset Z, where each row represents an image, and each column represents an extracted feature. Wait! Doesn’t this look a lot like our previous recipe data matrix? Can we make the following conjecture: in the problem we mentioned earlier, the things related to variables are actually the weights, and we can treat these weights just like ingredients. In other words, can we reconstruct the images corresponding to numbers by using the recipe method?"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2.html#image-reconstuction-view",
    "href": "Courses/mlwr2_2025/l2/l2.html#image-reconstuction-view",
    "title": "Lecture 2: Another Perspective on PCA",
    "section": "2.3 Image Reconstuction View",
    "text": "2.3 Image Reconstuction View\nWe are about to open the door to a new world. Let’s slightly modify the recipe formula mentioned in the first section to fit our digit image recipe.\n\n\n\n\n\nNext, I will use this formula with the first 30 PC weights to reconstruct the 8th image.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe reconstructed image on the left, though blurry, seems to capture the main features of the real image on the right. But don’t forget, we’ve only added 30 types of “ingredients.” In the animation below, you’ll see that as more “ingredients” are added, the reconstructed image becomes closer and closer to the original. Eventually, when all the “ingredients” in the “recipe” are used, the original digit 4 appears in its full form.\n\nAnimation-Demo of PCA Image Reconstruction\n\n  Replay\n\n\n\nFrom a culinary perspective, perfectly replicating a master chef’s creation is crucial, so you shouldn’t miss any ingredients in the recipe. However, for digit recognition, reconstructing an image identical to the original isn’t actually necessary. In fact, for a computer, having so many pixels might just be an unnecessary burden. Let’s take a look at the last six “ingredients” in the digit image “recipe,” which are the last six sets of PC weights.\n\n\n\n\n\n\n\n\n\n\n\nThis is the principle behind reconstructing images using PC weights. Next, we will use this principle to reinterpret PCA methods. Please note that this way of understanding is crucial for grasping neural networks and even deep learning. Before we continue, let’s take a moment to think about a small question to test our level of understanding.\n\nQuiz: Do you know what these so-called ingredients are called in linear algebra? The original dataset can also be seen as a ‘recipe’. So, what do the ingredients in that ‘recipe’ look like?"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section-8",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section-8",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Warning! The upcoming visuals may be too graphic—please make sure no children are watching with you."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-4",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-4",
    "title": "My Yggdrasil",
    "section": "",
    "text": "I’ll toss all the data into it."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section-2",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section-2",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Compared to the real London Bridge, we only need a limited set of LEGO pieces to build it."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section-3",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section-3",
    "title": "My Yggdrasil",
    "section": "",
    "text": "And all the famous land marks in the world can be reconstructed using these limited types of pieces. From the perspective of dimensionality reduction, LEGO does a great job!"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section-4",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section-4",
    "title": "My Yggdrasil",
    "section": "",
    "text": "This is a list of all the pieces in a LEGO toy set. Instead of rushing to the next page, take a guess—what will we get once it’s fully assembled?"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section",
    "title": "My Yggdrasil",
    "section": "",
    "text": "As a LEGO designer, I’ve been assigned a project to create an animal series."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-1",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-1",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Essentially, these adorable animals form my original dataset, \\(\\textbf{X}\\)."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-2",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-2",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Test yourself: think about the dimension of this data matrix."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-3",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-3",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Next, I’ll bring out my powerful PCA blender!"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-5",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-5",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Warning! The upcoming visuals may be too gory—please make sure no children are watching with you."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-6",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-6",
    "title": "My Yggdrasil",
    "section": "",
    "text": "After a good mix, you’ll first get a set of possible LEGO brick types. From the PCA perspective, these potential brick types are essentially the PCA weights, \\(\\textbf{w}_j\\), you need. More broadly, they are feature extraction functions, \\(g_{\\textbf{w}}(\\textbf{X})\\), if you consider other kind of feature extraction algorithm/machine."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-7",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-7",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Next, the various animals will be transformed into a new dataset through the effect of these feature extraction functions."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-8",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-8",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Of course, there are also some… I mean that would be the useless information."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-9",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-9",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Then, the kids can use the LEGO pieces I’ve designed to create any animals they like, like a cute little turtle."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-10",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-10",
    "title": "My Yggdrasil",
    "section": "",
    "text": "If you use another set of extracted feature values, you can also get a zebra."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Let’s first take a look at this photo. Can you recognize it?"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section-1",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section-1",
    "title": "My Yggdrasil",
    "section": "",
    "text": "How about this one? Yes, it is also a London bridge."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section-5",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section-5",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Yes, just as you guessed—it’s a helicopter! This was my eldest son’s first Creator set when he was not even three years old. Alright, I must admit, I was also quite eager to try out the LEGO set myself."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section-6",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego.html#section-6",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Let’s take a closer look at this LEGO “data.” I bet you’ve already noticed some extracted features. It’s quite obvious that these pieces can be perfectly assembled into a propeller."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-11",
    "href": "Courses/mlwr2_2025/l2/l2_slides_lego1.html#section-11",
    "title": "My Yggdrasil",
    "section": "",
    "text": "As long as the data is correct, making a lion is no problem either."
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_home.html",
    "href": "Courses/mlwr2_2025/l1/l1_home.html",
    "title": "Lecture 1: Feature Extraction and PCA",
    "section": "",
    "text": "In this lecture, we first introduced an important concept in machine learning: feature extraction. Then, we presented Principal Component Analysis (PCA), a linear solution that is essential in learning data-driven methods. After discussing its principles and implementation, we concluded this session with a discussion."
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_home.html#lecture-notes",
    "href": "Courses/mlwr2_2025/l1/l1_home.html#lecture-notes",
    "title": "Lecture 1: Feature Extraction and PCA",
    "section": "Lecture notes:",
    "text": "Lecture notes:\n\nRead the integrated notes: here\nRead the pagination notes: here"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_home.html#reading-guidelines-of-textbook",
    "href": "Courses/mlwr2_2025/l1/l1_home.html#reading-guidelines-of-textbook",
    "title": "Lecture 1: Feature Extraction and PCA",
    "section": "Reading guidelines of textbook",
    "text": "Reading guidelines of textbook\nRead section 12.2 and Lab 12.5.1"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_home.html",
    "href": "Courses/mlwr2_2025/l3/l3_home.html",
    "title": "Lecture 3: Artificial Neural Networks and Deep Learning",
    "section": "",
    "text": "In this lecture,"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_home.html#lecture-notes",
    "href": "Courses/mlwr2_2025/l3/l3_home.html#lecture-notes",
    "title": "Lecture 3: Artificial Neural Networks and Deep Learning",
    "section": "Lecture notes:",
    "text": "Lecture notes:\n\nRead the integrated notes: here"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_home.html#reading-guidelines-of-textbook",
    "href": "Courses/mlwr2_2025/l3/l3_home.html#reading-guidelines-of-textbook",
    "title": "Lecture 3: Artificial Neural Networks and Deep Learning",
    "section": "Reading guidelines of textbook",
    "text": "Reading guidelines of textbook\n\nCourse Homepage"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_home.html",
    "href": "Courses/mlwr2_2025/l2/l2_home.html",
    "title": "Lecture 2: Another Perspective on PCA",
    "section": "",
    "text": "In this lecture,"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_home.html#lecture-notes",
    "href": "Courses/mlwr2_2025/l2/l2_home.html#lecture-notes",
    "title": "Lecture 2: Another Perspective on PCA",
    "section": "Lecture notes:",
    "text": "Lecture notes:\n\nRead the integrated notes: here\nRead the pagination notes: here"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_home.html#reading-guidelines-of-textbook",
    "href": "Courses/mlwr2_2025/l2/l2_home.html#reading-guidelines-of-textbook",
    "title": "Lecture 2: Another Perspective on PCA",
    "section": "Reading guidelines of textbook",
    "text": "Reading guidelines of textbook\n\nCourse Homepage"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1.html#discussion",
    "href": "Courses/mlwr2_2025/l1/l1.html#discussion",
    "title": "Lecture 1: Feature Extraction and PCA",
    "section": "4. Discussion",
    "text": "4. Discussion\nLet’s conclude this lecture with a discussion. We call PCA a data-driven solution, meaning it does not rely on any prior information. However, it is important to emphasize that PCA is merely a linear feature extraction method. The concept of linearity is straightforward: all extracted feature variables are just weighted sums of the original variables. But how can we understand linearity from a geometric perspective? Let’s explore this idea with the following toy example.\n\n\n\n\n\n\n\n\n\n\n\nIn this example, we have two variables, or two dimensions, represented by the blue coordinate axes. The red and orange lines represent two sets of PC weights, meaning the new variables are the projections of all the observations onto these lines. This requires a bit of imagination. Imagine that all the observations fall perpendicularly onto the red line, which gives us the first new variable. Similarly, by projecting onto the orange direction, we get the second new variable. It’s easy to see that the variance (or information) along the red line is much higher than that along the orange line. Therefore, we can discard the orange variable, effectively reducing the dimensionality while preserving most of the information. Therefore, the linearity of PCA lies in the fact that we are simply re-examining the original feature space from different angles. In other words, we do not disturb the relative positions of the original observations.\nThe advantage of PCA is that the algorithm is simple and can be easily understood from a geometric perspective, but its limitations are quite apparent as well. First, its feature extraction does not rely on any target information to guide it. The feature extraction process only depends on the covariance structure of the original data. Second, its feature extraction capability is quite limited. We can see that all possible feature extraction methods are determined by the covariance structure inherent in the data. At most, we can have as many feature extraction functions as there are original variables, but most of them will be useless. Therefore, PCA is NOT very flexible, and when the problem becomes complex, it often fails to provide an effective feature extraction solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA is like God rotating a globe, viewing what happens on this blue planet from different angles. When there’s a problem, God steps in to fix it. However, this gentle approach is linear. But when humanity presents too many complicated challenges, even God might be powerless, allowing the devil to intervene with non-linear methods in extreme ways to solve the problem.\nThis might not be the best analogy, especially since I’ve received warnings and restrictions while generating images with ChatGPT, but it is indeed quite vivid. So, let’s remember to take care of our beautiful blue planet. 🌍\n\n\nLecture 1 Homepage"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3.html#model-graphicalization",
    "href": "Courses/mlwr2_2025/l3/l3.html#model-graphicalization",
    "title": "Lecture 3: ANN and Deep Learning",
    "section": "2.1 Model Graphicalization",
    "text": "2.1 Model Graphicalization\nMathematicians always take a bird’s-eye view of the world, seeking out its most fundamental elements. This symbolic representation of a machine learning model is both concise and insightful, constantly reminding us of what truly matters.\nHowever, this notation is so minimalistic that it overlooks many details, while the earlier graphical representation is too cumbersome and impractical for expressing more complex models. Next, we introduce a more efficient graphical method.\n\n\n\n\n\n\n\nModel Representation"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3.html#is-pca-also-a-machine",
    "href": "Courses/mlwr2_2025/l3/l3.html#is-pca-also-a-machine",
    "title": "Lecture 3: ANN and Deep Learning",
    "section": "2.2 Is PCA also a machine?",
    "text": "2.2 Is PCA also a machine?\nOf course, the answer is positive. If we understand PCA through the concept of image reconstruction, it is indeed a “machine” (a machine learning model). It is a transformation where the input consists of all the original variables, and the output is the reconstructed original variables. The specific performance of the model depends on all the PC weights.\nThat being the case, let’s now apply the graphical representation to this model.\n\n\n\n\n\n\n\nPCA is also a Machine\n\n\n\n\n\n\n\nIsn’t it fascinating? PCA can be seen as a very special kind of “machine.” Rather than focusing on its output, we are more interested in its internal byproducts—the principal components (PCs), \\(\\textbf{Z}\\). This is mainly because our ultimate goal is to use the feature variables extracted by PCA to predict our predefined target variable. In other words, it is an intermediate step.\nBut do you remember the limitations of PCA that we mentioned in the first lecture? Yes, PCA is a linear feature extraction method, which means it has low flexibility. However, in complex problems, we often need more flexible nonlinear feature extraction methods to create new variables. So, can we improve PCA? Again, the answer is positive.\n\n\n\n\n\n\n\nNonlinear PCA: AutoEncoder"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3.html#traditional-model-vs-new-age-model",
    "href": "Courses/mlwr2_2025/l3/l3.html#traditional-model-vs-new-age-model",
    "title": "Lecture 3: ANN and Deep Learning",
    "section": "2.3 Traditional Model vs New Age Model",
    "text": "2.3 Traditional Model vs New Age Model\nNext, let’s discuss a broader question: the fundamental workflow to machine learning modeling. Yes, the process of building machine learning models follows a regular routine. However, deep learning marks a clear boundary where traditional and modern modeling approaches diverge significantly.\nSimply put:\n- Traditional modeling follows a two-step approach.\n- Modern modeling, especially in deep learning, follows an end-to-end approach.\nLet me explain in detail with the following slides.\n\n\n\n\n\n\n\nEnd to End Learning\n\n\n\n\n\n\n\nSo how can we implement this end-to-end approach in practice? Let’s return to a powerful nonlinear feature extraction model, the autoencoder.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\n\nThis model indeed resembles a large network, but what’s its connection to neurons? On the far left of the diagram is the simple model we discussed earlier. If we replace the input with the neurons from the previous layer and the output with new neurons, we obtain the basic unit of an ANN as shown in the middle of the diagram. Doesn’t this basic unit look quite similar to a neuron in neuroscience?\n\n\n\n\n\nPeople in computer science are indeed great at naming things. Cool names like random forest, support vector machine, and so on pop up all the time in machine learning. However, just like how random forest has nothing to do with a real forest, ANN bears little resemblance to neurons in true neuroscience, both in terms of shape, scale, and working principles. ANN is just a cool name. However, there are scientists who are now researching the use of hardware to replicate real neurons. In my opinion, that could truly be the hopeful light for the future of artificial intelligence. Even with large language models being so popular right now, I still have to say this."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3.html#on-the-top-of-mount-tai",
    "href": "Courses/mlwr2_2025/l3/l3.html#on-the-top-of-mount-tai",
    "title": "Lecture 3: ANN and Deep Learning",
    "section": "2.4 On the top of Mount Tai",
    "text": "2.4 On the top of Mount Tai\n\n\n\n\nThis is a Chinese landscape painting depicting the scenic beauty of Mount Tai (泰山). In China, there are many majestic mountains, but Mount Tai is regarded as the foremost among them. People believe that once you stand atop Mount Tai, all other peaks can be seen at a glance.\n\n\nWe have reached the destination of this journey. Just like standing upon Mount Tai, if you have understood the content above, then you have already grasped the fundamental principles of deep learning. Now, let’s take a closer look at our neural network model and name its elements.\n\n\n\n\n\n\n\nANN components:"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3.html#gradient-descent-algorithm",
    "href": "Courses/mlwr2_2025/l3/l3.html#gradient-descent-algorithm",
    "title": "Lecture 3: ANN and Deep Learning",
    "section": "3.2 Gradient Descent Algorithm",
    "text": "3.2 Gradient Descent Algorithm\n\n\n\n\n\n\n\nBasic Idea of GD algorithm:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDemo of GD algorithm:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA proper learning rate is essential:"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3.html#gd-for-training-ann",
    "href": "Courses/mlwr2_2025/l3/l3.html#gd-for-training-ann",
    "title": "Lecture 3: ANN and Deep Learning",
    "section": "3.3 GD for training ANN",
    "text": "3.3 GD for training ANN"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3.html#train-a-ann-in-r",
    "href": "Courses/mlwr2_2025/l3/l3.html#train-a-ann-in-r",
    "title": "Lecture 3: ANN and Deep Learning",
    "section": "3.4 Train a ANN in R",
    "text": "3.4 Train a ANN in R"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3.html#new-era-opportunities-and-challenges",
    "href": "Courses/mlwr2_2025/l3/l3.html#new-era-opportunities-and-challenges",
    "title": "Lecture 3: ANN and Deep Learning",
    "section": "4.1 New Era: Opportunities and Challenges",
    "text": "4.1 New Era: Opportunities and Challenges"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3.html#convoluntional-neural-network",
    "href": "Courses/mlwr2_2025/l3/l3.html#convoluntional-neural-network",
    "title": "Lecture 3: ANN and Deep Learning",
    "section": "4.2 Convoluntional Neural Network",
    "text": "4.2 Convoluntional Neural Network\n\n4.2.1 What is Convolution?\n\n\n4.2.2 Convolution Layer\n\n\nLecture 3 Homepage"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_6_ANN.html#section-3",
    "href": "Courses/mlwr2_2025/l3/l3_slides_6_ANN.html#section-3",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Congratulations! You have just unlocked the most basic deep learning architecture, the artificial neural network."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-1",
    "href": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-1",
    "title": "My Yggdrasil",
    "section": "",
    "text": "I taught myself to ski at the grand age of 35 using the theory of gradient.\nSo, what is gradient?"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section",
    "href": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section",
    "title": "My Yggdrasil",
    "section": "",
    "text": "I live in Umeå, near the Arctic Circle. Skiing is a basic survival skill over there."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-2",
    "href": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-2",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Gradient is information that can be obtained from a curve or surface function. Simply put, it tells you which direction is the steepest. In the image, the red arrow points in the opposite direction of the gradient, meaning it indicates the steepest downward direction."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-3",
    "href": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-3",
    "title": "My Yggdrasil",
    "section": "",
    "text": "If you align skis (blue arrow) with the red arrow, you will descend quickly. When the skis perpendicular to the slope, you can stand still on it. Along the way, you can use the gradient as a reference to adjust the direction of your skis and thus control speed."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-4",
    "href": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-4",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Congratulations, you can ski now!\nGradient not only helps us reach the bottom of the mountain safely but also guides us in finding the optimal value of an objective function."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-5",
    "href": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-5",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Here, I have a smooth curve and have marked the gradient values at six points along it. Among them, \\(w = -0.75\\) is a special point. At this point, we obtain the minimum value of the function, and the gradient value here is 0."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-6",
    "href": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-6",
    "title": "My Yggdrasil",
    "section": "",
    "text": "First, let’s examine the differences between the colors. In the red section, all gradient values are positive, and the purple point is always to its left (negative direction). In the blue region, we can draw the opposite conclusion. Therefore, the sign of the gradient tells us which direction to move in order to reach the lowest point."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-7",
    "href": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-7",
    "title": "My Yggdrasil",
    "section": "",
    "text": "In other words, at each position, we have a “guide” that tells us which direction to go in order to reach the destination. Now, how far should we move at each position? Let’s continue and see."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-8",
    "href": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-8",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Next, let’s compare the same color sections. In the blue section, you can see that the larger the absolute value of the gradient, the steeper the slope at that point. We can draw the same conclusion for the red section. So, the smaller the absolute value of the gradient, the closer you are to the target, and you should slow down as you approach it."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-9",
    "href": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html#section-9",
    "title": "My Yggdrasil",
    "section": "",
    "text": "In other words, when we want to find the minimum of a loss function, \\(L(w)\\), we can start from any point and, using the guidance of the gradient, ultimately reach the destination. This is the basic ideas of the gradient descent algorithm."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html",
    "href": "Courses/mlwr2_2025/l3/l3_slides_8_GD.html",
    "title": "My Yggdrasil",
    "section": "",
    "text": "I live in Umeå, near the Arctic Circle. Skiing is a basic survival skill over there."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section",
    "href": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section",
    "title": "My Yggdrasil",
    "section": "",
    "text": "As we mentioned in the first part of the course, a machine learning model—also known as a “machine”—is essentially a black box."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section-1",
    "href": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section-1",
    "title": "My Yggdrasil",
    "section": "",
    "text": "This machine works in a simple way: it takes input feature variables, predicts the target variable, and outputs the result."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section-2",
    "href": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section-2",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Typically, this black box is complex and difficult to interpret, but let’s assume that it is a simple linear model, as we introduced in the first part. Then, let’s open it."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section-3",
    "href": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section-3",
    "title": "My Yggdrasil",
    "section": "",
    "text": "All the linear models we have learned share a common characteristic: they first compute a weighted sum of the feature variables and then add a constant term."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section-4",
    "href": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section-4",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Then, depending on the nature of the problem and the model being used, we need to choose an appropriate function—called the activation function—to obtain the final prediction."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section-5",
    "href": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section-5",
    "title": "My Yggdrasil",
    "section": "",
    "text": "For example, in a regression problem, we can choose the identity function. If our machine is a logistic regression model, then the activation function is the logistic function."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section-6",
    "href": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section-6",
    "title": "My Yggdrasil",
    "section": "",
    "text": "BTW, do you remember any other activation functions?"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_1_model.html",
    "href": "Courses/mlwr2_2025/l3/l3_slides_1_model.html",
    "title": "My Yggdrasil",
    "section": "",
    "text": "As we mentioned in the first part of the course, a machine learning model—also known as a “machine”—is essentially a black box."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section-7",
    "href": "Courses/mlwr2_2025/l3/l3_slides_1_model.html#section-7",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Mathematicians might find such diagrams too cumbersome, so they strip away all unnecessary details and abstract only the most essential parts. As a result, they express a machine learning model like this:"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3.html#loss-function",
    "href": "Courses/mlwr2_2025/l3/l3.html#loss-function",
    "title": "Lecture 3: ANN and Deep Learning",
    "section": "3.1 Loss function",
    "text": "3.1 Loss function\n\n\n\n\n\nLet’s return to this concise mathematical notation. We’ve mentioned that a successful machine learning model depends on the model architecture embedded in \\(f\\) and the appropriate parameters \\(\\textbf{w}\\). The next question is, assuming we have an appropriate \\(f\\), how do we obtain a suitable set of \\(w\\)? First, we need to clarify an important premise: we need data to guide us toward the appropriate parameters, or in other words, we need data to train our model.\nNext, we need to address two key questions:\n\nWhat is the standard for good parameters?\nWhat kind of algorithm should we use to obtain them?"
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3.html#objective-function",
    "href": "Courses/mlwr2_2025/l3/l3.html#objective-function",
    "title": "Lecture 3: ANN and Deep Learning",
    "section": "3.1 Objective function",
    "text": "3.1 Objective function\n\n\n\n\n\nLet’s return to this concise mathematical notation. We’ve mentioned that a successful machine learning model depends on the model architecture embedded in \\(f\\) and the appropriate parameters \\(\\textbf{w}\\). The next question is, assuming we have an appropriate \\(f\\), how do we obtain a suitable set of \\(w\\)? First, we need to clarify an important premise: we need data to guide us toward the appropriate parameters, or in other words, we need data to train our model.\nNext, we need to address two key questions:\n\nWhat is the standard for good parameters?\nWhat kind of algorithm should we use to obtain them?\n\nIf you remember the loss function we discussed in the first part of the course, the first question becomes a piece of cake. Simply put, we need to compare the model’s predicted values for each individual’s target variable with the actual values of the target variable to calculate the loss. The set of parameters that gives us the smallest loss is the set of appropriate values we are looking for. So, the training problem can be represented as an optimization problem.\n\\[\n  \\min_{\\textbf{W}} \\mathcal{L}( \\textbf{y}, f(\\textbf{X}; \\textbf{W}))\n\\] where \\(\\textbf{y}\\) is the target variable containing all the target values in a dataset, \\(\\textbf{X}\\) is the data matrix of all possible input features, and \\(\\textbf{W}\\) contains all the model parameters.\n\n\n\n\n\n\n\n\n\n\nAssuming our loss function is a simple quadratic curve, our goal is to find the set of \\(w\\) that gives us the smallest loss in the most efficient way possible."
  },
  {
    "objectID": "Courses/mlwr2_2025/l3/l3.html#traning-problem",
    "href": "Courses/mlwr2_2025/l3/l3.html#traning-problem",
    "title": "Lecture 3: ANN and Deep Learning",
    "section": "3.1 Traning Problem",
    "text": "3.1 Traning Problem\n\n\n\n\n\nLet’s return to this concise mathematical notation. We’ve mentioned that a successful machine learning model depends on the model architecture embedded in \\(f\\) and the appropriate parameters \\(\\textbf{w}\\). The next question is, assuming we have an appropriate \\(f\\), how do we obtain a suitable set of \\(w\\)? First, we need to clarify an important premise: we need data to guide us toward the appropriate parameters, or in other words, we need data to train our model.\nNext, we need to address two key questions:\n\nWhat is the standard for good parameters?\nWhat kind of algorithm should we use to obtain them?\n\nIf you remember the loss function we discussed in the first part of the course, the first question becomes a piece of cake. Simply put, we need to compare the model’s predicted values for each individual’s target variable with the actual values of the target variable to calculate the loss. The set of parameters that gives us the smallest loss is the set of appropriate values we are looking for. So, the training problem can be represented as an optimization problem.\n\\[\n  \\min_{\\textbf{W}} \\mathcal{L}( \\textbf{y}, f(\\textbf{X}; \\textbf{W}))\n\\] where \\(\\textbf{y}\\) is the target variable containing all the target values in a dataset, \\(\\textbf{X}\\) is the data matrix of all possible input features, and \\(\\textbf{W}\\) contains all the model parameters.\n\n\n\n\n\n\n\n\n\n\nAssuming our loss function is a simple quadratic curve, our goal is to find the set of \\(w\\) that gives us the smallest loss in the most efficient way possible. Next, we’ll introduce the efficient method: the gradient descent algorithm."
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_0.html",
    "href": "Courses/mlwr2_2025/l1/l1_0.html",
    "title": "Lecture 1: Feature Extraction and PCA",
    "section": "",
    "text": "In the first part of the course, we discussed one approach to solving the problem of data dimensionality reduction: feature selection. By using different feature selection methods, we can control the size of the feature space, thereby obtaining relatively simpler models to overcome the overfitting issue. In this lecture, we will introduce another type of methods of data dimensionality reduction, which is also an essential concept in machine learning: feature extraction. Feature extraction can be understood as a general term for methods that construct new variables from the original ones.\nBy using the original feature variables to create a relatively smaller set of new feature variables, we can control the size of the feature space. At the same time, a good machine learning model depends on two factors: efficient algorithms and a well-designed feature space. It is not hard to understand that if we have an absolutely ideal feature space—one that is linearly separable—then simply applying a basic algorithm can yield a perfect predictive model. Therefore, feature extraction plays a crucial role in machine learning applications.\nOutline:\n\n1.1 Introduction to feature extraction\n1.2 Principle Components Analysis\n1.3 PCA Algorithm and A Simple Example\n1.4 Discussion\n\n\nLecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_1.html",
    "href": "Courses/mlwr2_2025/l1/l1_1.html",
    "title": "1. Introduction to feature extraction",
    "section": "",
    "text": "So, how does feature extraction create new feature variables? Let’s start by looking at a few examples. The first example is one we’re all familiar with: BMI, or Body Mass Index. It is a measurement used to assess whether a person has a healthy body weight. It is calculated by dividing a person’s weight in kilograms by the square of their height in meters, \\[\n  \\text{BMI} = \\frac{\\text{weight (kg)}}{\\text{height (m)}^2}\n\\]\nClearly, BMI is a new variable created from the original variables. It not only reduces the number of variables (from two variables, height and weight, to one BMI variable) but also provides a more effective and convenient way to assess overall health. Thus, BMI is a classic example of feature extraction. From a mathematical perspective, BMI is a function of the original variables, height and weight, i.e. \\(\\text{BMI} = g(h,w)\\).\nBy the way, in statistics, we usually refer to variables derived from original variables as latent variables. Latent variables are typically used to represent quantities that cannot be directly observed or measured. This is easy to understand: we can use a ruler and a scale to measure height and weight, but there is no direct instrument to measure BMI—it has to be calculated from height and weight. Therefore, in machine learning, we often refer to the space formed by extracted feature variables as the latent space.\nA more complex example is the Gini coefficient. The Gini coefficient measures inequality in income or wealth distribution. It ranges from 0 (perfect equality) to 1 (perfect inequality) and is based on the Lorenz curve, comparing actual distribution to perfect equality. For example, the map below shows the Gini coefficients of various countries, allowing us to understand their levels of equity.\n\n\n\n\nFigure source: Wiki\n\n\n\nSo, how is the Gini coefficient calculated? The Gini coefficient is calculated using the Lorenz curve, which plots the cumulative share of income or wealth against the cumulative share of the population. The formula is: \\[\n  G = \\frac{A}{A + B}\n\\] where \\(A\\) is the area between the Lorenz curve and the line of perfect equality, and \\(A + B\\) is the total area below the line of perfect equality.\n\n\n\n\n\n\n\nIt can also be computed directly using income data as: \\[\n  G = 1 - \\sum_{i=1}^{n} (y_i + y_{i-1})(x_i - x_{i-1})\n\\] where \\(x\\) and \\(y\\) are cumulative population and income shares. People can collect household income data from different countries, but the Gini coefficient must be computed based on these raw data.\nNow, let’s look at an example more relevant to machine learning. Take a look at the scatter plot below. Clearly, we want to use the two feature variables, \\(X_1\\) and \\(X_2\\), to distinguish the red points from the black points. You can think about how to create a new variable using the two feature variables to solve this classification problem.\n\n\n\n\n\n\n\n\n\n\n\nYou may already have the answer. Yes, we can calculate the distance of each point from the origin to create a new feature variable, i.e. for each point \\(i\\),\n\\[\n  D_i = \\sqrt{ X_1^2 + X_2^2 }\n\\]\nThen, we can choose an appropriate cutoff to differentiate the black points from the red points. This is also an example of feature extraction, where the extracted feature is a function of the original feature variables, i.e. \\(D = g(X_1, X_2)\\).\nBy now, I guess you’ve probably realized something. Do you remember how we introduced linear classifiers in the first part of the course? Yes, first, we compute the weighted sum of the feature variables, then compare it with an appropriate cutoff. In fact, a linear classifier is essentially doing feature extraction first, and then comparing the extracted feature with a cutoff. This feature extraction is also a function of the original feature variables, that is:\n\\[\n  Z = g(X_1, X_2, \\dots, X_p) = \\sum_{j = 1}^p w_jX_j\n\\]\nNow, we can summarize the discussion. Feature extraction is about finding an appropriate function \\(g()\\) of the original feature variables to transform and create new feature variables.\nThe examples above share a common characteristic: they all design feature extraction methods based on the problem’s specific characteristics, using domain knowledge (prior knowledge) or specific rational analysis. This approach is often referred to as domain knowledge based feature extraction or manual feature extraction. The benefits of this method are obvious, but it has a significant drawback: it heavily depends on prior information. Next, we will introduce the star of this lesson, PCA, which is an algorithm that learns the feature extraction function from the data, i.e. data driven solution.\n\nQuiz:\n\nUse simple language to explain to your family what feature extraction is.\nWe introduced the concept of ‘feature mapping’ idea for creating a nonlinear model in the first part of this course. Is feature extraction also a kind of feature mapping?\n\n\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_2.html",
    "href": "Courses/mlwr2_2025/l1/l1_2.html",
    "title": "2. Principle Components Analysis",
    "section": "",
    "text": "PCA is an important data analysis method in statistics, particularly in multivariate statistical analysis. Mathematicians and statisticians have studied it extensively and in great depth. Among them, the Swedish 🇸🇪 mathematician and statistician Herman Wold made significant contributions to the understanding and expansion of PCA’s essence. In fact, if we review his works on PCA related issues, we can even catch glimpses of the modern artificial neural network models. Let us pay tribute to this great pioneer! 🎩👏"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_2.html#what-is-pca",
    "href": "Courses/mlwr2_2025/l1/l1_2.html#what-is-pca",
    "title": "2. Principle Components Analysis",
    "section": "2.1 What is PCA?",
    "text": "2.1 What is PCA?\n\n\n\n\nWhat is PCA? I took this picture when I was waiting for traffic lights close to Stroa Coop at Tomtebo in Umeå in January 2021. What perfect timing! Being stopped by traffic lights isn’t always a bad thing.\n\n\n\nPCA is a linear feature extraction tool. Let’s first provide the definition:\n\nPCA is a  linear  numerical method for creating a relatively smaller set of mutually  orthogonal   new variables  from the original dataset and the most  information  can be preserved in the new dataset.\n\nNext, let me explain each highlighted key words in the definition.\nNew variables and Linear: The term “new variable” is easy to understand. PCA is a type of feature extraction method, and the results of feature extraction are essentially new variables. However, this feature extraction, or the map \\(g()\\), is not arbitrary; we constrain it to be linear. In other words, all the new variables must satisfy the following equation:\n\\[\n  g_{\\textbf{w}}(\\textbf{x}) = w_1x_1 + w_2x_2 + \\dots + w_px_p\n\\] where \\(\\textbf{x}\\) expresses all the \\(p\\) original feature variables, and \\(\\textbf{w}\\) contains all the \\(p\\) coefficients. A straightforward example is the average. The average is a widely used method of summarizing information in real-life situations. It is the time to awkwardly show the generation gap. For example, in old-style music competitions, a singer would typically receive scores from several judges after their performance. Then, the host would say, “Remove the highest score, remove the lowest score, and the final score is……” The final score is just the average. More precisely, it is the truncated mean.\n\n\n\n\n\n\n\nHere, I want to emphasize something for you. To make it more memorable, let me start with a sad memory.\n\nSad memories: In middle school, I was very good at mathematics and physics, and also terrible at English and literature. The end of every exam was always the most awkward moment for me. Say I got 100 scores both for mathematics and physics, but 0 scores both for English and literature. My teacher simply informed the average score as the overall evaluation of my study to my parents. I guess she used the following formula \\[\n  \\frac{1}{4} \\text{Math} + \\frac{1}{4} \\text{Physics}+ \\frac{1}{4} \\text{English} + \\frac{1}{4} \\text{Literature}\n\\] Obviously, I was hopeless. However, my smart mother courageously stepped up, she simply adjusted the coefficients of the feature extraction function, \\[\n  \\frac{1}{2} \\text{Math} + \\frac{1}{2} \\text{Physics}+ 0 \\cdot \\text{English}+ 0 \\cdot \\text{Literature}\n\\] and told me that you are actually great!\n\nA good choice of coefficients not only can save a young people but also leads to a informative new variable for different purposes. This sad story highlights the role of coefficients in the feature extraction function. In one word, different coefficients lead to different information.\n\n\n\n\n\n\n\nI would like to use the above picture to close the discussion about this keyword. Essentially, the extracted feature is just the weighted sum of original features, while the weighted sum is called linear combination in linear algebra and the geometry meaning of linear combination is projection. I strongly suggest you read about mathematical projection if you are not familiar with this concept.\nThis hand shadow game is a good example. If you don’t know it, just looking at the two hands won’t immediately tell you what the performer wants to show the audience. But once the light casts a shadow, the image of a dog becomes clear. The basic idea of feature extraction is similar—by using the right feature extraction function, useful information can be presented in a way that is easy for a computer to recognize. In one words, the new variable can be viewed as a shadow of the object (\\(\\textbf{x}\\)) from a proper direction (\\(\\textbf{w}\\)).\nInformation : Based on the discussion above, it’s not difficult to see that the goal of the PCA algorithm is to find a set of suitable coefficients to achieve feature extraction. But what does “suitable” coefficients mean? Are there specific criteria for this? To understand the answer to this question, we need to take a closer look at the key term “information”.\nIn statistics and information theory, we have many measures of information. In PCA, however, we use a simple and familiar measure to quantify how much information a variable contains, that is “Variance”. Let’s start with an interesting example.\nI will present two events—think about which one you would be more eager to share with your family or friends:\nEvent 1: This morning, the sun rose in the east.\nEvent 2: NASA has just admitted to observing UFOs over the past 10 years.\nI think you already have the answer. No rush, let me and my wife simulate this scenario, as shown in the picture below.\n\n\n\n\n\n\n\nOn the left, my wife immediately saw through my trick of avoiding cooking at home. On the right, she was genuinely shocked by the news—although she still figured out my trick five minutes later. For us, the amount of information in a message depends on how surprising it is. Essentially, information amount is roughly equal to the degree of surprise.\nAnother example, I tell two students, “You can pass this exam.” The first student has prepared well and is confident in their answers, while the second student didn’t do well and feels uncertain. Clearly, the amount of information my message carries is completely different for them. Therefore, the degree of surprise in a message depends on the uncertainty of the event it describes. In statistics, uncertainty is usually measured by variance.\nLet me use one last example to convince you. Suppose we have an epidemiological dataset about cervical cancer, which includes age, gender, BMI, and various health indicators. Now, which variable do we absolutely not need? Think about what the variance of this variable would be.\nOrthogonal : At this point, we’ve basically understood the core idea of how PCA extracts new variables. However, there’s one more thing to clarify: for a dataset containing many variables, we usually extract a set of new variables. The PCA algorithm has a specific requirement for the relationship between these extracted variables, which is orthogonality. Simply put, orthogonality means that there is no linear relationship between the extracted variables, meaning their covariance is zero. We’ll see this more clearly in a concrete example later."
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_2.html#pca-problem",
    "href": "Courses/mlwr2_2025/l1/l1_2.html#pca-problem",
    "title": "2. Principle Components Analysis",
    "section": "2.2 PCA Problem",
    "text": "2.2 PCA Problem\nWith all this groundwork laid out, it becomes much easier to understand how PCA extracts variables from the original dataset. Simply put, PCA first aims to find a set of coefficients to calculate new variables, and this set of coefficients is designed to maximize the variance of the extracted new variables. Suppose, we have \\(p\\) variables in a dataset. \\[\n  \\max_{\\textbf{w}} \\left\\{ \\text{Var} \\left( \\underbrace{w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\dots + w_p \\cdot x_p}_{\\text{extracted feature}} \\right) \\right\\}\n\\]\nBy solving this optimization problem, we obtain an optimizer \\(\\textbf{w}_1\\) that is a set of coefficients for computing the first new variable. We call \\(\\textbf{w}_1\\) the first Principal Component weights (PC weithgs), and the variable calculated using these coefficients is commonly known as the first principal component. Of course, these are just statistical terms. In machine learning, this is simply a feature extraction function obtained through an algorithm under certain constraints.\nAs mentioned earlier, we usually need to extract a series of new variables from the original dataset to replace the old ones, achieving dimensionality reduction. Finding the second set of coefficients is not much different from the previous problem—we still aim to maximize the variance of the resulting variable. However, the key difference is that we need to add a constraint to ensure that we do not obtain the same first PC weights again. This constraint is what we call orthogonality before.\n\\[\n  \\max_{\\textbf{w}: \\textbf{w} \\perp \\textbf{w}_1} \\left\\{ \\text{Var} \\left( \\underbrace{w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\dots + w_p \\cdot x_p}_{\\text{extracted feature}} \\right) \\right\\}\n\\] Some linear algebra knowledge is needed to fully understand \\(\\textbf{w} \\perp \\textbf{w}_1\\), but you can ignore the details for now and just remember two key points:\n\nThis condition prevents us from obtaining the first set of PC weights again.\n\nThe second principal component (new variable) obtained this way will be linearly uncorrelated with the first principal component.\n\nOf course, if needed, we continue searching for the third set of PC weights. This time, we need to add two orthogonality constraints to ensure it remains uncorrelated with both the first and second principal components, that is \\(\\textbf{w} \\perp \\textbf{w}_1\\) and \\(\\textbf{w} \\perp \\textbf{w}_2\\). By following this approach, we can continue finding more PC weights. In fact, we can obtain up to \\(p\\) new variables—yes, the same number as in the original dataset.\nYou might be wondering: How does this achieve dimensionality reduction? Let’s explore this in the next section with a concrete example.\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_3.html",
    "href": "Courses/mlwr2_2025/l1/l1_3.html",
    "title": "3. Principle Components Analysis",
    "section": "",
    "text": "In the previous section, we discussed what PCA is and the optimization problem it involves. This optimization problem is actually quite easy to solve because it is equivalent to an eigenvalue decomposition problem. Here, we will skip the mathematical details and directly present the algorithm, explaining how to obtain PC weights and use them to compute the new data matrix. Finally, we will demonstrate this with a small example.\n\n\nAlgorithm: Principal Components Analysis\n\n\nInputs:\n\n\\(\\textbf{X}\\), \\(n\\times p\\) matrix, containing all \\(p\\) feature variables.\n\n\nSteps:\n\nCalculate the covariance matrix, \\(S_{\\textbf{X}}\\)\nPerform eigenvalue decomposition on the covariance matrix, \\(S_{\\textbf{X}} = \\textbf{W} \\boldsymbol{\\Lambda} \\textbf{W}^{\\top}\\). \\(\\mathbf{W}\\) is a \\(p \\times p\\) matrix, where each column, \\(\\textbf{w}_j\\), is an eigenvector, and we call it weights matrix. \\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix, where each diagonal element \\(\\lambda_j\\) is the eigenvalue corresponding to its eigenvector \\(\\textbf{w}_j\\).\nCalculate the new data matrix: \\(\\mathbf{Z} = \\mathbf{XW}\\)\n\n\nOutput: The new data matrix, \\(\\mathbf{Z}\\), the weights matrix, \\(\\mathbf{W}\\), and the matrix of eigenvalues, \\(\\boldsymbol{\\Lambda}\\).\n\nFirst, it is not difficult to guess that the PC weights we are looking for are exactly the column vectors in \\(\\mathbf{W}\\). Each column vector (eigenvector) \\(\\textbf{w}_j\\) is a p-dimensional vector, which contains the weights needed for each original variable when computing the new variables.\nSecond, an observation no mater in the original data matrix, \\(\\textbf{x}_i\\), i.e. each row in \\(\\textbf{X}\\), or a new observation, \\(\\textbf{x}_{new}\\), we can calculate the new variable (extracted feature) as \\(\\textbf{x}_i^{\\top}\\textbf{w}_j\\) or \\(\\textbf{x}_{new}^{\\top}\\textbf{w}_j\\). For the entire original data matrix, we can compute all possible extracted feature variables by matrix multiplication at once, i.e. \\(\\mathbf{Z} = \\mathbf{XW}\\).\nFinally, the eigenvalue matrix, \\(\\boldsymbol{\\Lambda}\\), contains the variance of all extracted features, which represents the amount of information they carry.\n\nRemark: In many textbooks, authors emphasize that the mean of all variables should be removed before performing PCA. This step is not essential, but it can lead to slight differences in the calculation of eigenvectors depending on the algorithm used. We will not go into too much detail on this here."
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_3.html#pca-algorithm",
    "href": "Courses/mlwr2_2025/l1/l1_3.html#pca-algorithm",
    "title": "3. Principle Components Analysis",
    "section": "",
    "text": "In the previous section, we discussed what PCA is and the optimization problem it involves. This optimization problem is actually quite easy to solve because it is equivalent to an eigenvalue decomposition problem. Here, we will skip the mathematical details and directly present the algorithm, explaining how to obtain PC weights and use them to compute the new data matrix. Finally, we will demonstrate this with a small example.\n\n\nAlgorithm: Principal Components Analysis\n\n\nInputs:\n\n\\(\\textbf{X}\\), \\(n\\times p\\) matrix, containing all \\(p\\) feature variables.\n\n\nSteps:\n\nCalculate the covariance matrix, \\(S_{\\textbf{X}}\\)\nPerform eigenvalue decomposition on the covariance matrix, \\(S_{\\textbf{X}} = \\textbf{W} \\boldsymbol{\\Lambda} \\textbf{W}^{\\top}\\). \\(\\mathbf{W}\\) is a \\(p \\times p\\) matrix, where each column, \\(\\textbf{w}_j\\), is an eigenvector, and we call it weights matrix. \\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix, where each diagonal element \\(\\lambda_j\\) is the eigenvalue corresponding to its eigenvector \\(\\textbf{w}_j\\).\nCalculate the new data matrix: \\(\\mathbf{Z} = \\mathbf{XW}\\)\n\n\nOutput: The new data matrix, \\(\\mathbf{Z}\\), the weights matrix, \\(\\mathbf{W}\\), and the matrix of eigenvalues, \\(\\boldsymbol{\\Lambda}\\).\n\nFirst, it is not difficult to guess that the PC weights we are looking for are exactly the column vectors in \\(\\mathbf{W}\\). Each column vector (eigenvector) \\(\\textbf{w}_j\\) is a p-dimensional vector, which contains the weights needed for each original variable when computing the new variables.\nSecond, an observation no mater in the original data matrix, \\(\\textbf{x}_i\\), i.e. each row in \\(\\textbf{X}\\), or a new observation, \\(\\textbf{x}_{new}\\), we can calculate the new variable (extracted feature) as \\(\\textbf{x}_i^{\\top}\\textbf{w}_j\\) or \\(\\textbf{x}_{new}^{\\top}\\textbf{w}_j\\). For the entire original data matrix, we can compute all possible extracted feature variables by matrix multiplication at once, i.e. \\(\\mathbf{Z} = \\mathbf{XW}\\).\nFinally, the eigenvalue matrix, \\(\\boldsymbol{\\Lambda}\\), contains the variance of all extracted features, which represents the amount of information they carry.\n\nRemark: In many textbooks, authors emphasize that the mean of all variables should be removed before performing PCA. This step is not essential, but it can lead to slight differences in the calculation of eigenvectors depending on the algorithm used. We will not go into too much detail on this here."
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_3.html#a-simple-example",
    "href": "Courses/mlwr2_2025/l1/l1_3.html#a-simple-example",
    "title": "3. Principle Components Analysis",
    "section": "3.2 A Simple Example",
    "text": "3.2 A Simple Example\nIn this section, we use a simple example to demonstrate how to implement PCA with a simple program in R. First, we import the data, record some basic information, and visualize the data\n\ndat = iris #import data\nX = as.matrix(dat[,-5]) #take the 4 feature variables and save them in a matrix\nn = dim(X)[1] #record the sample size\np = dim(X)[2] #record the number of variables\npairs(X, col = dat$Species) #visualize the data in a pairwise scatter plot\n\n\n\n\n\n\n\n\nNext we do PCA on data matrix X. First, we calculate the covariance matrix.\n\nS = cov(X) # calculate the covariance matrix\n\nSecond, do eigenvalue decomposition on covariance matrix S and save the results in res. In R, we can apply function eigen to perform eigenvalue decomposition.\n\nres = eigen(S)\nstr(res)\n\nList of 2\n $ values : num [1:4] 4.2282 0.2427 0.0782 0.0238\n $ vectors: num [1:4, 1:4] 0.3614 -0.0845 0.8567 0.3583 0.6566 ...\n - attr(*, \"class\")= chr \"eigen\"\n\n\nAs shown above, the results of the eigenvalue decomposition contain all the information we need. slot values contains all the eigenvalues, and vectors contains all the eigenvectors, that is the optimal weights matrix.\n\nW = res$vectors # define the weights matrix W\nw_1 = W[,1] # the first column of W is the first PC weights.\nw_1\n\n[1]  0.36138659 -0.08452251  0.85667061  0.35828920\n\n\nAbove, we define the weights matrix \\(\\textbf{W}\\), and the first column, \\(\\textbf{w}_i\\), contains the optimal weights for calculating the first extracted variables. Next, we extract the first PC of the first flower.\n\nX[1, , drop = F]\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]          5.1         3.5          1.4         0.2\n\nX[1, , drop = F]%*%w_1 # or, you can try sum(X[1,]*w_1) which is more like a weighted sum.\n\n        [,1]\n[1,] 2.81824\n\n\nSo, \\(2.81824\\) is just value of first PC for the first flower. We can extract the first PC for all the flowers together, then we get the first new variable.\n\nz_1 = X%*%w_1\n\nYou can print out z_1 in your own computer and you will see it is a \\(150 \\times 1\\) vector. We can even use matrix multiplication more efficiently to extract all possible principal components (PCs) at once.\n\nZ = X%*%W\ndim(Z)\n\n[1] 150   4\n\n\nYou can see that the new data matrix Z has the same dimension as the original data matrix X. But what is the advantage of the new data matrix? We can find the answer by comparing the covariance matrices of the two data matrices.\n\nround(cov(X),2)\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         0.69       -0.04         1.27        0.52\nSepal.Width         -0.04        0.19        -0.33       -0.12\nPetal.Length         1.27       -0.33         3.12        1.30\nPetal.Width          0.52       -0.12         1.30        0.58\n\nround(cov(Z),2)\n\n     [,1] [,2] [,3] [,4]\n[1,] 4.23 0.00 0.00 0.00\n[2,] 0.00 0.24 0.00 0.00\n[3,] 0.00 0.00 0.08 0.00\n[4,] 0.00 0.00 0.00 0.02\n\n\nIt is easy to see that, compared to the original data matrix, the covariance matrix of the new data matrix is much simpler. The new variables are not only uncorrelated, but their variances gradually decrease.\nThis means that the amount of information contained in the variables of the new data matrix gradually decreases. The first new variable contains the most information, while the last new variable carries very little. Based on this, we can easily decide which variables to discard from the new matrix. This is the key essence of the PCA algorithm.\n\nround(res$values,2)\n\n[1] 4.23 0.24 0.08 0.02\n\n\nFinally, let’s take a look at the eigenvalues obtained from the eigenvalue decomposition. Yes, the eigenvalues represent the variances of the new variables, which indicate the amount of information they contain.\n\nPrevious page | Lecture 1 Homepage | Next page"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_4.html",
    "href": "Courses/mlwr2_2025/l1/l1_4.html",
    "title": "4. Discussion",
    "section": "",
    "text": "Let’s conclude this lecture with a discussion. We call PCA a data-driven solution, meaning it does not rely on any prior information. However, it is important to emphasize that PCA is merely a linear feature extraction method. The concept of linearity is straightforward: all extracted feature variables are just weighted sums of the original variables. But how can we understand linearity from a geometric perspective? Let’s explore this idea with the following toy example.\n\n\n\n\n\n\n\n\n\n\n\nIn this example, we have two variables, or two dimensions, represented by the blue coordinate axes. The red and orange lines represent two sets of PC weights, meaning the new variables are the projections of all the observations onto these lines. This requires a bit of imagination. Imagine that all the observations fall perpendicularly onto the red line, which gives us the first new variable. Similarly, by projecting onto the orange direction, we get the second new variable. It’s easy to see that the variance (or information) along the red line is much higher than that along the orange line. Therefore, we can discard the orange variable, effectively reducing the dimensionality while preserving most of the information. Therefore, the linearity of PCA lies in the fact that we are simply re-examining the original feature space from different angles. In other words, we do not disturb the relative positions of the original observations.\nThe advantage of PCA is that the algorithm is simple and can be easily understood from a geometric perspective, but its limitations are quite apparent as well. First, its feature extraction does not rely on any target information to guide it. The feature extraction process only depends on the covariance structure of the original data. Second, its feature extraction capability is quite limited. We can see that all possible feature extraction methods are determined by the covariance structure inherent in the data. At most, we can have as many feature extraction functions as there are original variables, but most of them will be useless. Therefore, PCA is NOT very flexible, and when the problem becomes complex, it often fails to provide an effective feature extraction solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA is like God rotating a globe, viewing what happens on this blue planet from different angles. When there’s a problem, God steps in to fix it. However, this gentle approach is linear. But when humanity presents too many complicated challenges, even God might be powerless, allowing the devil to intervene with non-linear methods in extreme ways to solve the problem.\nThis might not be the best analogy, especially since I’ve received warnings and restrictions while generating images with ChatGPT, but it is indeed quite vivid. So, let’s remember to take care of our beautiful blue planet. 🌍\n\nPrevious page | Lecture 1 Homepage"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_home.html#tasks",
    "href": "Courses/mlwr2_2025/l1/l1_home.html#tasks",
    "title": "Lecture 1: Feature Extraction and PCA",
    "section": "Tasks",
    "text": "Tasks\n\nIf you are still not familiar with PCA, please carefully read through all of the lecture notes in this session. If you have already learned PCA, use my notes to review and test whether you can understand PCA from the perspective of feature extraction.\nFamiliarize yourself with the R codes in the simple example from Section 3 of the lecture notes.\n\n\nCourse Homepage"
  },
  {
    "objectID": "Courses/mlwr2_2025/l1/l1_home.html#your-tsasks",
    "href": "Courses/mlwr2_2025/l1/l1_home.html#your-tsasks",
    "title": "Lecture 1: Feature Extraction and PCA",
    "section": "Your tsasks",
    "text": "Your tsasks\n\nIf you are still not familiar with PCA, please carefully read through all of the lecture notes in this session. If you have already learned PCA, use my notes to review and test whether you can understand PCA from the perspective of feature extraction.\nFamiliarize yourself with the R codes in the simple example from Section 3 of the lecture notes. This will be the necessary preparation for the lab session after the second lecture.\n\n\nCourse Homepage"
  },
  {
    "objectID": "Courses/mlwr2_2025/mlwr2_index.html",
    "href": "Courses/mlwr2_2025/mlwr2_index.html",
    "title": "Machine Learning with R, Part 2",
    "section": "",
    "text": "This is a public course designed for students in different levels in Umeå University, and it is given at the department of statistics, Umeå University. Students need to have elementary programming knowledge and some understanding of basic statistics, at most understanding regression analysis. In the first part of this course (Part I), we focused on familiarizing students with the basic concepts of machine learning through basic linear models. Now, in the second part of the course, we will focus on nonlinear models. There are many nonlinear models in machine learning, such as kernel methods, ensemble methods, etc. However, we will concentrate on understanding complex artificial neural network models and introduce what is known as deep learning.\n\n\n\n\nIn this new era, learning various skills in data science is as straightforward as learning to get a driver’s license, but not everyone can become an engineer or technician for the Ferrari racing team. My visions: First, every student can get their license and add new tools to their data analysis toolbox. Second, I hope this course can provide some helps to those who want to understand the engine behind the tools. If that sounds like you, you’ll need to invest some time and a bit of patience :)"
  },
  {
    "objectID": "Courses/mlwr2_2025/mlwr2_index.html#introduction",
    "href": "Courses/mlwr2_2025/mlwr2_index.html#introduction",
    "title": "Machine Learning with R, Part 2",
    "section": "",
    "text": "This is a public course designed for students in different levels in Umeå University, and it is given at the department of statistics, Umeå University. Students need to have elementary programming knowledge and some understanding of basic statistics, at most understanding regression analysis. In the first part of this course (Part I), we focused on familiarizing students with the basic concepts of machine learning through basic linear models. Now, in the second part of the course, we will focus on nonlinear models. There are many nonlinear models in machine learning, such as kernel methods, ensemble methods, etc. However, we will concentrate on understanding complex artificial neural network models and introduce what is known as deep learning.\n\n\n\n\nIn this new era, learning various skills in data science is as straightforward as learning to get a driver’s license, but not everyone can become an engineer or technician for the Ferrari racing team. My visions: First, every student can get their license and add new tools to their data analysis toolbox. Second, I hope this course can provide some helps to those who want to understand the engine behind the tools. If that sounds like you, you’ll need to invest some time and a bit of patience :)"
  },
  {
    "objectID": "Courses/mlwr2_2025/slides_course_intro.html#section",
    "href": "Courses/mlwr2_2025/slides_course_intro.html#section",
    "title": "My Yggdrasil",
    "section": "",
    "text": "There are many ways to introduce and explain neural networks, most of which require learners to have a certain level of prior knowledge."
  },
  {
    "objectID": "Courses/mlwr2_2025/slides_course_intro.html#section-1",
    "href": "Courses/mlwr2_2025/slides_course_intro.html#section-1",
    "title": "My Yggdrasil",
    "section": "",
    "text": "However, how to quickly understand deep learning based on neural network models within a limited time, building on the first part of the course, is a challenge."
  },
  {
    "objectID": "Courses/mlwr2_2025/slides_course_intro.html#section-2",
    "href": "Courses/mlwr2_2025/slides_course_intro.html#section-2",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Here, I will use the most cost-effective approach to help you deeply understand neural network models. No free lunches. We first need to take some time to build a bridge."
  },
  {
    "objectID": "Courses/mlwr2_2025/slides_course_intro.html#section-3",
    "href": "Courses/mlwr2_2025/slides_course_intro.html#section-3",
    "title": "My Yggdrasil",
    "section": "",
    "text": "To build this bridge, we will introduce the concept of feature extraction in the first lecture and present the most fundamental data-driven solution, PCA."
  },
  {
    "objectID": "Courses/mlwr2_2025/slides_course_intro.html#section-4",
    "href": "Courses/mlwr2_2025/slides_course_intro.html#section-4",
    "title": "My Yggdrasil",
    "section": "",
    "text": "Then, in the second lecture, we will revisit PCA from a machine learning perspective using a practical example."
  },
  {
    "objectID": "Courses/mlwr2_2025/slides_course_intro.html#section-5",
    "href": "Courses/mlwr2_2025/slides_course_intro.html#section-5",
    "title": "My Yggdrasil",
    "section": "",
    "text": "I believe that once you fully understand the content of the first two lectures, you will smoothly grasp ANN models and the core concepts of deep learning."
  },
  {
    "objectID": "Courses/mlwr2_2025/mlwr2_index.html#tips-for-your-readings-ca",
    "href": "Courses/mlwr2_2025/mlwr2_index.html#tips-for-your-readings-ca",
    "title": "Machine Learning with R, Part 2",
    "section": "Tips for your readings ( CA )",
    "text": "Tips for your readings ( CA )\n\nFor each lecture, I offer multiple ways to read. You can choose the integrated notes, allowing you to scroll up and down with ease, and use the side menu to jump between sections. However, if you find lengthy notes overwhelming, you might prefer the paginated version. It’s like how I divide a 2000-meter swim into four sessions—breaking it up makes it feel more achievable. Finally, I also provide a PDF version of the notes, making it convenient to print and read at your own pace.\nIn academia, people often use abbreviation, especially in technical writing. While this can be convenient for the author, it’s not always beginner-friendly. In my notes, I’ll do my best to implement a hover-over feature for annotating abbreviation. When you hover over the cursor on the abbreviation for a second, its full term will appear on the screen. For example: move your cursor on  IAT .  BTW , if you see  NE , it means “Not essential and you may skip”."
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_home.html#discussion",
    "href": "Courses/mlwr2_2025/l2/l2_home.html#discussion",
    "title": "Lecture 2: Another Perspective on PCA",
    "section": "Discussion:",
    "text": "Discussion:"
  },
  {
    "objectID": "Courses/mlwr2_2025/l2/l2_home.html#lab-session",
    "href": "Courses/mlwr2_2025/l2/l2_home.html#lab-session",
    "title": "Lecture 2: Another Perspective on PCA",
    "section": "Lab session:",
    "text": "Lab session:\nLaboratory: Entrance\nSolutions: [TBA]\nR file: download [TBA]\n\nCourse Homepage"
  }
]