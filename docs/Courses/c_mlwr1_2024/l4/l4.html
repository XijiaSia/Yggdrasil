<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Lecture 4: Gaussian Discriminant Analysis – My Yggdrasil</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../Images/main_logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">My Yggdrasil</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../01_c_index.html"> 
<span class="menu-text">Norns’ Blessing</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../02_m_index.html"> 
<span class="menu-text">Mímisbrunnr</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../03_s_index.html"> 
<span class="menu-text">Skalds</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../04_about_me.html"> 
<span class="menu-text">About me</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#basic-ideas" id="toc-basic-ideas" class="nav-link active" data-scroll-target="#basic-ideas">4.1 Basic Ideas</a></li>
  <li><a href="#classifier-constructed-based-on-gaussian-model" id="toc-classifier-constructed-based-on-gaussian-model" class="nav-link" data-scroll-target="#classifier-constructed-based-on-gaussian-model">4.2 Classifier constructed based on Gaussian model</a>
  <ul class="collapse">
  <li><a href="#gaussian-discriminan-analysis" id="toc-gaussian-discriminan-analysis" class="nav-link" data-scroll-target="#gaussian-discriminan-analysis">4.2.1 Gaussian Discriminan Analysis</a></li>
  <li><a href="#linear-or-nonlinear" id="toc-linear-or-nonlinear" class="nav-link" data-scroll-target="#linear-or-nonlinear">4.2.2 Linear or Nonlinear?</a></li>
  <li><a href="#training-algorithm-and-example-in-r" id="toc-training-algorithm-and-example-in-r" class="nav-link" data-scroll-target="#training-algorithm-and-example-in-r">4.2.3 Training algorithm and Example in R</a></li>
  </ul></li>
  <li><a href="#further-discussion" id="toc-further-discussion" class="nav-link" data-scroll-target="#further-discussion">4.3 Further Discussion</a>
  <ul class="collapse">
  <li><a href="#more-carefully" id="toc-more-carefully" class="nav-link" data-scroll-target="#more-carefully">4.3.1 More carefully</a></li>
  <li><a href="#multiple-labels" id="toc-multiple-labels" class="nav-link" data-scroll-target="#multiple-labels">4.3.2 Multiple labels</a></li>
  <li><a href="#r-functions-for-gda" id="toc-r-functions-for-gda" class="nav-link" data-scroll-target="#r-functions-for-gda">4.3.3 R functions for GDA</a></li>
  <li><a href="#fishers-idea" id="toc-fishers-idea" class="nav-link" data-scroll-target="#fishers-idea">4.3.4 Fisher’s idea</a></li>
  </ul></li>
  <li><a href="#model-evaluations" id="toc-model-evaluations" class="nav-link" data-scroll-target="#model-evaluations">4.4 Model Evaluations</a>
  <ul class="collapse">
  <li><a href="#confusion-matrix-and-related-statistics" id="toc-confusion-matrix-and-related-statistics" class="nav-link" data-scroll-target="#confusion-matrix-and-related-statistics">4.4.1 Confusion Matrix and related statistics</a></li>
  <li><a href="#more-choices" id="toc-more-choices" class="nav-link" data-scroll-target="#more-choices">4.4.2 More Choices</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 4: Gaussian Discriminant Analysis</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In this lecture, we will learn about the first set of machine learning models, which are also the first group of classifiers, namely Gaussian Discriminant Analysis. In addition, we will also learn how to evaluate the performance of a classifier. We will introduce several different statistics to assess a classifier from different perspectives, thereby enriching model evaluation methods beyond just accuracy.</p>
<section id="basic-ideas" class="level2">
<h2 class="anchored" data-anchor-id="basic-ideas">4.1 Basic Ideas</h2>
<p>Let me start with a motivating example. I have data on a guy’s height and weight: He is 170 cm tall and weigh 68 kg. Given that I know he is either Swedish or Chinese, which country do you think they are from? Do you have any ideas?</p>
<p>Let’s analyze this problem.</p>
<ul>
<li><p>First, it’s clearly a classification problem—we want to make a judgment based on height and weight data.</p></li>
<li><p>Next, essentially, we now have an observation, so we can measure the likelihood value of this observation. However, if we want to measure the likelihood value, we first need to find an appropriate model.</p></li>
<li><p>This problem is simple; the bivariate normal distribution is definitely a fit, but we still need specific parameters to determine this normal model.</p></li>
<li><p>Returning to the problem itself: our premise is to make a judgment between Sweden and China. Clearly, we have two candidate models in front of us—the Swedish normal model and the Chinese normal model.</p></li>
</ul>
<p>Now we have a plan. We can set up two candidate models and obtain their parameters from each country’s statistical department—that is, two means and a 2D covariance matrix. Then, we use these two models to evaluate the observed likelihood values for this person. If the likelihood value calculated from the Chinese model is higher than that from the Swedish model, we guess that they are Chinese; otherwise, we guess they are Swedish.</p>
<p>To do so, I got the statistics for both models, that is the average height and weight are <span class="math inline">\((179, 75)\)</span>, the SDs are <span class="math inline">\((5,4)\)</span>, and the correlation is <span class="math inline">\(0.6\)</span> in Sweden; for China, the average values are <span class="math inline">\((170, 65)\)</span>, SDs are <span class="math inline">\((5.5,3)\)</span>, and correlation is also <span class="math inline">\(0.6\)</span>. Next, I plot the contour graphs of density function for both model and find the position of this guy in R</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ellipse)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># my function for generating the contour map</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>contour.g <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">mu =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), Sigma, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">text =</span> <span class="st">""</span>){</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="dv">0</span>,<span class="dv">0</span>, <span class="at">xlim=</span><span class="fu">c</span>(mu[<span class="dv">1</span>]<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>(Sigma[<span class="dv">1</span>,<span class="dv">1</span>]),mu[<span class="dv">1</span>]<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>(Sigma[<span class="dv">1</span>,<span class="dv">1</span>])), </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim=</span><span class="fu">c</span>(mu[<span class="dv">2</span>]<span class="sc">-</span><span class="dv">1</span><span class="sc">*</span><span class="fu">sqrt</span>(Sigma[<span class="dv">2</span>,<span class="dv">2</span>]),mu[<span class="dv">2</span>]<span class="sc">+</span><span class="dv">1</span><span class="sc">*</span><span class="fu">sqrt</span>(Sigma[<span class="dv">2</span>,<span class="dv">2</span>])), <span class="at">asp =</span> <span class="dv">1</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span>, <span class="at">main =</span> text)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  cv <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.01</span>,<span class="fl">0.05</span>,<span class="fl">0.1</span>,<span class="fl">0.25</span>,<span class="fl">0.5</span>,<span class="fl">0.75</span>,<span class="fl">0.9</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  co <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"purple"</span>,<span class="st">"blue"</span>,<span class="st">"green"</span>,<span class="st">"yellow"</span>,<span class="st">"light blue"</span>,<span class="st">"orange"</span>, <span class="st">"red"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>){</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(<span class="fu">ellipse</span>(<span class="at">x =</span> Sigma, <span class="at">centre =</span> mu, <span class="at">level=</span>cv[i], <span class="at">npoints=</span><span class="dv">250</span>), <span class="at">col=</span>co[i], <span class="at">lwd=</span>lwd, <span class="at">type =</span> <span class="st">"l"</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># model Sweden</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>mu1 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">179</span>, <span class="dv">75</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>sigma1 <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">5</span><span class="sc">^</span><span class="dv">2</span>, <span class="fl">0.6</span><span class="sc">*</span><span class="dv">5</span><span class="sc">*</span><span class="dv">4</span>, <span class="fl">0.6</span><span class="sc">*</span><span class="dv">5</span><span class="sc">*</span><span class="dv">4</span>, <span class="dv">4</span><span class="sc">^</span><span class="dv">2</span>), <span class="at">byrow =</span> T, <span class="dv">2</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># model China</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>mu2 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">170</span>, <span class="dv">65</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">5.5</span><span class="sc">^</span><span class="dv">2</span>, <span class="fl">0.6</span><span class="sc">*</span><span class="fl">5.5</span><span class="sc">*</span><span class="dv">3</span>, <span class="fl">0.6</span><span class="sc">*</span><span class="fl">5.5</span><span class="sc">*</span><span class="dv">3</span>, <span class="dv">3</span><span class="sc">^</span><span class="dv">2</span>), <span class="at">byrow =</span> T, <span class="dv">2</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="fu">contour.g</span>(mu1, sigma1, <span class="at">text =</span> <span class="st">"Swedish Men"</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x =</span> <span class="dv">170</span>, <span class="at">y =</span> <span class="dv">68</span>, <span class="st">"Sia"</span> ) <span class="co"># find my position</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="fu">contour.g</span>(mu2, sigma2, <span class="at">text =</span> <span class="st">"Chinese Men"</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x =</span> <span class="dv">170</span>, <span class="at">y =</span> <span class="dv">68</span>, <span class="st">"Sia"</span> ) <span class="co"># find my position</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="l4_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Oops, it was me. Obviously, under the Chinese model, I’m already a bit overweight but still within the light blue likelihood level, while under the Swedish model, I’m somewhat on the margins. So, the final judgment is that this guy is Chinese. As you know, I am originally from China, so our idea works. Great!</p>
<p><strong>Disclaimer</strong>: Except for my personal data, all other data is fabricated purely for educational purposes, with no malice or discrimination intended.</p>
</section>
<section id="classifier-constructed-based-on-gaussian-model" class="level2">
<h2 class="anchored" data-anchor-id="classifier-constructed-based-on-gaussian-model">4.2 Classifier constructed based on Gaussian model</h2>
<p>Let’s summarize the basic idea of the motivating example. Essentially, we propose two candidate models based on the labels, use these two models to evaluate the object we want to predict, and then make a judgment based on the likelihood evaluation results. The candidate models here are normal distributions. Remember? A probabilistic model is essentially a probability distribution.</p>
<section id="gaussian-discriminan-analysis" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-discriminan-analysis">4.2.1 Gaussian Discriminan Analysis</h3>
<p>Up to this point, we have actually designed a function where the input consists of the two feature variables, height and weight, and the output is the prediction of the label. A classifier constructed based on this idea is called <strong>Gaussian Discriminant Analysis</strong> (GDA). Next, let’s describe this method using precise mathematical language.</p>
<p>Suppose we have a target variable, <span class="math inline">\(Y\)</span> which has <span class="math inline">\(\{-1, 1\}\)</span> as possible values. We denote the feature variables as <span class="math inline">\(\textbf{X} = (X_1, X_2, \dots, X_p)^{\top}\)</span>. Just emphasize that <span class="math inline">\(\textbf{X}\)</span> is not a specific observation, and it is just a random variable now. Then we assume that <span class="math display">\[
  \begin{matrix}
    \textbf{x}|y=1 &amp; \sim &amp; \mathcal{N}(\boldsymbol{\mu}_1,\boldsymbol{\Sigma}_1)\\
    \textbf{x}|y=-1 &amp; \sim &amp; \mathcal{N}(\boldsymbol{\mu}_2,\boldsymbol{\Sigma}_2)
  \end{matrix}
\]</span> With this model, we make the prediction by comparing the likelihood values of a new case with respect to each distribution, i.e.&nbsp;a new case <span class="math inline">\(\textbf{x}_{\text{new}}\)</span> is predicted as category <span class="math inline">\(1\)</span> if <span class="math display">\[
  f(\textbf{x}_{new}; \boldsymbol{\mu}_1,\boldsymbol{\Sigma}_1) &gt; f(\textbf{x}_{new}; \boldsymbol{\mu}_2,\boldsymbol{\Sigma}_2)
\]</span> where <span class="math inline">\(f\)</span> is the density function of multivariate Gaussian distribution, otherwise, the new case will be classified to another group.</p>
</section>
<section id="linear-or-nonlinear" class="level3">
<h3 class="anchored" data-anchor-id="linear-or-nonlinear">4.2.2 Linear or Nonlinear?</h3>
<p>In Lecture 1, we explained the general form of a linear classifier, i.e., the decision boundary of the classifier is a line, a plane, or a hyperplane in higher dimensions. So far, our GDA is just an R function, kind of black box, so the question is: Is it a linear classifier? We are going to investigate it in this subsection.</p>
<p>First, to understand the nature of the classifier’s decision boundary, it is best to express it using mathematical language. Review the decision rule presented in the previous subsection, it is very easy to see the decision boundary can be represented as <span class="math display">\[
  \ell = \left\{ \textbf{x}: f(\textbf{x}; \boldsymbol{\mu}_1,\boldsymbol{\Sigma}_1) = f(\textbf{x}; \boldsymbol{\mu}_2,\boldsymbol{\Sigma}_2) \right\}
\]</span> where <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> are p.d.f of normal distribution. While the notation above is in set theory language, it is not difficult to understand it. It means that, in set <span class="math inline">\(\ell\)</span>, we collect all the points <span class="math inline">\(\textbf{x}\)</span> that satisfy the condition, <span class="math inline">\(f(\textbf{x}; \boldsymbol{\mu}_1,\boldsymbol{\Sigma}_1) = f(\textbf{x}; \boldsymbol{\mu}_2,\boldsymbol{\Sigma}_2)\)</span>, i.e.&nbsp;equal likelihood in a certain dimensional space.</p>
<p>Next, I will do some experiments in R to investigate the properties the decision boundary. Obviously, the decision boundary is determined by the two density functions together. So, my plan is to assign different parameter values to the two density functions, then, using the contour graphs of the two density functions, find the points where their values are the same, and connect them. This way, we can roughly understand the shape of the decision boundary.</p>
<p>Before we begin, there’s one thing we need to clarify. We only need to focus on the effect of covariance matrix, because it controls the shape of the distribution, and the shape of the distribution determines the shape of the decision boundary.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>First, let’s check the simplest situation, that is the two groups all have the simplest covariance matrix, i.e.&nbsp; <span class="math display">\[
  \boldsymbol{\Sigma}_1 = \boldsymbol{\Sigma}_2 =   \left(
  \begin{matrix}
    1 &amp; 0 \\
    0 &amp; 1
  \end{matrix}
  \right)
\]</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/L4_lda1.gif" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:88.8%"></p>
</figure>
</div>
</div>
</div>
<p>From the animation above, we can see that if we connect all the intersection points of contours with the same color, it will roughly form a straight line. In this very special case, our decision boundary is linear. Now, let’s relax the conditions a bit so that the Gaussian model can cover more possibilities, but not too far—just a small step.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Let’s assume that the two distributions have the same, but arbitrary, covariance matrix， i.e. <span class="math display">\[
  \boldsymbol{\Sigma}_1 = \boldsymbol{\Sigma}_2 =   \left(
  \begin{matrix}
    1 &amp; 0.3 \\
    0.3 &amp; 0.5
  \end{matrix}
  \right)
\]</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/L4_lda2.gif" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
</div>
<p>Wow, it is still a linear classifier. Now, let’s further relax the conditions to the most general case, where the two distributions have completely different and arbitrary covariance matrices.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="math display">\[
  \boldsymbol{\Sigma}_1 =   \left(
  \begin{matrix}
    0.5 &amp; 0.25 \\
    0.25 &amp; 1
  \end{matrix}
  \right)
\]</span> <span class="math display">\[
  \boldsymbol{\Sigma}_2 =   \left(
  \begin{matrix}
    1 &amp; -0.3 \\
    -0.3 &amp; 0.5
  \end{matrix}
  \right)
\]</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/L4_lda3.gif" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
<p>From the animation above, we can that the decision boundary is not linear anymore. Alright, we can draw a conclusion: the decision boundary in GDA is determined by the covariance matrices of the two distributions. If the two distributions share the same covariance matrix, then the classifier is linear, often referred to as Linear Discriminant Analysis (LDA). Otherwise, it is nonlinear, though not overly flexible—the decision boundary is a quadratic function, which is why it’s also known as Quadratic Discriminant Analysis (QDA).</p>
<p>Of course, this is just an empirical conclusion and not strictly formal. If you’re interested, you can try deriving the decision boundary using the density function of the multivariate normal distribution. You can refer to a textbook for a deeper understanding—we won’t go into detail here. (Or leave it to Sia in future. ToDo4Sia, write a separate note to explain. )</p>
</section>
<section id="training-algorithm-and-example-in-r" class="level3">
<h3 class="anchored" data-anchor-id="training-algorithm-and-example-in-r">4.2.3 Training algorithm and Example in R</h3>
<p>One relatively unique aspect is that the algorithm for training this classifier is very simple—it simply estimates the parameters of the two populations based on the data. More specifically, suppose we have a data set containing both target variable <code>y</code> and the data matrix of feature variables, <code>X</code>. We can use the target variable to find all rows from each class in <code>X</code> and estimate the means and covariance matrix respectively. Then we are just ready to construct the classifier with those parameters estimation.</p>
<p>There’s just one detail to pay attention to: in LDA, how do we estimate the shared covariance matrix? This shared covariance is called the pooled covariance matrix. You might recall that in a two-sample t-test in basic statistics, we also need to calculate the pooled variance. It’s the same principle here. To estimate the pooled covariance matrix, we can first mean-center each data set separately and then use all the mean-centered data to estimate the covariance matrix.</p>
<p>Let’s illustrate this with the following example in R.</p>
<p><strong>R Examples</strong></p>
<p>First, we import a data set for this demo. Here, we use iris data, but only consider two two species, <code>versicolor</code> and <code>virginica</code></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> iris[<span class="dv">51</span><span class="sc">:</span><span class="dv">150</span>,]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>Species <span class="ot">=</span> <span class="fu">as.factor</span>(<span class="fu">as.numeric</span>(data<span class="sc">$</span>Species))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(data<span class="sc">$</span>Species) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"versicolor"</span>,<span class="st">"virginica"</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(data[,<span class="sc">-</span><span class="dv">5</span>], <span class="at">col =</span> data[,<span class="dv">5</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="l4_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Next, we want to write a R function to classify each flower based on the 4 feature variables.</p>
<p>Second, we can use target variable <code>y</code> to find out the rows of <code>X</code> for each species and estimate the mean and contrivance matrix.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># split data to X and Y</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">as.matrix</span>(data[,<span class="sc">-</span><span class="dv">5</span>])</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> data[,<span class="dv">5</span>]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># estimate the mean vectors and covariance matrices </span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>(<span class="at">mu1 =</span> <span class="fu">colMeans</span>(X[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>,]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
       5.936        2.770        4.260        1.326 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>(<span class="at">S1 =</span> <span class="fu">cov</span>(X[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>,]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length   0.26643265  0.08518367   0.18289796  0.05577959
Sepal.Width    0.08518367  0.09846939   0.08265306  0.04120408
Petal.Length   0.18289796  0.08265306   0.22081633  0.07310204
Petal.Width    0.05577959  0.04120408   0.07310204  0.03910612</code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>(<span class="at">mu2 =</span> <span class="fu">colMeans</span>(X[<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>),]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
       6.588        2.974        5.552        2.026 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>(<span class="at">S2 =</span> <span class="fu">cov</span>(X[<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>),]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length   0.40434286  0.09376327   0.30328980  0.04909388
Sepal.Width    0.09376327  0.10400408   0.07137959  0.04762857
Petal.Length   0.30328980  0.07137959   0.30458776  0.04882449
Petal.Width    0.04909388  0.04762857   0.04882449  0.07543265</code></pre>
</div>
</div>
<p>Now, we are ready to build the function of our classifier.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># function for making decision</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>classifier <span class="ot">=</span> <span class="cf">function</span>(x,mu1,S1,mu2,S2){</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Here, we use function 'dmvnorm' in package 'mvtnorm'</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  ell1 <span class="ot">=</span> <span class="fu">dmvnorm</span>(x,mu1,S1)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>  ell2 <span class="ot">=</span> <span class="fu">dmvnorm</span>(x,mu2,S2)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>  res <span class="ot">=</span> <span class="fu">ifelse</span>(ell1 <span class="sc">&gt;</span> ell2, <span class="st">"versicolor"</span>, <span class="st">"virginica"</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(res)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can apply our classifier on the 27th flower and check the accuracy of the classifier.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>id <span class="ot">=</span> <span class="dv">27</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">classifier</span>(X[id,],mu1, S1,mu2,S2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "versicolor"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>y[id]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] versicolor
Levels: versicolor virginica</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>res <span class="ot">=</span> <span class="fu">numeric</span>(<span class="dv">100</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>){</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  res[i] <span class="ot">=</span> <span class="fu">classifier</span>(X[i,],mu1, S1,mu2,S2)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>(<span class="at">acc =</span> <span class="fu">mean</span>(res <span class="sc">==</span> y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.97</code></pre>
</div>
</div>
<p>Looks good. Quiz: is it LDA or QDA? You need to estimate the pooled contrivance matrix if you want to go for QDA.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>demeanX <span class="ot">=</span> X</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>demeanX[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>,] <span class="ot">=</span> X[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>,] <span class="sc">-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(mu1,<span class="dv">50</span>), <span class="at">ncol =</span> <span class="dv">4</span>, <span class="at">byrow =</span> T)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>demeanX[<span class="dv">51</span><span class="sc">:</span><span class="dv">100</span>,] <span class="ot">=</span> X[<span class="dv">51</span><span class="sc">:</span><span class="dv">100</span>,] <span class="sc">-</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(mu2,<span class="dv">50</span>), <span class="at">ncol =</span> <span class="dv">4</span>, <span class="at">byrow =</span> T)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>S <span class="ot">=</span> <span class="fu">cov</span>(demeanX)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="further-discussion" class="level2">
<h2 class="anchored" data-anchor-id="further-discussion">4.3 Further Discussion</h2>
<p>So far so good! We have developed the first model, or algorithm, for a binary classification problem. I hope you got a real feel for classifiers. Next, I will discuss this classifier from various perspectives.</p>
<section id="more-carefully" class="level3">
<h3 class="anchored" data-anchor-id="more-carefully">4.3.1 More carefully</h3>
<p>Let’s go back to the initial motivating example. In fact, we are still missing a crucial piece of information for this problem: the distribution of the target variable. For example, suppose I add a condition before you make a decision: this observation is from an athlete in the Swedish delegation at the Olympic Village. Then, you certainly wouldn’t classify this person as belonging to the Chinese group. This is an extreme example, but it truly highlights an issue: the distribution of the label variable can impact your prediction. Here, the distribution of label can be presented as <span class="math inline">\(\Pr(Y=1) = 1\)</span> and <span class="math inline">\(\Pr(Y=-1) = 0\)</span>. Let’s modify this example to make it less extreme. Suppose I obtained this observation in Idivuoma. You can Google this place name, and you’ll find that it would be quite reasonable to say that the proportion of Swedish people here is 95%. Then, how do you make the prediction?</p>
<p>Let’s see a toy example. As shown in the image below, let’s assume we have two categories, and their proportions are not 50/50. It’s clear that the blue category appears significantly more often than the orange one. Now, I have a green point, and its distance to the orange center is significantly shorter than to the blue center. This means that the likelihood of the green point under the orange model is higher than under the blue model. So, the question is: which category do you think it belongs to?</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="l4_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" style="width:66.6%"></p>
<figcaption>500 blue points generated from the bivariate normal distribution with mean (0,0), equal variance 1, and correlation 0. 10 orange points are observations from bivariate normal with mean (3,0), and the same covariance matrix. The green point is (2,1).</figcaption>
</figure>
</div>
</div>
</div>
<p>In this example, the green point is closer to the orange group, however, a more reasonable label is blue since we have more chance to observe a blue point than an orange point in the neighborhood of green point. Next, we discuss how to use the prior probability to correct our prediction. First, let’s clarify our decision rule. Essentially, our previous decision-making method can be summarized by an expression.</p>
<p><span class="math display">\[
  \widehat{y}=\arg \max_{y}\Pr(\textbf{x}|y)
\]</span> In this expression, <span class="math inline">\(\arg \max_{y}\)</span> means return the value of <span class="math inline">\(y\)</span> such that the conditional probability (likelihood) of observation of <span class="math inline">\(\textbf{x}\)</span>, <span class="math inline">\(\Pr(\textbf{x}|y)\)</span>, is larger. To take the prior probability <span class="math inline">\(\Pr(y = 1)\)</span> and <span class="math inline">\(\Pr(y = -1)\)</span> into account, we can add this prior probability on the RHS of the expression, such that the majority gets more weight for the final decision. That is <span class="math display">\[
  \widehat{y}=\arg \max_{y} \Pr(\textbf{x}|y)\Pr(y)
\]</span> If we normalize this expression by <span class="math inline">\(\Pr(\textbf{x})\)</span>, and apply Bayes formula, then we can get the final expression of our classifier, that is <span class="math display">\[
  \widehat{y} = \arg \max_{y} \Pr(y| \textbf{x})
\]</span> So, instead of evaluating the conditional likelihood, we predict the value of target variable <span class="math inline">\(y\)</span> as the one with larger posterior likelihood. How does this change affect the decision boundary of LDA? We still need a bit math to make it clear, but the conclusion is simple: this change only affects the bias term <span class="math inline">\(w_0\)</span> of the decision boundary. That is the bias term will be corrected as <span class="math inline">\(w_0-\log\left(\Pr(y=-1)/\Pr(y=1)\right)\)</span>.</p>
<p><strong>Remark</strong>: Actually, <span class="math inline">\(\widehat{y} = \arg \max_{y} \Pr(y| \textbf{x})\)</span> is the general decision rule for classification. It is not only for GDA classifier, but also for other classifier, for example, we will see it again in the discussion of logistic regression.</p>
</section>
<section id="multiple-labels" class="level3">
<h3 class="anchored" data-anchor-id="multiple-labels">4.3.2 Multiple labels</h3>
<p>So far, we discussed the binary-label case, however, it can be naturally extend to a multiple-label setting. In a binary-label case, we only need to evaluate the posterior likelihood when <span class="math inline">\(y=1\)</span> or <span class="math inline">\(-1\)</span>. In a <span class="math inline">\(k\)</span>-label setting, we need to estimate <span class="math inline">\(k\)</span> different Normal model and evaluate <span class="math inline">\(k\)</span> posterior likelihood of observation <span class="math inline">\(\textbf{x}\)</span>, then predict the label as the one holding the largest posterior likelihood.</p>
</section>
<section id="r-functions-for-gda" class="level3">
<h3 class="anchored" data-anchor-id="r-functions-for-gda">4.3.3 R functions for GDA</h3>
<p>We have implemented this idea in subsection 4.2.3. Is there any functions that can perform LDA and QDA directly? Yes, you can apply functions <code>lda</code> and <code>qda</code> in <code>MASS</code> package. We use the same demo data as before.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">=</span> iris[<span class="dv">51</span><span class="sc">:</span><span class="dv">150</span>,]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>Species <span class="ot">=</span> <span class="fu">as.factor</span>(<span class="fu">as.numeric</span>(dat<span class="sc">$</span>Species))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(dat<span class="sc">$</span>Species) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"versicolor"</span>,<span class="st">"virginica"</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(dat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "data.frame"</code></pre>
</div>
</div>
<p><strong>Remark</strong>: Notice that the type of <code>data</code> is data frame. It is very essential. Typically, functions in R packages require the input data to be of type ‘data frame’. This simplifies the function syntax and makes it easier to use the resulting model for predictions.</p>
<p>Next, we apply <code>lda</code> to train the classifier.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>model_lda <span class="ot">=</span> <span class="fu">lda</span>(Species<span class="sc">~</span>., <span class="at">data =</span> dat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>predict_res <span class="ot">=</span> <span class="fu">predict</span>(model_lda, dat)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(predict_res)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>List of 3
 $ class    : Factor w/ 2 levels "versicolor","virginica": 1 1 1 1 1 1 1 1 1 1 ...
 $ posterior: num [1:100, 1:2] 1 0.999 0.997 0.994 0.991 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:100] "51" "52" "53" "54" ...
  .. ..$ : chr [1:2] "versicolor" "virginica"
 $ x        : num [1:100, 1] -2.47 -1.94 -1.53 -1.34 -1.26 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:100] "51" "52" "53" "54" ...
  .. ..$ : chr "LD1"</code></pre>
</div>
</div>
<p>The usage of <code>qda</code> is very similar to <code>lda</code>, so I leave it for you to explore. Currently, we have a classifier in the form of an R function, which is very convenient to use. But what exactly does it look like? In other words, as a linear classifier, how can we write out its decision boundary according to the R outputs?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>model_lda</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
lda(Species ~ ., data = dat)

Prior probabilities of groups:
versicolor  virginica 
       0.5        0.5 

Group means:
           Sepal.Length Sepal.Width Petal.Length Petal.Width
versicolor        5.936       2.770        4.260       1.326
virginica         6.588       2.974        5.552       2.026

Coefficients of linear discriminants:
                    LD1
Sepal.Length -0.9431178
Sepal.Width  -1.4794287
Petal.Length  1.8484510
Petal.Width   3.2847304</code></pre>
</div>
</div>
<p>We’ll discuss this in the next subsection.</p>
</section>
<section id="fishers-idea" class="level3">
<h3 class="anchored" data-anchor-id="fishers-idea">4.3.4 Fisher’s idea</h3>
<p>How can we write out the decision boundary for LDA based on R outputs? Well, let’s start by discussing a simpler problem.</p>
<p><strong>Question 1</strong>: <em>Suppose we have only one feature variable—how would we classify in this case?</em></p>
<p>First, one thing is certain: we need to find a threshold value for comparison, just like in the coin sort example. So, the question is, from a classification perspective, how do we choose the optimal threshold value? Let’s take a look at the figure below.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="l4_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The above figure shows the distribution of the feature variable <span class="math inline">\(X\)</span> for two populations, both following a normal distribution with a shared variance. Now, here’s the question: among the three suggested threshold values—red, blue, and purple—which do you think is most suitable for classification? The red dashed line is definitely not suitable. Although the blue population would be correctly classified, most observations to the right of the red line would be misclassified. Similarly, the blue dashed line is slightly better but still has a 50% chance of misclassification. Given this, the purple dashed line appears to be the optimal classification boundary. The next question is, do you know how to calculate the position of the purple dashed line? From the figure, it’s clear that the purple threshold value is simply the average of the centers (means) of the two populations. Therefore, the decision boundary for a single-feature Linear Discriminant Analysis (LDA) is: <span class="math display">\[
  X = \frac{\mu_1 + \mu_2}{2}
\]</span></p>
<p>where <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> are the means of the two populations, i.e.&nbsp;an observation with a value of <span class="math inline">\(X\)</span> larger than <span class="math inline">\(\frac{\mu_1 + \mu_2}{2}\)</span> will be classified as blue.</p>
<p>All right, I can explain Fisher’s idea now. When we have multiple feature variables, we first assign a weight to each feature. Then, we calculate a weighted sum of all the feature variables to produce a score value. <span class="math display">\[
  \text{score} = w_1 x_1 + w_2x_2 + \dots + w_px_p
\]</span> In this way, the problem is reduced to the same form as in Problem 1 above, where we classify based on a threshold applied to this score. In other words, we need to find the mean score for each population separately and then calculate their average. This average serves as the threshold for classification, similar to the approach with a single feature. By comparing an observation’s score to this threshold, we can classify it accordingly.</p>
<p>Let’s look at the outputs of <code>lda</code> function of the previous example again. Think about how to write down the decision boundary of LDA classifier?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>model_lda</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
lda(Species ~ ., data = dat)

Prior probabilities of groups:
versicolor  virginica 
       0.5        0.5 

Group means:
           Sepal.Length Sepal.Width Petal.Length Petal.Width
versicolor        5.936       2.770        4.260       1.326
virginica         6.588       2.974        5.552       2.026

Coefficients of linear discriminants:
                    LD1
Sepal.Length -0.9431178
Sepal.Width  -1.4794287
Petal.Length  1.8484510
Petal.Width   3.2847304</code></pre>
</div>
</div>
<p>Let me tell the quick answer. The last part of the outputs, ‘coefficients of linear discriminant’, tells us the weights for calculating the scores. Then, the mean scores of <code>versicolor</code> can be calculated as the weighted sum of means for <code>versicolor</code> in the ‘Group means’ <span class="math display">\[
  \textcolor{blue}{-0.94} \times 5.94 + (\textcolor{blue}{-1.48}) \times 2.77 + \textcolor{blue}{1.85} \times 4.26 + \textcolor{blue}{3.28} \times 1.32 = 2.53
\]</span> Similarly, the mean scores of <code>virginica</code> is <span class="math display">\[
  \textcolor{blue}{-0.94} \times 6.59 + (\textcolor{blue}{-1.48}) \times 2.97 + \textcolor{blue}{1.85} \times 5.55 + \textcolor{blue}{3.28} \times 2.03 = 6.34
\]</span> <strong>Quiz</strong>: Please write down the decision boundary of the LDA classifier.</p>
<p>So far, we only explained the first step of Fisher’s idea. However, how to find the optimal weights is still unclear. Let’s continue to investigate with the next question.</p>
<p><strong>Question 2</strong> (<span title="Not essential and you may skip"> <strong>NE</strong> </span>): <em>What are the conditions for the optimal weights for a classification problem?</em></p>
<p>You might think the answer is simple. Referring back to the single feature variable case, based on the analysis above, the further apart the centers of the two populations are, the smaller the overlapping portion of their distributions will be, and the more likely we are to get a better threshold value. However, in reality, the distance between the centers of the two distributions alone does not fully reflect the optimal situation. See the figure below.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="l4_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>First, which situation do you think would lead to a better classifier? Clearly, in the case on the RHS, we can obtain a better classifier than in the case in LHS, because the two populations on the RHS have very little overlap. However, let’s take a look at their mean differences. The mean difference on the LHS is 15, which is greater than the 5 on the RHS.</p>
<p><strong>Summary</strong>: We need to find a set of weights through which we can calculate the score values for the two populations. The best set of weights is the one that minimizes the distance between the centers of the score values for the two classes, while also minimizing the variance of the score values within each class. This optimal weights is called Fisher’s projection.</p>
<p>For now, let’s leave the algorithm for calculating the optimal weights to your future self, or ToDo4Sia. Here, you only need to know that the decision boundary obtained through Fisher’s projection is the same as the decision boundary from LDA. This is why LDA is often referred to as Fisher’s Linear Discriminant Analysis.</p>
</section>
</section>
<section id="model-evaluations" class="level2">
<h2 class="anchored" data-anchor-id="model-evaluations">4.4 Model Evaluations</h2>
<p>The evaluation of a classification model is crucial to assess its performance and ensure its effectiveness in real-world applications. Accuracy is the first metric that comes to mind when evaluating a model, but it is <strong>not sufficient</strong>. For example, in a study with 100 observations, where 95 are healthy individuals and 5 are cancer patients, a model that simply classifies every observation as healthy would achieve 95% accuracy. However, this would fail to identify the cancer patients, making the model useless for the task at hand. Next, we will explore some model evaluation methods to better understand and measure the performance of classification models.</p>
<section id="confusion-matrix-and-related-statistics" class="level3">
<h3 class="anchored" data-anchor-id="confusion-matrix-and-related-statistics">4.4.1 Confusion Matrix and related statistics</h3>
<p>A confusion matrix is a powerful tool used to evaluate the performance of a classification model. It shows the counts of actual versus predicted classifications, providing insights into how well the model performs across different classes.</p>
<p>A general confusion matrix for a binary classification problem has the following form:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Predicted Positive (P)</th>
<th>Predicted Negative (N)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual Positive (P)</strong></td>
<td>True Positive (TP)</td>
<td>False Negative (FN)</td>
</tr>
<tr class="even">
<td><strong>Actual Negative (N)</strong></td>
<td>False Positive (FP)</td>
<td>True Negative (TN)</td>
</tr>
</tbody>
</table>
<p>In the matrix, the rows represent the actual class of the observations in the data set, i.e.&nbsp;the true labels. The first row (Actual Positive) contains all cases that actually belong to the positive class (e.g., cancer patients), while, the second row contains all cases that actually belong to the negative class (e.g., healthy individuals).</p>
<p>The columns represent the predicted class according to the model. The first column (Predicted Positive) contains all cases that the model predicted to be positive (e.g., predicted cancer), and the second column (Predicted Negative) contains all cases that the model predicted to be negative (e.g., predicted healthy).</p>
<p>With this structure, each cell in the matrix contains different meaning:</p>
<ul>
<li><strong>TP</strong>: The number of correct predictions where the actual class is positive and the model predicted positive.</li>
<li><strong>FP</strong>: The number of incorrect predictions where the actual class is negative but the model predicted positive.</li>
<li><strong>FN</strong>: The number of incorrect predictions where the actual class is positive but the model predicted negative.</li>
<li><strong>TN</strong>: The number of correct predictions where the actual class is negative and the model predicted negative.</li>
</ul>
<p>These metrics provide a comprehensive way to assess the performance of a classification model, especially when dealing with imbalanced data sets. For example, the confusion matrix of the useless classifier mentioned above is displayed below.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>Predicted Cancer</th>
<th>Predicted Healthy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual Cancer</strong></td>
<td>0</td>
<td>5</td>
</tr>
<tr class="even">
<td><strong>Actual Healthy</strong></td>
<td>0</td>
<td>95</td>
</tr>
</tbody>
</table>
<p>In this example, in addition to accuracy, we can further calculate other statistics to comprehensively evaluate the performance of this classifier.</p>
<ul>
<li><p><strong>Sensitivity</strong>: (True positive rate) The proportion of true positive predictions out of all actual positive cases. This statistic indicates how effectively the classifier identifies the cases of interest, showing how sensitive it is to detecting positive instances.<br>
<span class="math display">\[
\text{Sensitivity} = \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{0}{0+5} = 0
\]</span></p></li>
<li><p><strong>Specificity</strong>: (True negative rate) The proportion of true negative predictions out of all actual negative cases. <span class="math display">\[
\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}} = \frac{95}{0+95} = 1
\]</span> In the lazy classifier example, although this lazy classifier has very extremely high specificity, 100%, and high accuracy, 95%, we can’t say it is good at all as the extremely low sensitivity, 0. So, people usually simultaneously use the three statistics, i.e.&nbsp;accuracy, sensitivity, and specificity, to evaluate the performance of a classifier.</p></li>
<li><p><strong>Precision</strong>: Sometime, people are also interested in the quality of positive predictions, then the proportion of true positive predictions out of all predicted positive cases, i.e.&nbsp;precision, is used. <span class="math display">\[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
\]</span> In the lazy classifier example, it is an extreme cases and precision is not defined as no case is predicted as positive. Mathematically, we also arrive at the same conclusion. Since there are no positive predictions, 0 appears in the denominator, and therefore this ratio is not defined.</p></li>
</ul>
</section>
<section id="more-choices" class="level3">
<h3 class="anchored" data-anchor-id="more-choices">4.4.2 More Choices</h3>
<p>It is often difficult and inconvenient to compare different things by considering several dimensions at once. The best approach is to find a statistic that can simultaneously evaluate a classifier from multiple perspectives.</p>
<p><strong>F-score</strong>: it is a statistic that combines <strong>precision</strong> and <strong>sensitivity</strong> into a single measure to evaluate the performance of a classifier, especially in situations where both false positives and false negatives are important. Essentially, it is the harmonic mean of precision and recall, giving a balance between the two metrics.</p>
<p><span class="math display">\[
  \text{F-score} = 2 \times \frac{\text{Precision} \times \text{sensitivity}}{\text{Precision} + \text{sensitivity}}
\]</span> F-score ranges from 0 to 1, and it indicates the perfect precision and sensitivity (best performance) for a classifier when it is <span class="math inline">\(1\)</span>, but worst performance when it is <span class="math inline">\(0\)</span> With the same example above, suppose we have a classifier always predict a person as a cancer patient, then this classifier has perfect sensitivity but very low precision which is 0.05. The F-score is <span class="math inline">\(2\times\frac{0.05 \times 1}{0.05 + 1} = 0.095\)</span>. If someone is willing to use this classifier, they must have ignored the negative effects of misclassifying a healthy person as a cancer patient.</p>
<p><strong>Cohen Kappa Statistics</strong>: it is another option that can be used to comprehensively evaluate a classifier. Essentially, it is used to measure the agreement between two raters (classifiers). For example, suppose there are two classifiers both classify 100 cases. If the two classifiers agree with each other, then we can get the following matrix that is similar to the idea of confusion matrix.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Classifier 2: Positive</strong></th>
<th><strong>Classifier 2: Negative</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Classifier 1: Positive</strong></td>
<td>30</td>
<td>0</td>
</tr>
<tr class="even">
<td><strong>Classifier 1: Negative</strong></td>
<td>0</td>
<td>70</td>
</tr>
</tbody>
</table>
<p>Ignoring whether they are good classifiers, we can say that the two classifiers have the exactly same predictions, in another word, the two classifiers agree with each other. Let’s see another example,</p>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Classifier 2: Positive</strong></th>
<th><strong>Classifier 2: Negative</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Classifier 1: Positive</strong></td>
<td>30</td>
<td>10</td>
</tr>
<tr class="even">
<td><strong>Classifier 1: Negative</strong></td>
<td>5</td>
<td>55</td>
</tr>
</tbody>
</table>
<p>In this case, apparently the two classifiers don’t have exactly the same predictions, since there are 5 cases that are predicted as negative by classifier 1 but positive by classifier 2, also 10 disagreements are in an opposite way. However, they still show a certain level of agreement. So, the question is can we design a statistic to quantify the agreement. Of course, the answer is Cohen Kappa statistic. Before showing you the formula of Kappa statistic, let’s clarify one thing. If we set ‘Classifier 1’ as the classifier you want to evaluate, and ‘Classifier 2’ as the ground truth, then this statistic will measure the agreement between your model and the ground truth, and the matrix becomes the confusion matrix.</p>
<p>Next, let’s have a look at the calculations of this statistic with the notations in a confusion matrix. <span class="math display">\[
  \kappa = \frac{P_o - P_e}{1 - P_e}
\]</span></p>
<ul>
<li><span class="math inline">\(P_o\)</span> is the <strong>observed agreement</strong>: the proportion of times the two raters agree, i.e.&nbsp;the accuracy <span class="math display">\[
P_o = \frac{TP + TN}{TP + TN + FP + FN}
\]</span></li>
<li><span class="math inline">\(P_e\)</span> is the <strong>expected agreement</strong>: the proportion of times the two raters would be expected to agree by chance <span class="math display">\[
P_e = \left( \frac{(TP + FP)(TP + FN)}{(TP + TN + FP + FN)^2} \right) + \left( \frac{(TN + FP)(TN + FN)}{(TP + TN + FP + FN)^2} \right)
\]</span> In general, we can use the following table as reference to evaluate a classifier. Here is the information in table format:</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Kappa (κ) Value</strong></th>
<th><strong>Interpretation</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>κ ≥ 0.81</td>
<td>Almost perfect agreement</td>
</tr>
<tr class="even">
<td>0.61 ≤ κ &lt; 0.80</td>
<td>Substantial agreement</td>
</tr>
<tr class="odd">
<td>0.41 ≤ κ &lt; 0.60</td>
<td>Moderate agreement</td>
</tr>
<tr class="even">
<td>0.21 ≤ κ &lt; 0.40</td>
<td>Fair agreement</td>
</tr>
<tr class="odd">
<td>κ ≤ 0.20</td>
<td>Slight agreement</td>
</tr>
<tr class="even">
<td>Negative</td>
<td>Worse than random chance</td>
</tr>
</tbody>
</table>
<p>Let’s go back to the previous example:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Classifier 2: Positive</strong></th>
<th><strong>Classifier 2: Negative</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Classifier 1: Positive</strong></td>
<td>30 (TP)</td>
<td>10 (FP)</td>
</tr>
<tr class="even">
<td><strong>Classifier 1: Negative</strong></td>
<td>5 (FN)</td>
<td>55 (TN)</td>
</tr>
</tbody>
</table>
<p>In this case, <span class="math inline">\(\kappa = 0.68\)</span>, and it suggests a substantial agreement between the two classifiers. If the ‘classifier 2’ represents the ground truth, then <span class="math inline">\(\kappa\)</span> indicates that ‘classifier 1’ is a rather good classifier.</p>
<div style="text-align: center; margin: 30px 0">
<p><a href="../../../Courses/c_mlwr1_2024/l4/l4_home.html"><strong>Lecture 4 Homepage</strong></a></p>
</div>


</section>
</section>

</main> <!-- /main -->
<div style="display: flex; justify-content: space-between; padding: 10px; font-size: 14px; color: #666; border-top: 1px solid #ddd;">
  <div>© 2024 Xijia Liu. All rights reserved. Contact: xijia.liu AT umu.se</div>
  <div><img src="../../../Images/logo.png" alt="Logo" style="width: 60px;"></div>
</div>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>