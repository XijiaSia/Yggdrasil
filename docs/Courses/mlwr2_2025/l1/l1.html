<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Lecture 1: Feature Extraction and PCA – My Yggdrasil</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../Images/main_logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">My Yggdrasil</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../01_c_index.html"> 
<span class="menu-text">Norns’ Blessing</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../02_m_index.html"> 
<span class="menu-text">Mímisbrunnr</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../03_s_index.html"> 
<span class="menu-text">Skalds</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../04_about_me.html"> 
<span class="menu-text">About me</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-feature-extraction" id="toc-introduction-to-feature-extraction" class="nav-link active" data-scroll-target="#introduction-to-feature-extraction">1. Introduction to feature extraction</a></li>
  <li><a href="#principle-components-analysis" id="toc-principle-components-analysis" class="nav-link" data-scroll-target="#principle-components-analysis">2. Principle Components Analysis</a>
  <ul class="collapse">
  <li><a href="#what-is-pca" id="toc-what-is-pca" class="nav-link" data-scroll-target="#what-is-pca">2.1 What is PCA?</a></li>
  <li><a href="#pca-problem" id="toc-pca-problem" class="nav-link" data-scroll-target="#pca-problem">2.2 PCA Problem</a></li>
  </ul></li>
  <li><a href="#pca-algorithm-and-a-simple-example" id="toc-pca-algorithm-and-a-simple-example" class="nav-link" data-scroll-target="#pca-algorithm-and-a-simple-example">3. PCA Algorithm and A Simple Example</a>
  <ul class="collapse">
  <li><a href="#pca-algorithm" id="toc-pca-algorithm" class="nav-link" data-scroll-target="#pca-algorithm">3.1 PCA Algorithm</a></li>
  <li><a href="#a-simple-example" id="toc-a-simple-example" class="nav-link" data-scroll-target="#a-simple-example">3.2 A Simple Example</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">4. Discussion</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 1: Feature Extraction and PCA</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In the first part of the course, we discussed one approach to solving the problem of data dimensionality reduction: <strong>feature selection</strong>. By using different feature selection methods, we can control the size of the feature space, thereby obtaining relatively simpler models to overcome the overfitting issue. In this lecture, we will introduce another type of methods of data dimensionality reduction, which is also an essential concept in machine learning: <strong>feature extraction</strong>. Feature extraction can be understood as a general term for methods that construct new variables from the original ones.</p>
<p>By using the original feature variables to create a relatively smaller set of new feature variables, we can control the size of the feature space. At the same time, a good machine learning model depends on two factors: efficient algorithms and a well-designed feature space. It is not hard to understand that if we have an absolutely ideal feature space—one that is linearly separable—then simply applying a basic algorithm can yield a perfect predictive model. Therefore, feature extraction plays a crucial role in machine learning applications.</p>
<!--- --->
<section id="introduction-to-feature-extraction" class="level1">
<h1>1. Introduction to feature extraction</h1>
<p>So, how does feature extraction create new feature variables? Let’s start by looking at a few examples. The first example is one we’re all familiar with: <em>BMI</em>, or Body Mass Index. It is a measurement used to assess whether a person has a healthy body weight. It is calculated by dividing a person’s weight in kilograms by the square of their height in meters, <span class="math display">\[
  \text{BMI} = \frac{\text{weight (kg)}}{\text{height (m)}^2}
\]</span></p>
<p>Clearly, BMI is a new variable created from the original variables. It not only reduces the number of variables (from two variables, height and weight, to one BMI variable) but also provides a more effective and convenient way to assess overall health. Thus, BMI is a classic example of feature extraction. From a mathematical perspective, BMI is a function of the original variables, height and weight, i.e.&nbsp;<span class="math inline">\(\text{BMI} = g(h,w)\)</span>.</p>
<p>By the way, in statistics, we usually refer to variables derived from original variables as <strong>latent variables</strong>. Latent variables are typically used to represent quantities that cannot be directly observed or measured. This is easy to understand: we can use a ruler and a scale to measure height and weight, but there is no direct instrument to measure BMI—it has to be calculated from height and weight. Therefore, in machine learning, we often refer to the space formed by extracted feature variables as the <strong>latent space</strong>.</p>
<p>A more complex example is the <em>Gini coefficient</em>. The Gini coefficient measures inequality in income or wealth distribution. It ranges from 0 (perfect equality) to 1 (perfect inequality) and is based on the Lorenz curve, comparing actual distribution to perfect equality. For example, the map below shows the Gini coefficients of various countries, allowing us to understand their levels of equity.</p>
<div class="custom-figure-block">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/gini0.png" class="img-fluid figure-img" style="width:88.8%"></p>
<figcaption>Figure source: Wiki</figcaption>
</figure>
</div>
</div>
<p>So, how is the Gini coefficient calculated? The Gini coefficient is calculated using the Lorenz curve, which plots the cumulative share of income or wealth against the cumulative share of the population. The formula is: <span class="math display">\[
  G = \frac{A}{A + B}
\]</span> where <span class="math inline">\(A\)</span> is the area between the Lorenz curve and the line of perfect equality, and <span class="math inline">\(A + B\)</span> is the total area below the line of perfect equality.</p>
<div class="custom-figure-block">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/gini.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:88.8%"></p>
</figure>
</div>
</div>
<p>It can also be computed directly using income data as: <span class="math display">\[
  G = 1 - \sum_{i=1}^{n} (y_i + y_{i-1})(x_i - x_{i-1})
\]</span> where <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are cumulative population and income shares. People can collect household income data from different countries, but the Gini coefficient must be computed based on these raw data.</p>
<p>Now, let’s look at an example more relevant to machine learning. Take a look at the scatter plot below. Clearly, we want to use the two feature variables, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, to distinguish the red points from the black points. You can think about how to create a new variable using the two feature variables to solve this classification problem.</p>
<div class="custom-Rfigure-block">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="l1_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<p>You may already have the answer. Yes, we can calculate the distance of each point from the origin to create a new feature variable, i.e.&nbsp;for each point <span class="math inline">\(i\)</span>,</p>
<p><span class="math display">\[
  D_i = \sqrt{ X_1^2 + X_2^2 }
\]</span></p>
<p>Then, we can choose an appropriate cutoff to differentiate the black points from the red points. This is also an example of feature extraction, where the extracted feature is a function of the original feature variables, i.e.&nbsp;<span class="math inline">\(D = g(X_1, X_2)\)</span>.</p>
<p>By now, I guess you’ve probably realized something. Do you remember how we introduced linear classifiers in the first part of the course? Yes, first, we compute the weighted sum of the feature variables, then compare it with an appropriate cutoff. In fact, a linear classifier is essentially doing feature extraction first, and then comparing the extracted feature with a cutoff. This feature extraction is also a function of the original feature variables, that is:</p>
<p><span class="math display">\[
  Z = g(X_1, X_2, \dots, X_p) = \sum_{j = 1}^p w_jX_j
\]</span></p>
<p>Now, we can summarize the discussion. <strong>Feature extraction</strong> is about finding an appropriate function <span class="math inline">\(g()\)</span> of the original feature variables to transform and create new feature variables.</p>
<p>The examples above share a common characteristic: they all design feature extraction methods based on the problem’s specific characteristics, using <strong>domain knowledge</strong> (<strong>prior knowledge</strong>) or specific <strong>rational analysis</strong>. This approach is often referred to as domain knowledge based feature extraction or manual feature extraction. The benefits of this method are obvious, but it has a significant drawback: it heavily depends on prior information. Next, we will introduce the star of this lesson, PCA, which is an algorithm that learns the feature extraction function from the data, i.e.&nbsp;<strong>data driven solution</strong>.</p>
<div class="custom-block2">
<p><strong>Quiz</strong>:</p>
<ol type="1">
<li><p>Use simple language to explain to your family what feature extraction is.</p></li>
<li><p>We introduced the concept of ‘feature mapping’ idea for creating a nonlinear model in the first part of this course. Is feature extraction also a kind of feature mapping?</p></li>
</ol>
</div>
<!--- --->
</section>
<section id="principle-components-analysis" class="level1">
<h1>2. Principle Components Analysis</h1>
<p>PCA is an important data analysis method in statistics, particularly in multivariate statistical analysis. Mathematicians and statisticians have studied it extensively and in great depth. Among them, the <strong>Swedish</strong> 🇸🇪 mathematician and statistician <strong>Herman Wold</strong> made significant contributions to the understanding and expansion of PCA’s essence. In fact, if we review his works on PCA related issues, we can even catch glimpses of the <strong>modern artificial neural network models</strong>. Let us pay tribute to this great pioneer! 🎩👏</p>
<div class="custom-figure-block">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/HermanWold.png" class="img-fluid figure-img" style="width:66.6%"></p>
<figcaption>Herman Wold was a Swedish statistician and econometrician who served as a professor at <strong>Uppsala University</strong> and is renowned for his contributions to sequential regression, partial least squares (PLS), and stochastic processes.</figcaption>
</figure>
</div>
</div>
<section id="what-is-pca" class="level2">
<h2 class="anchored" data-anchor-id="what-is-pca">2.1 What is PCA?</h2>
<div class="custom-figure-block">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/plate-PCA.PNG" class="img-fluid figure-img" style="width:88.8%"></p>
<figcaption>What is PCA? I took this picture when I was waiting for traffic lights close to Stroa Coop at Tomtebo in Umeå in January 2021. What perfect timing! Being stopped by traffic lights isn’t always a bad thing.</figcaption>
</figure>
</div>
</div>
<p>PCA is a linear feature extraction tool. Let’s first provide the definition:</p>
<div class="custom-block2">
<p><strong>PCA</strong> is a <span style="color: red;"> <strong>linear</strong> </span> numerical method for creating a relatively smaller set of mutually <span style="color: green;"> <strong>orthogonal</strong> </span> <span style="color: red;"> <strong>new variables</strong> </span> from the original dataset and the most <span style="color: blue;"> <strong>information</strong> </span> can be preserved in the new dataset.</p>
</div>
<p>Next, let me explain each highlighted key words in the definition.</p>
<p><span style="color: red;"><strong>New variables</strong></span> and <span style="color: red;"><strong>Linear</strong></span>: The term “new variable” is easy to understand. PCA is a type of feature extraction method, and the results of feature extraction are essentially new variables. However, this feature extraction, or the map <span class="math inline">\(g()\)</span>, is not arbitrary; we constrain it to be linear. In other words, all the new variables must satisfy the following equation:</p>
<p><span class="math display">\[
  g_{\textbf{w}}(\textbf{x}) = w_1x_1 + w_2x_2 + \dots + w_px_p
\]</span> where <span class="math inline">\(\textbf{x}\)</span> expresses all the <span class="math inline">\(p\)</span> original feature variables, and <span class="math inline">\(\textbf{w}\)</span> contains all the <span class="math inline">\(p\)</span> coefficients. A straightforward example is the average. The average is a widely used method of summarizing information in real-life situations. It is the time to awkwardly show the generation gap. For example, in old-style music competitions, a singer would typically receive scores from several judges after their performance. Then, the host would say, “Remove the highest score, remove the lowest score, and the final score is……” The final score is just the average. More precisely, it is the truncated mean.</p>
<div class="custom-figure-block">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/scoring.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:88.8%"></p>
</figure>
</div>
</div>
<p>Here, I want to emphasize something for you. To make it more memorable, let me start with a sad memory.</p>
<div class="custom-block2">
<p><strong>Sad memories</strong>: In middle school, I was very good at mathematics and physics, and also terrible at English and literature. The end of every exam was always the most awkward moment for me. Say I got 100 scores both for mathematics and physics, but 0 scores both for English and literature. My teacher simply informed the average score as the overall evaluation of my study to my parents. I guess she used the following formula <span class="math display">\[
  \frac{1}{4} \text{Math} + \frac{1}{4} \text{Physics}+ \frac{1}{4} \text{English} + \frac{1}{4} \text{Literature}
\]</span> Obviously, I was hopeless. However, my smart mother courageously stepped up, she simply adjusted the coefficients of the feature extraction function, <span class="math display">\[
  \frac{1}{2} \text{Math} + \frac{1}{2} \text{Physics}+ 0 \cdot \text{English}+ 0 \cdot \text{Literature}
\]</span> and told me that you are actually great!</p>
</div>
<p>A good choice of coefficients not only can save a young people but also leads to a informative new variable for different purposes. This sad story highlights the role of coefficients in the feature extraction function. In one word, different coefficients lead to different information.</p>
<div class="custom-figure-block">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/handshadow.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:44.4%"></p>
</figure>
</div>
</div>
<p>I would like to use the above picture to close the discussion about this keyword. Essentially, the extracted feature is just the weighted sum of original features, while the weighted sum is called linear combination in linear algebra and the geometry meaning of linear combination is <strong>projection</strong>. I strongly suggest you read about <a href="../../../MathToolBox/la/la_07.html">mathematical projection</a> if you are not familiar with this concept.</p>
<p>This hand shadow game is a good example. If you don’t know it, just looking at the two hands won’t immediately tell you what the performer wants to show the audience. But once the light casts a shadow, the image of a dog becomes clear. The basic idea of feature extraction is similar—by using the right feature extraction function, useful information can be presented in a way that is easy for a computer to recognize. In one words, the new variable can be viewed as a shadow of the object (<span class="math inline">\(\textbf{x}\)</span>) from a proper direction (<span class="math inline">\(\textbf{w}\)</span>).</p>
<p><span style="color: blue;"><strong>Information</strong> </span>: Based on the discussion above, it’s not difficult to see that the goal of the PCA algorithm is to find a set of suitable coefficients to achieve feature extraction. But what does “suitable” coefficients mean? Are there specific criteria for this? To understand the answer to this question, we need to take a closer look at the key term “information”.</p>
<p>In statistics and information theory, we have many measures of information. In PCA, however, we use a simple and familiar measure to quantify how much information a variable contains, that is “<strong>Variance</strong>”. Let’s start with an interesting example.<br>
I will present two events—think about which one you would be more eager to share with your family or friends:</p>
<p><strong>Event 1:</strong> This morning, the sun rose in the east.<br>
<strong>Event 2:</strong> NASA has just admitted to observing UFOs over the past 10 years.</p>
<p>I think you already have the answer. No rush, let me and my wife simulate this scenario, as shown in the picture below.</p>
<div class="custom-figure-block">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/message.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:88.8%"></p>
</figure>
</div>
</div>
<p>On the left, my wife immediately saw through my trick of avoiding cooking at home. On the right, she was genuinely shocked by the news—although she still figured out my trick five minutes later. For us, the amount of information in a message depends on how surprising it is. Essentially, information amount is roughly equal to the <strong>degree of surprise</strong>.</p>
<p>Another example, I tell two students, “You can pass this exam.” The first student has prepared well and is confident in their answers, while the second student didn’t do well and feels uncertain. Clearly, the amount of information my message carries is completely different for them. Therefore, the <strong>degree of surprise</strong> in a message depends on the uncertainty of the event it describes. In statistics, uncertainty is usually measured by <strong>variance</strong>.</p>
<p>Let me use one last example to convince you. Suppose we have an epidemiological dataset about cervical cancer, which includes age, gender, BMI, and various health indicators. Now, which variable do we absolutely not need? Think about what the variance of this variable would be.</p>
<p><span style="color: green;"><strong>Orthogonal</strong> </span>: At this point, we’ve basically understood the core idea of how PCA extracts new variables. However, there’s one more thing to clarify: for a dataset containing many variables, we usually extract a set of new variables. The PCA algorithm has a specific requirement for the relationship between these extracted variables, which is orthogonality. Simply put, orthogonality means that there is no linear relationship between the extracted variables, meaning their covariance is zero. We’ll see this more clearly in a concrete example later.</p>
</section>
<section id="pca-problem" class="level2">
<h2 class="anchored" data-anchor-id="pca-problem">2.2 PCA Problem</h2>
<p>With all this groundwork laid out, it becomes much easier to understand how PCA extracts variables from the original dataset. Simply put, PCA first aims to find a set of coefficients to calculate new variables, and this set of coefficients is designed to maximize the variance of the extracted new variables. Suppose, we have <span class="math inline">\(p\)</span> variables in a dataset. <span class="math display">\[
  \max_{\textbf{w}} \left\{ \text{Var} \left( \underbrace{w_1 \cdot x_1 + w_2 \cdot x_2 + \dots + w_p \cdot x_p}_{\text{extracted feature}} \right) \right\}
\]</span></p>
<p>By solving this optimization problem, we obtain an optimizer <span class="math inline">\(\textbf{w}_1\)</span> that is a set of coefficients for computing the first new variable. We call <span class="math inline">\(\textbf{w}_1\)</span> the <strong>first Principal Component weights (PC weithgs)</strong>, and the variable calculated using these coefficients is commonly known as the <strong>first principal component</strong>. Of course, these are just statistical terms. In machine learning, this is simply a <strong>feature extraction</strong> function obtained through an algorithm under certain constraints.</p>
<p>As mentioned earlier, we usually need to extract a series of new variables from the original dataset to replace the old ones, achieving <strong>dimensionality reduction</strong>. Finding the second set of coefficients is not much different from the previous problem—we still aim to maximize the variance of the resulting variable. However, the key difference is that we need to add a constraint to ensure that we do not obtain the same <strong>first PC weights</strong> again. This constraint is what we call <strong>orthogonality</strong> before.</p>
<p><span class="math display">\[
  \max_{\textbf{w}: \textbf{w} \perp \textbf{w}_1} \left\{ \text{Var} \left( \underbrace{w_1 \cdot x_1 + w_2 \cdot x_2 + \dots + w_p \cdot x_p}_{\text{extracted feature}} \right) \right\}
\]</span> Some linear algebra knowledge is needed to fully understand <span class="math inline">\(\textbf{w} \perp \textbf{w}_1\)</span>, but you can ignore the details for now and just remember two key points:</p>
<ol type="1">
<li>This condition prevents us from obtaining the first set of <strong>PC weights</strong> again.<br>
</li>
<li>The second <strong>principal component</strong> (new variable) obtained this way will be linearly uncorrelated with the first principal component.</li>
</ol>
<p>Of course, if needed, we continue searching for the third set of PC weights. This time, we need to add two orthogonality constraints to ensure it remains uncorrelated with both the first and second principal components, that is <span class="math inline">\(\textbf{w} \perp \textbf{w}_1\)</span> and <span class="math inline">\(\textbf{w} \perp \textbf{w}_2\)</span>. By following this approach, we can continue finding more <strong>PC weights</strong>. In fact, we can obtain up to <span class="math inline">\(p\)</span> new variables—yes, the same number as in the original dataset.<br>
You might be wondering: <strong>How does this achieve dimensionality reduction?</strong> Let’s explore this in the next section with a concrete example.</p>
<!--- --->
</section>
</section>
<section id="pca-algorithm-and-a-simple-example" class="level1">
<h1>3. PCA Algorithm and A Simple Example</h1>
<section id="pca-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="pca-algorithm">3.1 PCA Algorithm</h2>
<p>In the previous section, we discussed what PCA is and the optimization problem it involves. This optimization problem is actually quite easy to solve because it is equivalent to an <strong>eigenvalue decomposition problem</strong>. Here, we will skip the mathematical details and directly present the algorithm, explaining how to obtain PC weights and use them to compute the new data matrix. Finally, we will demonstrate this with a small example.</p>
<div class="custom-algorithm-block">
<div style="font-size: 18px;">
<p><strong>Algorithm</strong>: Principal Components Analysis</p>
</div>
<hr>
<p><strong>Inputs</strong>:</p>
<ul>
<li><span class="math inline">\(\textbf{X}\)</span>, <span class="math inline">\(n\times p\)</span> matrix, containing all <span class="math inline">\(p\)</span> feature variables.</li>
</ul>
<hr>
<p><strong>Steps</strong>:</p>
<ol type="1">
<li>Calculate the covariance matrix, <span class="math inline">\(S_{\textbf{X}}\)</span></li>
<li>Perform eigenvalue decomposition on the covariance matrix, <span class="math inline">\(S_{\textbf{X}} = \textbf{W} \boldsymbol{\Lambda} \textbf{W}^{\top}\)</span>. <span class="math inline">\(\mathbf{W}\)</span> is a <span class="math inline">\(p \times p\)</span> matrix, where each column, <span class="math inline">\(\textbf{w}_j\)</span>, is an eigenvector, and we call it weights matrix. <span class="math inline">\(\boldsymbol{\Lambda}\)</span> is a diagonal matrix, where each diagonal element <span class="math inline">\(\lambda_j\)</span> is the eigenvalue corresponding to its eigenvector <span class="math inline">\(\textbf{w}_j\)</span>.</li>
<li>Calculate the new data matrix: <span class="math inline">\(\mathbf{Z} = \mathbf{XW}\)</span></li>
</ol>
<hr>
<p><strong>Output</strong>: The new data matrix, <span class="math inline">\(\mathbf{Z}\)</span>, the weights matrix, <span class="math inline">\(\mathbf{W}\)</span>, and the matrix of eigenvalues, <span class="math inline">\(\boldsymbol{\Lambda}\)</span>.</p>
</div>
<p>First, it is not difficult to guess that the PC weights we are looking for are exactly the column vectors in <span class="math inline">\(\mathbf{W}\)</span>. Each column vector (eigenvector) <span class="math inline">\(\textbf{w}_j\)</span> is a p-dimensional vector, which contains the weights needed for each original variable when computing the new variables.</p>
<p>Second, an observation no mater in the original data matrix, <span class="math inline">\(\textbf{x}_i\)</span>, i.e.&nbsp;each row in <span class="math inline">\(\textbf{X}\)</span>, or a new observation, <span class="math inline">\(\textbf{x}_{new}\)</span>, we can calculate the new variable (extracted feature) as <span class="math inline">\(\textbf{x}_i^{\top}\textbf{w}_j\)</span> or <span class="math inline">\(\textbf{x}_{new}^{\top}\textbf{w}_j\)</span>. For the entire original data matrix, we can compute all possible extracted feature variables by matrix multiplication at once, i.e.&nbsp;<span class="math inline">\(\mathbf{Z} = \mathbf{XW}\)</span>.</p>
<p>Finally, the eigenvalue matrix, <span class="math inline">\(\boldsymbol{\Lambda}\)</span>, contains the variance of all extracted features, which represents the amount of information they carry.</p>
<div class="custom-block2">
<p><strong>Remark</strong>: In many textbooks, authors emphasize that the mean of all variables should be removed before performing PCA. This step is not essential, but it can lead to slight differences in the calculation of eigenvectors depending on the algorithm used. We will not go into too much detail on this here.</p>
</div>
</section>
<section id="a-simple-example" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-example">3.2 A Simple Example</h2>
<p>In this section, we use a simple example to demonstrate how to implement PCA with a simple program in R. First, we import the data, record some basic information, and visualize the data</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">=</span> iris <span class="co">#import data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">as.matrix</span>(dat[,<span class="sc">-</span><span class="dv">5</span>]) <span class="co">#take the 4 feature variables and save them in a matrix</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">dim</span>(X)[<span class="dv">1</span>] <span class="co">#record the sample size</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">dim</span>(X)[<span class="dv">2</span>] <span class="co">#record the number of variables</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(X, <span class="at">col =</span> dat<span class="sc">$</span>Species) <span class="co">#visualize the data in a pairwise scatter plot</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="l1_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Next we do PCA on data matrix <code>X</code>. First, we calculate the covariance matrix.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>S <span class="ot">=</span> <span class="fu">cov</span>(X) <span class="co"># calculate the covariance matrix</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Second, do eigenvalue decomposition on covariance matrix <code>S</code> and save the results in <code>res</code>. In R, we can apply function <code>eigen</code> to perform eigenvalue decomposition.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>res <span class="ot">=</span> <span class="fu">eigen</span>(S)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(res)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>List of 2
 $ values : num [1:4] 4.2282 0.2427 0.0782 0.0238
 $ vectors: num [1:4, 1:4] 0.3614 -0.0845 0.8567 0.3583 0.6566 ...
 - attr(*, "class")= chr "eigen"</code></pre>
</div>
</div>
<p>As shown above, the results of the eigenvalue decomposition contain all the information we need. slot <code>values</code> contains all the eigenvalues, and <code>vectors</code> contains all the eigenvectors, that is the optimal weights matrix.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>W <span class="ot">=</span> res<span class="sc">$</span>vectors <span class="co"># define the weights matrix W</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>w_1 <span class="ot">=</span> W[,<span class="dv">1</span>] <span class="co"># the first column of W is the first PC weights.</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>w_1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  0.36138659 -0.08452251  0.85667061  0.35828920</code></pre>
</div>
</div>
<p>Above, we define the weights matrix <span class="math inline">\(\textbf{W}\)</span>, and the first column, <span class="math inline">\(\textbf{w}_i\)</span>, contains the optimal weights for calculating the first extracted variables. Next, we extract the first PC of the first flower.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>X[<span class="dv">1</span>, , drop <span class="ot">=</span> F]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     Sepal.Length Sepal.Width Petal.Length Petal.Width
[1,]          5.1         3.5          1.4         0.2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>X[<span class="dv">1</span>, , drop <span class="ot">=</span> F]<span class="sc">%*%</span>w_1 <span class="co"># or, you can try sum(X[1,]*w_1) which is more like a weighted sum.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        [,1]
[1,] 2.81824</code></pre>
</div>
</div>
<p>So, <span class="math inline">\(2.81824\)</span> is just value of first PC for the first flower. We can extract the first PC for all the flowers together, then we get the first new variable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>z_1 <span class="ot">=</span> X<span class="sc">%*%</span>w_1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can print out <code>z_1</code> in your own computer and you will see it is a <span class="math inline">\(150 \times 1\)</span> vector. We can even use matrix multiplication more efficiently to extract all possible principal components (PCs) at once.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">=</span> X<span class="sc">%*%</span>W</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(Z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 150   4</code></pre>
</div>
</div>
<p>You can see that the new data matrix <code>Z</code> has the same dimension as the original data matrix <code>X</code>. But what is the advantage of the new data matrix? We can find the answer by comparing the covariance matrices of the two data matrices.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cov</span>(X),<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length         0.69       -0.04         1.27        0.52
Sepal.Width         -0.04        0.19        -0.33       -0.12
Petal.Length         1.27       -0.33         3.12        1.30
Petal.Width          0.52       -0.12         1.30        0.58</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cov</span>(Z),<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3] [,4]
[1,] 4.23 0.00 0.00 0.00
[2,] 0.00 0.24 0.00 0.00
[3,] 0.00 0.00 0.08 0.00
[4,] 0.00 0.00 0.00 0.02</code></pre>
</div>
</div>
<p>It is easy to see that, compared to the original data matrix, the covariance matrix of the new data matrix is much simpler. The new variables are not only uncorrelated, but their variances gradually decrease.<br>
This means that the amount of information contained in the variables of the new data matrix gradually decreases. The first new variable contains the most information, while the last new variable carries very little. Based on this, we can easily decide which variables to discard from the new matrix. This is the key essence of the PCA algorithm.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(res<span class="sc">$</span>values,<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4.23 0.24 0.08 0.02</code></pre>
</div>
</div>
<p>Finally, let’s take a look at the eigenvalues obtained from the eigenvalue decomposition. Yes, the eigenvalues represent the variances of the new variables, which indicate the amount of information they contain.</p>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">4. Discussion</h2>
<p>Let’s conclude this lecture with a discussion. We call PCA a data-driven solution, meaning it does not rely on any prior information. However, it is important to emphasize that PCA is merely a <strong>linear feature extraction method</strong>. The concept of <strong>linearity</strong> is straightforward: all extracted feature variables are just weighted sums of the original variables. But how can we understand linearity from a <strong>geometric perspective</strong>? Let’s explore this idea with the following toy example.</p>
<div class="custom-Rfigure-block">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="l1_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<p>In this example, we have two variables, or two dimensions, represented by the blue coordinate axes. The red and orange lines represent two sets of <strong>PC weights</strong>, meaning the new variables are the projections of all the observations onto these lines. This requires a bit of imagination. Imagine that all the observations fall perpendicularly onto the red line, which gives us the first new variable. Similarly, by projecting onto the orange direction, we get the second new variable. It’s easy to see that the variance (or information) along the red line is much higher than that along the orange line. Therefore, we can discard the orange variable, effectively reducing the dimensionality while preserving most of the information. Therefore, the linearity of PCA lies in the fact that we are simply re-examining the original feature space from different angles. In other words, we do not disturb the relative positions of the original observations.</p>
<p>The advantage of PCA is that the algorithm is simple and can be easily understood from a geometric perspective, but its limitations are quite apparent as well. First, its feature extraction does not rely on any target information to guide it. The feature extraction process only depends on the covariance structure of the original data. Second, its feature extraction capability is quite limited. We can see that all possible feature extraction methods are determined by the covariance structure inherent in the data. At most, we can have as many feature extraction functions as there are original variables, but most of them will be useless. Therefore, PCA is <strong>NOT very flexible</strong>, and when the problem becomes complex, it often fails to provide an effective feature extraction solution.</p>
<div class="custom-figure-block">
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/god.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:88.8%"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/demon.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:88.8%"></p>
</figure>
</div>
</div>
</div>
</div>
<p>PCA is like God rotating a globe, viewing what happens on this blue planet from different angles. When there’s a problem, God steps in to fix it. However, this gentle approach is linear. But when humanity presents too many complicated challenges, even God might be powerless, allowing the devil to intervene with non-linear methods in extreme ways to solve the problem.</p>
<p>This might not be the best analogy, especially since I’ve received warnings and restrictions while generating images with ChatGPT, but it is indeed quite vivid. So, let’s remember to take care of our beautiful blue planet. 🌍</p>
<!--- End  --->
<div style="text-align: center; margin: 30px 0">
<p><a href="../../../Courses/mlwr2_2025/l1/l1_home.html"><strong>Lecture 1 Homepage</strong></a></p>
</div>
<!--- --->


</section>
</section>

</main> <!-- /main -->
<div style="display: flex; justify-content: space-between; padding: 10px; font-size: 14px; color: #666; border-top: 1px solid #ddd;">
  <div>© 2024 Xijia Liu. All rights reserved. Contact: xijia.liu AT umu.se</div>
  <div><img src="../../../Images/logo.png" alt="Logo" style="width: 60px;"></div>
</div>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>